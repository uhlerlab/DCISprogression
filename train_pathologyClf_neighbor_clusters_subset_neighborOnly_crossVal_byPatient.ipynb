{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import scanpy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import models.loadImg as loadImg\n",
    "import models.modelsCNN as modelsCNN\n",
    "import models.optimizer as optimizer\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gc\n",
    "from skimage import io\n",
    "import scipy.stats\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "use_cuda=True\n",
    "datadir='/media/xinyi/dcis2idc/data'\n",
    "name='exp0'\n",
    "plotsavepath='/media/xinyi/dcis2idc/plots/cnnvae'+name\n",
    "sampledir=plotsavepath\n",
    "clustersavedir_alltrain=os.path.join(sampledir,'cluster_alltrain_reordered')\n",
    "ep=311\n",
    "with open(os.path.join(datadir,'processed','train_cnnvae_names'), 'rb') as input:\n",
    "    allImgNames=pickle.load(input)\n",
    "#plot by disease progression\n",
    "br1003aSpecs=pd.read_excel('/media/xinyi/dcis2idc/data/BR1003a specs.xlsx',header=10)\n",
    "br301Specs=pd.read_excel('/media/xinyi/dcis2idc/data/BR301 specs.xlsx',header=10)\n",
    "br8018aSpecs=pd.read_excel('/media/xinyi/dcis2idc/data/BR8018a specs.xlsx',header=10)\n",
    "br1003aSpecs.index=br1003aSpecs.loc[:,'Position']\n",
    "br301Specs.index=br301Specs.loc[:,'Position']\n",
    "br8018aSpecs.index=br8018aSpecs.loc[:,'Position']\n",
    "\n",
    "progList=np.copy(allImgNames)\n",
    "for s in np.unique(allImgNames):\n",
    "    ssplit=s.split('_')\n",
    "    if 'br1003a'==ssplit[0]:\n",
    "        prog_s=br1003aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br301'==ssplit[0]:\n",
    "        prog_s=br301Specs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br8018a'==ssplit[0]:\n",
    "        prog_s=br8018aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    progList[allImgNames==s]=prog_s\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=pd.read_csv(os.path.join(datadir,'Supplementary Table 1_v1.csv'),header=0)\n",
    "metadata.index=metadata.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPatientIDs=np.repeat('none',allImgNames.size).astype(object)\n",
    "for s in np.unique(allImgNames):\n",
    "    sidx=allImgNames==s\n",
    "    slideID=s.split('_')[0]\n",
    "    coreID=s.split('_')[-1]\n",
    "    allPatientIDs[sidx]=metadata.patient_id[slideID+'_'+str.lower(coreID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3]\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[0 1 2 3]\n",
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "savenamesample='alltrain'\n",
    "\n",
    "neworder=[1, 5, 3, 7, 2, 0, 4, 6]\n",
    "#use chosen subcluster number and save plots\n",
    "scanpy.settings.verbosity = 3\n",
    "# subcluster=8\n",
    "subclusterDict={0:[4],1:[6],2:[8],3:[6],4:[6],5:[6],6:[6],7:[4]}\n",
    "ncluster=8\n",
    "\n",
    "plotepoch=311\n",
    "clusterplotdir=os.path.join(clustersavedir_alltrain,'plots')\n",
    "n_pcs=50\n",
    "savenamecluster='minibatchkmean_ncluster'+str(ncluster)+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)\n",
    "with open(os.path.join(clustersavedir_alltrain,'minibatchkmean_ncluster'+str(ncluster)+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)+'_all'), 'rb') as output:\n",
    "    clusterRes=pickle.load(output)\n",
    "\n",
    "kmeans_sub=(np.zeros(clusterRes.size)-1).astype(str)\n",
    "savenameAdd='_plottingIdx_progBalanced_'+str(0)\n",
    "subclusternumbers=[4,6,8,6,6,6,6,4]\n",
    "savenamecluster='minibatchkmean_ncluster'+str(ncluster)+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)+savenameAdd\n",
    "for cnew in np.unique(clusterRes):\n",
    "#     print('cluster'+str(c))\n",
    "    c=neworder[cnew]\n",
    "    \n",
    "    subclustersavedir_alltrain=os.path.join(clustersavedir_alltrain,savenamecluster+'_subcluster'+str(c))\n",
    "    with open(os.path.join(subclustersavedir_alltrain,'minibatchkmean_ncluster'+str(subclusternumbers[c])+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)+'_all'), 'rb') as output:\n",
    "        subclusterRes=pickle.load(output)\n",
    "    print(np.unique(subclusterRes))\n",
    "    kmeans_sub[clusterRes==cnew]=np.char.add(np.repeat(str(cnew)+'-',subclusterRes.size),subclusterRes.astype(str))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir,'processed','train_cnnvae_coord'), 'rb') as output:\n",
    "    coordlist=pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in np.unique(progList):\n",
    "    if p=='Ductal carcinoma in situ':\n",
    "        progList[progList==p]='DCIS and breast tissue'\n",
    "    elif p=='Ductal carcinoma in situ and breast tissue':\n",
    "        progList[progList==p]='DCIS and breast tissue'\n",
    "    elif p=='Ductal carcinoma in situ with early infiltratio':\n",
    "        progList[progList==p]='DCIS with early infiltration'\n",
    "    \n",
    "    elif p=='Micropapillary type ductal carcinoma in situ wi':\n",
    "        progList[progList==p]='DCIS with early infiltration'    \n",
    "#     elif p=='Atypical hyperlasia':\n",
    "#         progList[progList==p]='Hyperplasia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "progInclude=np.array(['Breast tissue','Cancer adjacent normal breast tissue','Hyperplasia','Atypical hyperplasia','DCIS and breast tissue',  'DCIS with early infiltration','Invasive ductal carcinoma and breast tissue','Invasive ductal carcinoma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "progIncludeIdx=np.repeat(False,progList.size)\n",
    "for p in progInclude:\n",
    "    progIncludeIdx[progList==p]=True\n",
    "    \n",
    "coordlist=coordlist[progIncludeIdx]\n",
    "allImgNames=allImgNames[progIncludeIdx]\n",
    "clusterRes=clusterRes[progIncludeIdx]\n",
    "kmeans_sub=kmeans_sub[progIncludeIdx]\n",
    "progList=progList[progIncludeIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atypical hyperplasia\n",
      "14\n",
      "Breast tissue\n",
      "20\n",
      "Cancer adjacent normal breast tissue\n",
      "13\n",
      "DCIS and breast tissue\n",
      "16\n",
      "DCIS with early infiltration\n",
      "30\n",
      "Hyperplasia\n",
      "41\n",
      "Invasive ductal carcinoma\n",
      "70\n",
      "Invasive ductal carcinoma and breast tissue\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sUnique,sidx_start=np.unique(allImgNames,return_index=True)\n",
    "progUnique,labels_train,progCounts=np.unique(progList[sidx_start],return_counts=True,return_inverse=True)\n",
    "for p in range(progUnique.size):\n",
    "    print(progUnique[p])\n",
    "    print(progCounts[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHistMatrix_clusters(labels,ctlist,nrow=ncluster,ncol=ncluster):\n",
    "    res=np.zeros((nrow,ncol))\n",
    "    for li in range(res.shape[0]):\n",
    "        l=li\n",
    "        nl=np.sum(labels==l)\n",
    "        ctlist_l=ctlist[labels==l]\n",
    "        for ci in range(res.shape[1]):\n",
    "            c=ci\n",
    "            res[l,c]=np.sum(ctlist_l==c)\n",
    "#             res[li,ci]=np.sum(ctlist_l==c)/nl\n",
    "        if nl!=0:\n",
    "            res[li]=res[li]/nl\n",
    "    return res\n",
    "\n",
    "neighborhoodSize=48*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get neighborhood composition\n",
    "\n",
    "inputNeighborhood=np.zeros((sUnique.size,ncluster*ncluster))\n",
    "for i in range(sUnique.size):\n",
    "    imgN=sUnique[i]\n",
    "    nsamples=np.sum(allImgNames==imgN)\n",
    "    cluster_i=clusterRes[allImgNames==imgN]\n",
    "    neighbor_i=np.tile(cluster_i,(nsamples,1))\n",
    "    self_i=np.tile(cluster_i.reshape((-1,1)),(1,nsamples))\n",
    "\n",
    "    dist=pairwise_distances(coordlist[allImgNames==imgN],n_jobs=-1)\n",
    "    distIn=np.logical_and(dist<neighborhoodSize,dist>0)\n",
    "    res=getHistMatrix_clusters(self_i[distIn],neighbor_i[distIn])\n",
    "    \n",
    "    inputNeighborhood[i]=res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,inputCounts=np.unique(allImgNames,return_counts=True)\n",
    "inputAll_train=np.concatenate((inputNeighborhood,inputCounts.reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val cores (as validation cores) and val samples (as test cores)\n",
    "clustersavedir_valcores=os.path.join(sampledir,'cluster_valcores_reordered')\n",
    "clustersavedir_valsamples=os.path.join(sampledir,'cluster_valsamples_reordered')\n",
    "\n",
    "with open(os.path.join(datadir,'processed','train_cnnvae_coord_valcores'), 'rb') as output:\n",
    "    coordlist_valcores=pickle.load(output)\n",
    "with open(os.path.join(datadir,'processed','train_cnnvae_coord_valsamples'), 'rb') as output:\n",
    "    coordlist_valsamples=pickle.load(output)\n",
    "\n",
    "savenamecluster='minibatchkmean_ncluster'+str(ncluster)+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)\n",
    "with open(os.path.join(clustersavedir_valcores,savenamecluster+'_all'), 'rb') as output:\n",
    "    clusterRes_valcores=pickle.load(output)\n",
    "with open(os.path.join(clustersavedir_valsamples,'minibatchkmean_ncluster'+str(ncluster)+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)+'_all'), 'rb') as output:\n",
    "    clusterRes_valsamples=pickle.load(output)\n",
    "    \n",
    "kmeans_sub_valcores=(np.zeros(clusterRes_valcores.size)-1).astype(str)\n",
    "for c in np.unique(clusterRes_valcores):\n",
    "    subclustersavedir=os.path.join(clustersavedir_valcores,savenamecluster+'_plottingIdx_progBalanced_'+str(0)+'_subcluster'+str(neworder[c]))\n",
    "    with open(os.path.join(subclustersavedir,'minibatchkmean_ncluster'+str(subclusterDict[neworder[c]][0])+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)+'_all'), 'rb') as output:\n",
    "        subclusterRes=pickle.load(output)\n",
    "    kmeans_sub_valcores[clusterRes_valcores==c]=np.char.add(np.repeat(str(c)+'-',subclusterRes.size),subclusterRes.astype(str))\n",
    "    \n",
    "kmeans_sub_valsamples=(np.zeros(clusterRes_valsamples.size)-1).astype(str)\n",
    "for c in np.unique(clusterRes_valsamples):\n",
    "    subclustersavedir=os.path.join(clustersavedir_valsamples,savenamecluster+'_plottingIdx_progBalanced_'+str(0)+'_subcluster'+str(neworder[c]))\n",
    "    with open(os.path.join(subclustersavedir,'minibatchkmean_ncluster'+str(subclusterDict[neworder[c]][0])+'n_pcs'+str(n_pcs)+'epoch'+str(plotepoch)+'_all'), 'rb') as output:\n",
    "        subclusterRes=pickle.load(output)\n",
    "    kmeans_sub_valsamples[clusterRes_valsamples==c]=np.char.add(np.repeat(str(c)+'-',subclusterRes.size),subclusterRes.astype(str))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir,'processed','train_cnnvae_names_valcores'), 'rb') as input:\n",
    "    allImgNames_valcores=pickle.load(input)\n",
    "with open(os.path.join(datadir,'processed','train_cnnvae_names_valsamples'), 'rb') as input:\n",
    "    allImgNames_valsamples=pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot by disease progression\n",
    "br1003aSpecs=pd.read_excel('/media/xinyi/dcis2idc/data/BR1003a specs.xlsx',header=10)\n",
    "br301Specs=pd.read_excel('/media/xinyi/dcis2idc/data/BR301 specs.xlsx',header=10)\n",
    "br8018aSpecs=pd.read_excel('/media/xinyi/dcis2idc/data/BR8018a specs.xlsx',header=10)\n",
    "br1003aSpecs.index=br1003aSpecs.loc[:,'Position']\n",
    "br301Specs.index=br301Specs.loc[:,'Position']\n",
    "br8018aSpecs.index=br8018aSpecs.loc[:,'Position']\n",
    "\n",
    "progList_valcores=np.copy(allImgNames_valcores)\n",
    "for s in np.unique(allImgNames_valcores):\n",
    "    ssplit=s.split('_')\n",
    "    if 'br1003a'==ssplit[0]:\n",
    "        prog_s=br1003aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br301'==ssplit[0]:\n",
    "        prog_s=br301Specs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br8018a'==ssplit[0]:\n",
    "        prog_s=br8018aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    progList_valcores[allImgNames_valcores==s]=prog_s\n",
    "    \n",
    "progList_valsamples=np.copy(allImgNames_valsamples)\n",
    "for s in np.unique(allImgNames_valsamples):\n",
    "    ssplit=s.split('_')\n",
    "    if 'br1003a'==ssplit[0]:\n",
    "        prog_s=br1003aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br301'==ssplit[0]:\n",
    "        prog_s=br301Specs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br8018a'==ssplit[0]:\n",
    "        prog_s=br8018aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    progList_valsamples[allImgNames_valsamples==s]=prog_s\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in np.unique(progList_valcores):\n",
    "    if p=='Ductal carcinoma in situ':\n",
    "        progList_valcores[progList_valcores==p]='DCIS and breast tissue'\n",
    "    elif p=='Ductal carcinoma in situ and breast tissue':\n",
    "        progList_valcores[progList_valcores==p]='DCIS and breast tissue'\n",
    "    elif p=='Ductal carcinoma in situ with early infiltratio':\n",
    "        progList_valcores[progList_valcores==p]='DCIS with early infiltration'\n",
    "    \n",
    "    elif p=='Micropapillary type ductal carcinoma in situ wi':\n",
    "        progList_valcores[progList_valcores==p]='DCIS with early infiltration'    \n",
    "#     elif p=='Atypical hyperlasia':\n",
    "#         progList_valcores[progList_valcores==p]='Hyperplasia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in np.unique(progList_valsamples):\n",
    "    if p=='Ductal carcinoma in situ':\n",
    "        progList_valsamples[progList_valsamples==p]='DCIS and breast tissue'\n",
    "    elif p=='Ductal carcinoma in situ and breast tissue':\n",
    "        progList_valsamples[progList_valsamples==p]='DCIS and breast tissue'\n",
    "    elif p=='Ductal carcinoma in situ with early infiltrati':\n",
    "        progList_valsamples[progList_valsamples==p]='DCIS with early infiltration'\n",
    "    \n",
    "    elif p=='Micropapillary type ductal carcinoma in situ w':\n",
    "        progList_valsamples[progList_valsamples==p]='DCIS with early infiltration'    \n",
    "#     elif p=='Atypical hyperlasia':\n",
    "#         progList_valsamples[progList_valsamples==p]='Hyperplasia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "progIncludeIdx_valcores=np.repeat(False,progList_valcores.size)\n",
    "for p in progInclude:\n",
    "    progIncludeIdx_valcores[progList_valcores==p]=True\n",
    "    \n",
    "progIncludeIdx_valsamples=np.repeat(False,progList_valsamples.size)\n",
    "for p in progInclude:\n",
    "    progIncludeIdx_valsamples[progList_valsamples==p]=True\n",
    "    \n",
    "coordlist_valcores=coordlist_valcores[progIncludeIdx_valcores]\n",
    "allImgNames_valcores=allImgNames_valcores[progIncludeIdx_valcores]\n",
    "clusterRes_valcores=clusterRes_valcores[progIncludeIdx_valcores]\n",
    "kmeans_sub_valcores=kmeans_sub_valcores[progIncludeIdx_valcores]\n",
    "progList_valcores=progList_valcores[progIncludeIdx_valcores]\n",
    "\n",
    "coordlist_valsamples=coordlist_valsamples[progIncludeIdx_valsamples]\n",
    "allImgNames_valsamples=allImgNames_valsamples[progIncludeIdx_valsamples]\n",
    "clusterRes_valsamples=clusterRes_valsamples[progIncludeIdx_valsamples]\n",
    "kmeans_sub_valsamples=kmeans_sub_valsamples[progIncludeIdx_valsamples]\n",
    "progList_valsamples=progList_valsamples[progIncludeIdx_valsamples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atypical hyperplasia\n",
      "15\n",
      "Breast tissue\n",
      "20\n",
      "Cancer adjacent normal breast tissue\n",
      "1\n",
      "Hyperplasia\n",
      "35\n",
      "Invasive ductal carcinoma\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "sUnique_valcores,sidx_start_valcores=np.unique(allImgNames_valcores,return_index=True)\n",
    "progUnique_valcores,progCounts_valcores=np.unique(progList_valcores[sidx_start_valcores],return_counts=True)\n",
    "for p in range(progUnique_valcores.size):\n",
    "    print(progUnique_valcores[p])\n",
    "    print(progCounts_valcores[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atypical hyperplasia\n",
      "10\n",
      "Breast tissue\n",
      "14\n",
      "Cancer adjacent normal breast tissue\n",
      "4\n",
      "DCIS and breast tissue\n",
      "16\n",
      "DCIS with early infiltration\n",
      "29\n",
      "Hyperplasia\n",
      "25\n",
      "Invasive ductal carcinoma\n",
      "66\n",
      "Invasive ductal carcinoma and breast tissue\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sUnique_valsamples,sidx_start_valsamples=np.unique(allImgNames_valsamples,return_index=True)\n",
    "progUnique_valsamples,progCounts_valsamples=np.unique(progList_valsamples[sidx_start_valsamples],return_counts=True)\n",
    "for p in range(progUnique_valsamples.size):\n",
    "    print(progUnique_valsamples[p])\n",
    "    print(progCounts_valsamples[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct labels\n",
    "labels_valcores=np.zeros(progList_valcores[sidx_start_valcores].size)\n",
    "for i in range(progUnique.size):\n",
    "    labels_valcores[progList_valcores[sidx_start_valcores]==progUnique[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct labels\n",
    "labels_valsamples=np.zeros(progList_valsamples[sidx_start_valsamples].size)\n",
    "for i in range(progUnique.size):\n",
    "    labels_valsamples[progList_valsamples[sidx_start_valsamples]==progUnique[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputNeighborhood_valcores=np.zeros((sUnique_valcores.size,ncluster*ncluster))\n",
    "for i in range(sUnique_valcores.size):\n",
    "    imgN=sUnique_valcores[i]\n",
    "    nsamples=np.sum(allImgNames_valcores==imgN)\n",
    "    cluster_i=clusterRes_valcores[allImgNames_valcores==imgN]\n",
    "    neighbor_i=np.tile(cluster_i,(nsamples,1))\n",
    "    self_i=np.tile(cluster_i.reshape((-1,1)),(1,nsamples))\n",
    "\n",
    "    dist=pairwise_distances(coordlist_valcores[allImgNames_valcores==imgN],n_jobs=-1)\n",
    "    distIn=np.logical_and(dist<neighborhoodSize,dist>0)\n",
    "    res=getHistMatrix_clusters(self_i[distIn],neighbor_i[distIn])\n",
    "    \n",
    "    inputNeighborhood_valcores[i]=res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputNeighborhood_valsamples=np.zeros((sUnique_valsamples.size,ncluster*ncluster))\n",
    "for i in range(sUnique_valsamples.size):\n",
    "    imgN=sUnique_valsamples[i]\n",
    "    nsamples=np.sum(allImgNames_valsamples==imgN)\n",
    "    cluster_i=clusterRes_valsamples[allImgNames_valsamples==imgN]\n",
    "    neighbor_i=np.tile(cluster_i,(nsamples,1))\n",
    "    self_i=np.tile(cluster_i.reshape((-1,1)),(1,nsamples))\n",
    "\n",
    "    dist=pairwise_distances(coordlist_valsamples[allImgNames_valsamples==imgN],n_jobs=-1)\n",
    "    distIn=np.logical_and(dist<neighborhoodSize,dist>0)\n",
    "    res=getHistMatrix_clusters(self_i[distIn],neighbor_i[distIn])\n",
    "    \n",
    "    inputNeighborhood_valsamples[i]=res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,inputCounts_valcores=np.unique(allImgNames_valcores,return_counts=True)\n",
    "inputAll_valcores=np.concatenate((inputNeighborhood_valcores,inputCounts_valcores.reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,inputCounts_valsamples=np.unique(allImgNames_valsamples,return_counts=True)\n",
    "inputAll_valsamples=np.concatenate((inputNeighborhood_valsamples,inputCounts_valsamples.reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate cores\n",
    "inputAll=np.concatenate((inputAll_train,np.concatenate((inputAll_valcores,inputAll_valsamples),axis=0)),axis=0)\n",
    "imgNamesAll=np.concatenate((allImgNames[sidx_start],np.concatenate((allImgNames_valcores[sidx_start_valcores],allImgNames_valsamples[sidx_start_valsamples]))))\n",
    "labelsAll=np.concatenate((labels_train,np.concatenate((labels_valcores,labels_valsamples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,progCountsAll=np.unique(labelsAll,return_counts=True)\n",
    "weights_train=np.sum(progCountsAll)/progCountsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img sizes\n",
    "allImgNamesAll=np.concatenate((allImgNames,np.concatenate((allImgNames_valcores,allImgNames_valsamples))))\n",
    "progListAll=np.concatenate((progList,np.concatenate((progList_valcores,progList_valsamples))))\n",
    "sidxAll=np.concatenate((sidx_start,np.concatenate((sidx_start_valcores,sidx_start_valsamples))))\n",
    "coordlistAll=np.concatenate((coordlist,np.concatenate((coordlist_valcores,coordlist_valsamples),axis=0)),axis=0)\n",
    "\n",
    "imgSizeAll={}\n",
    "for p in progUnique:\n",
    "    img_cores=allImgNamesAll[sidxAll[progListAll[sidxAll]==p]]\n",
    "    pSizes=np.zeros((img_cores.size))\n",
    "    for si in range(img_cores.size):\n",
    "        scoord=coordlistAll[allImgNamesAll==img_cores[si]]\n",
    "        hsize=np.pi*np.square(np.max(scoord[:,0])-np.min(scoord[:,0]))\n",
    "        vsize=np.pi*np.square(np.max(scoord[:,1])-np.min(scoord[:,1]))\n",
    "        pSizes[si]=min(hsize,vsize)\n",
    "    imgSizeAll[p]=pSizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSize_median={}\n",
    "for p in progUnique:\n",
    "    imgSize_median[p]=np.median(imgSizeAll[p])\n",
    "plt.violinplot(list(imgSizeAll.values()))\n",
    "plt.scatter(np.arange(progUnique.size)+1,list(imgSize_median.values()))\n",
    "plt.xticks(np.arange(progUnique.size)+1,list(imgSizeAll),rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir,'processed','imgSizeByPath'), 'wb') as output:\n",
    "    pickle.dump(imgSize_median,output,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir,'processed','imgSizeByPath'), 'rb') as output:\n",
    "    imgSize_median=pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize count\n",
    "with open(os.path.join(datadir,'processed','imgSizeByPath'), 'rb') as output:\n",
    "    imgSize_median=pickle.load(output)\n",
    "areaAll=np.zeros(labelsAll.size)\n",
    "for s in range(labelsAll.size):\n",
    "    areaAll[s]=imgSize_median[progUnique[labelsAll.astype(int)][s]]\n",
    "inputAll[:,-1]=inputAll[:,-1]/areaAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=3\n",
    "epochs=6000\n",
    "saveFreq=200\n",
    "lr=0.001 #initial learning rate\n",
    "weight_decay=0 \n",
    "\n",
    "# batchsize=4\n",
    "batchsize=6000\n",
    "model_str='fc3'\n",
    "\n",
    "fc_dim1=64\n",
    "fc_dim2=64\n",
    "fc_dim3=64\n",
    "\n",
    "\n",
    "dropout=0.01\n",
    "\n",
    "name='exp0_pathologyClf_neighbor_clusters_exp0_subset_neighborOnly_crossVal_countAreaNorm_wAH_byPatient_s0'\n",
    "logsavepath='/media/xinyi/dcis2idc/log/cnnvae'+name\n",
    "modelsavepath='/media/xinyi/dcis2idc/models/cnnvae'+name\n",
    "plotsavepath='/media/xinyi/dcis2idc/plots/cnnvae'+name\n",
    "\n",
    "\n",
    "if not os.path.exists(logsavepath):\n",
    "    os.mkdir(logsavepath)\n",
    "if not os.path.exists(modelsavepath):\n",
    "    os.mkdir(modelsavepath)\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.mkdir(plotsavepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputAll=scipy.stats.zscore(inputAll,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,trainInput,labels_train):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(trainInput)\n",
    "\n",
    "    loss=lossCE(pred,labels_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%500==0:\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'loss_train: {:.4f}'.format(loss))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pIDList=allPatientIDs[np.concatenate((sidx_start,np.concatenate((sidx_start_valcores,sidx_start_valsamples))))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 loss_train: 2.0783\n",
      "Epoch: 0500 loss_train: 0.5197\n",
      "Epoch: 1000 loss_train: 0.2380\n",
      "Epoch: 1500 loss_train: 0.1908\n",
      "Epoch: 2000 loss_train: 0.1671\n",
      "Epoch: 2500 loss_train: 0.1322\n",
      "Epoch: 3000 loss_train: 0.1264\n",
      "Epoch: 3500 loss_train: 0.1261\n",
      "Epoch: 4000 loss_train: 0.1219\n",
      "Epoch: 4500 loss_train: 0.0856\n",
      "Epoch: 5000 loss_train: 0.0562\n",
      "Epoch: 5500 loss_train: 0.0824\n",
      " total time: 9.9001s\n",
      "7.02699613571167\n",
      "Epoch: 0000 loss_train: 2.0812\n",
      "Epoch: 0500 loss_train: 0.4649\n",
      "Epoch: 1000 loss_train: 0.2517\n",
      "Epoch: 1500 loss_train: 0.1720\n",
      "Epoch: 2000 loss_train: 0.1187\n",
      "Epoch: 2500 loss_train: 0.1398\n",
      "Epoch: 3000 loss_train: 0.1174\n",
      "Epoch: 3500 loss_train: 0.1071\n",
      "Epoch: 4000 loss_train: 0.0856\n",
      "Epoch: 4500 loss_train: 0.0713\n",
      "Epoch: 5000 loss_train: 0.0756\n",
      "Epoch: 5500 loss_train: 0.1233\n",
      " total time: 9.6571s\n",
      "0.058859992772340775\n",
      "Epoch: 0000 loss_train: 2.0827\n",
      "Epoch: 0500 loss_train: 0.4851\n",
      "Epoch: 1000 loss_train: 0.2640\n",
      "Epoch: 1500 loss_train: 0.1611\n",
      "Epoch: 2000 loss_train: 0.1379\n",
      "Epoch: 2500 loss_train: 0.1152\n",
      "Epoch: 3000 loss_train: 0.1342\n",
      "Epoch: 3500 loss_train: 0.0882\n",
      "Epoch: 4000 loss_train: 0.0825\n",
      "Epoch: 4500 loss_train: 0.1087\n",
      "Epoch: 5000 loss_train: 0.0696\n",
      "Epoch: 5500 loss_train: 0.0741\n",
      " total time: 9.5411s\n",
      "4.849152088165283\n",
      "Epoch: 0000 loss_train: 2.0790\n",
      "Epoch: 0500 loss_train: 0.4728\n",
      "Epoch: 1000 loss_train: 0.2670\n",
      "Epoch: 1500 loss_train: 0.2100\n",
      "Epoch: 2000 loss_train: 0.1199\n",
      "Epoch: 2500 loss_train: 0.1608\n",
      "Epoch: 3000 loss_train: 0.0997\n",
      "Epoch: 3500 loss_train: 0.0976\n",
      "Epoch: 4000 loss_train: 0.0847\n",
      "Epoch: 4500 loss_train: 0.0651\n",
      "Epoch: 5000 loss_train: 0.0786\n",
      "Epoch: 5500 loss_train: 0.0760\n",
      " total time: 9.6497s\n",
      "2.6127045154571533\n",
      "Epoch: 0000 loss_train: 2.0803\n",
      "Epoch: 0500 loss_train: 0.4677\n",
      "Epoch: 1000 loss_train: 0.2681\n",
      "Epoch: 1500 loss_train: 0.2100\n",
      "Epoch: 2000 loss_train: 0.1190\n",
      "Epoch: 2500 loss_train: 0.1340\n",
      "Epoch: 3000 loss_train: 0.1044\n",
      "Epoch: 3500 loss_train: 0.1109\n",
      "Epoch: 4000 loss_train: 0.0683\n",
      "Epoch: 4500 loss_train: 0.0832\n",
      "Epoch: 5000 loss_train: 0.0672\n",
      "Epoch: 5500 loss_train: 0.0619\n",
      " total time: 9.6104s\n",
      "1.6622123718261719\n",
      "Epoch: 0000 loss_train: 2.0862\n",
      "Epoch: 0500 loss_train: 0.5020\n",
      "Epoch: 1000 loss_train: 0.2314\n",
      "Epoch: 1500 loss_train: 0.1602\n",
      "Epoch: 2000 loss_train: 0.1333\n",
      "Epoch: 2500 loss_train: 0.1395\n",
      "Epoch: 3000 loss_train: 0.0923\n",
      "Epoch: 3500 loss_train: 0.1202\n",
      "Epoch: 4000 loss_train: 0.1085\n",
      "Epoch: 4500 loss_train: 0.0567\n",
      "Epoch: 5000 loss_train: 0.0694\n",
      "Epoch: 5500 loss_train: 0.0678\n",
      " total time: 9.9611s\n",
      "15.13892650604248\n",
      "Epoch: 0000 loss_train: 2.0790\n",
      "Epoch: 0500 loss_train: 0.4737\n",
      "Epoch: 1000 loss_train: 0.2493\n",
      "Epoch: 1500 loss_train: 0.1703\n",
      "Epoch: 2000 loss_train: 0.1223\n",
      "Epoch: 2500 loss_train: 0.1207\n",
      "Epoch: 3000 loss_train: 0.0969\n",
      "Epoch: 3500 loss_train: 0.0954\n",
      "Epoch: 4000 loss_train: 0.0846\n",
      "Epoch: 4500 loss_train: 0.0839\n",
      "Epoch: 5000 loss_train: 0.1039\n",
      "Epoch: 5500 loss_train: 0.0547\n",
      " total time: 9.4687s\n",
      "71.59423828125\n",
      "Epoch: 0000 loss_train: 2.0845\n",
      "Epoch: 0500 loss_train: 0.4969\n",
      "Epoch: 1000 loss_train: 0.3049\n",
      "Epoch: 1500 loss_train: 0.1863\n",
      "Epoch: 2000 loss_train: 0.1257\n",
      "Epoch: 2500 loss_train: 0.1328\n",
      "Epoch: 3000 loss_train: 0.1076\n",
      "Epoch: 3500 loss_train: 0.1119\n",
      "Epoch: 4000 loss_train: 0.0541\n",
      "Epoch: 4500 loss_train: 0.0852\n",
      "Epoch: 5000 loss_train: 0.0706\n",
      "Epoch: 5500 loss_train: 0.0609\n",
      " total time: 9.4403s\n",
      "0.4331091642379761\n",
      "Epoch: 0000 loss_train: 2.0839\n",
      "Epoch: 0500 loss_train: 0.4318\n",
      "Epoch: 1000 loss_train: 0.2516\n",
      "Epoch: 1500 loss_train: 0.1774\n",
      "Epoch: 2000 loss_train: 0.1133\n",
      "Epoch: 2500 loss_train: 0.1453\n",
      "Epoch: 3000 loss_train: 0.0942\n",
      "Epoch: 3500 loss_train: 0.1244\n",
      "Epoch: 4000 loss_train: 0.0778\n",
      "Epoch: 4500 loss_train: 0.0628\n",
      "Epoch: 5000 loss_train: 0.0681\n",
      "Epoch: 5500 loss_train: 0.0682\n",
      " total time: 9.6258s\n",
      "0.8402720093727112\n",
      "Epoch: 0000 loss_train: 2.0811\n",
      "Epoch: 0500 loss_train: 0.4526\n",
      "Epoch: 1000 loss_train: 0.2555\n",
      "Epoch: 1500 loss_train: 0.1639\n",
      "Epoch: 2000 loss_train: 0.1350\n",
      "Epoch: 2500 loss_train: 0.1307\n",
      "Epoch: 3000 loss_train: 0.1000\n",
      "Epoch: 3500 loss_train: 0.0896\n",
      "Epoch: 4000 loss_train: 0.1300\n",
      "Epoch: 4500 loss_train: 0.0885\n",
      "Epoch: 5000 loss_train: 0.0964\n",
      "Epoch: 5500 loss_train: 0.1058\n",
      " total time: 9.6000s\n",
      "0.28004148602485657\n",
      "Epoch: 0000 loss_train: 2.0835\n",
      "Epoch: 0500 loss_train: 0.4744\n",
      "Epoch: 1000 loss_train: 0.2988\n",
      "Epoch: 1500 loss_train: 0.1516\n",
      "Epoch: 2000 loss_train: 0.1470\n",
      "Epoch: 2500 loss_train: 0.1123\n",
      "Epoch: 3000 loss_train: 0.1059\n",
      "Epoch: 3500 loss_train: 0.0926\n",
      "Epoch: 4000 loss_train: 0.0908\n",
      "Epoch: 4500 loss_train: 0.0890\n",
      "Epoch: 5000 loss_train: 0.0646\n",
      "Epoch: 5500 loss_train: 0.0644\n",
      " total time: 9.5076s\n",
      "0.5734506249427795\n",
      "Epoch: 0000 loss_train: 2.0803\n",
      "Epoch: 0500 loss_train: 0.4361\n",
      "Epoch: 1000 loss_train: 0.2585\n",
      "Epoch: 1500 loss_train: 0.1574\n",
      "Epoch: 2000 loss_train: 0.1143\n",
      "Epoch: 2500 loss_train: 0.0911\n",
      "Epoch: 3000 loss_train: 0.0930\n",
      "Epoch: 3500 loss_train: 0.1145\n",
      "Epoch: 4000 loss_train: 0.0861\n",
      "Epoch: 4500 loss_train: 0.0681\n",
      "Epoch: 5000 loss_train: 0.0728\n",
      "Epoch: 5500 loss_train: 0.0841\n",
      " total time: 9.8765s\n",
      "0.0\n",
      "Epoch: 0000 loss_train: 2.0854\n",
      "Epoch: 0500 loss_train: 0.4865\n",
      "Epoch: 1000 loss_train: 0.2804\n",
      "Epoch: 1500 loss_train: 0.2079\n",
      "Epoch: 2000 loss_train: 0.1207\n",
      "Epoch: 2500 loss_train: 0.1747\n",
      "Epoch: 3000 loss_train: 0.1140\n",
      "Epoch: 3500 loss_train: 0.1323\n",
      "Epoch: 4000 loss_train: 0.0644\n",
      "Epoch: 4500 loss_train: 0.1483\n",
      "Epoch: 5000 loss_train: 0.0818\n",
      "Epoch: 5500 loss_train: 0.1246\n",
      " total time: 10.0513s\n",
      "0.10743185132741928\n",
      "Epoch: 0000 loss_train: 2.0848\n",
      "Epoch: 0500 loss_train: 0.4352\n",
      "Epoch: 1000 loss_train: 0.2815\n",
      "Epoch: 1500 loss_train: 0.2093\n",
      "Epoch: 2000 loss_train: 0.1277\n",
      "Epoch: 2500 loss_train: 0.1974\n",
      "Epoch: 3000 loss_train: 0.0906\n",
      "Epoch: 3500 loss_train: 0.1211\n",
      "Epoch: 4000 loss_train: 0.1341\n",
      "Epoch: 4500 loss_train: 0.0717\n",
      "Epoch: 5000 loss_train: 0.0604\n",
      "Epoch: 5500 loss_train: 0.0517\n",
      " total time: 9.9312s\n",
      "0.0009485845803283155\n",
      "Epoch: 0000 loss_train: 2.0831\n",
      "Epoch: 0500 loss_train: 0.4752\n",
      "Epoch: 1000 loss_train: 0.2693\n",
      "Epoch: 1500 loss_train: 0.1815\n",
      "Epoch: 2000 loss_train: 0.1320\n",
      "Epoch: 2500 loss_train: 0.0945\n",
      "Epoch: 3000 loss_train: 0.1063\n",
      "Epoch: 3500 loss_train: 0.0718\n",
      "Epoch: 4000 loss_train: 0.0852\n",
      "Epoch: 4500 loss_train: 0.0798\n",
      "Epoch: 5000 loss_train: 0.0831\n",
      "Epoch: 5500 loss_train: 0.0663\n",
      " total time: 9.8331s\n",
      "3.248950242996216\n",
      "Epoch: 0000 loss_train: 2.0810\n",
      "Epoch: 0500 loss_train: 0.4653\n",
      "Epoch: 1000 loss_train: 0.2693\n",
      "Epoch: 1500 loss_train: 0.2032\n",
      "Epoch: 2000 loss_train: 0.1100\n",
      "Epoch: 2500 loss_train: 0.1217\n",
      "Epoch: 3000 loss_train: 0.1025\n",
      "Epoch: 3500 loss_train: 0.1093\n",
      "Epoch: 4000 loss_train: 0.0940\n",
      "Epoch: 4500 loss_train: 0.0716\n",
      "Epoch: 5000 loss_train: 0.0650\n",
      "Epoch: 5500 loss_train: 0.0856\n",
      " total time: 9.8778s\n",
      "0.0\n",
      "Epoch: 0000 loss_train: 2.0805\n",
      "Epoch: 0500 loss_train: 0.4500\n",
      "Epoch: 1000 loss_train: 0.2439\n",
      "Epoch: 1500 loss_train: 0.1615\n",
      "Epoch: 2000 loss_train: 0.1039\n",
      "Epoch: 2500 loss_train: 0.0942\n",
      "Epoch: 3000 loss_train: 0.0885\n",
      "Epoch: 3500 loss_train: 0.1061\n",
      "Epoch: 4000 loss_train: 0.0938\n",
      "Epoch: 4500 loss_train: 0.0595\n",
      "Epoch: 5000 loss_train: 0.0713\n",
      "Epoch: 5500 loss_train: 0.0624\n",
      " total time: 9.8365s\n",
      "0.008926349692046642\n",
      "Epoch: 0000 loss_train: 2.0854\n",
      "Epoch: 0500 loss_train: 0.4244\n",
      "Epoch: 1000 loss_train: 0.2577\n",
      "Epoch: 1500 loss_train: 0.1924\n",
      "Epoch: 2000 loss_train: 0.1108\n",
      "Epoch: 2500 loss_train: 0.1223\n",
      "Epoch: 3000 loss_train: 0.0967\n",
      "Epoch: 3500 loss_train: 0.1213\n",
      "Epoch: 4000 loss_train: 0.0873\n",
      "Epoch: 4500 loss_train: 0.1075\n",
      "Epoch: 5000 loss_train: 0.0524\n",
      "Epoch: 5500 loss_train: 0.0539\n",
      " total time: 10.0925s\n",
      "14.685823440551758\n",
      "Epoch: 0000 loss_train: 2.0817\n",
      "Epoch: 0500 loss_train: 0.4808\n",
      "Epoch: 1000 loss_train: 0.2508\n",
      "Epoch: 1500 loss_train: 0.2148\n",
      "Epoch: 2000 loss_train: 0.1142\n",
      "Epoch: 2500 loss_train: 0.1252\n",
      "Epoch: 3000 loss_train: 0.1079\n",
      "Epoch: 3500 loss_train: 0.1216\n",
      "Epoch: 4000 loss_train: 0.0945\n",
      "Epoch: 4500 loss_train: 0.0605\n",
      "Epoch: 5000 loss_train: 0.0878\n",
      "Epoch: 5500 loss_train: 0.0633\n",
      " total time: 9.9878s\n",
      "0.0\n",
      "Epoch: 0000 loss_train: 2.0850\n",
      "Epoch: 0500 loss_train: 0.4925\n",
      "Epoch: 1000 loss_train: 0.2870\n",
      "Epoch: 1500 loss_train: 0.2024\n",
      "Epoch: 2000 loss_train: 0.1218\n",
      "Epoch: 2500 loss_train: 0.1804\n",
      "Epoch: 3000 loss_train: 0.1125\n",
      "Epoch: 3500 loss_train: 0.1059\n",
      "Epoch: 4000 loss_train: 0.0510\n",
      "Epoch: 4500 loss_train: 0.1074\n",
      "Epoch: 5000 loss_train: 0.0982\n",
      "Epoch: 5500 loss_train: 0.1055\n",
      " total time: 9.8908s\n",
      "0.4851468503475189\n",
      "Epoch: 0000 loss_train: 2.0829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0500 loss_train: 0.4893\n",
      "Epoch: 1000 loss_train: 0.2931\n",
      "Epoch: 1500 loss_train: 0.1860\n",
      "Epoch: 2000 loss_train: 0.1226\n",
      "Epoch: 2500 loss_train: 0.0953\n",
      "Epoch: 3000 loss_train: 0.0901\n",
      "Epoch: 3500 loss_train: 0.0781\n",
      "Epoch: 4000 loss_train: 0.0689\n",
      "Epoch: 4500 loss_train: 0.0771\n",
      "Epoch: 5000 loss_train: 0.0651\n",
      "Epoch: 5500 loss_train: 0.0956\n",
      " total time: 9.8416s\n",
      "0.07343684136867523\n",
      "Epoch: 0000 loss_train: 2.0848\n",
      "Epoch: 0500 loss_train: 0.4342\n",
      "Epoch: 1000 loss_train: 0.2760\n",
      "Epoch: 1500 loss_train: 0.1895\n",
      "Epoch: 2000 loss_train: 0.1127\n",
      "Epoch: 2500 loss_train: 0.0953\n",
      "Epoch: 3000 loss_train: 0.0854\n",
      "Epoch: 3500 loss_train: 0.1724\n",
      "Epoch: 4000 loss_train: 0.0780\n",
      "Epoch: 4500 loss_train: 0.0477\n",
      "Epoch: 5000 loss_train: 0.0892\n",
      "Epoch: 5500 loss_train: 0.0448\n",
      " total time: 9.5814s\n",
      "10.023831367492676\n",
      "Epoch: 0000 loss_train: 2.0818\n",
      "Epoch: 0500 loss_train: 0.4771\n",
      "Epoch: 1000 loss_train: 0.2611\n",
      "Epoch: 1500 loss_train: 0.1751\n",
      "Epoch: 2000 loss_train: 0.1262\n",
      "Epoch: 2500 loss_train: 0.1629\n",
      "Epoch: 3000 loss_train: 0.1164\n",
      "Epoch: 3500 loss_train: 0.1067\n",
      "Epoch: 4000 loss_train: 0.0825\n",
      "Epoch: 4500 loss_train: 0.0594\n",
      "Epoch: 5000 loss_train: 0.0820\n",
      "Epoch: 5500 loss_train: 0.0921\n",
      " total time: 9.5549s\n",
      "1.1998075246810913\n",
      "Epoch: 0000 loss_train: 2.0866\n",
      "Epoch: 0500 loss_train: 0.4858\n",
      "Epoch: 1000 loss_train: 0.2577\n",
      "Epoch: 1500 loss_train: 0.1815\n",
      "Epoch: 2000 loss_train: 0.1584\n",
      "Epoch: 2500 loss_train: 0.1126\n",
      "Epoch: 3000 loss_train: 0.1401\n",
      "Epoch: 3500 loss_train: 0.0935\n",
      "Epoch: 4000 loss_train: 0.0897\n",
      "Epoch: 4500 loss_train: 0.0549\n",
      "Epoch: 5000 loss_train: 0.1226\n",
      "Epoch: 5500 loss_train: 0.0909\n",
      " total time: 9.4373s\n",
      "6.252957820892334\n",
      "Epoch: 0000 loss_train: 2.0828\n",
      "Epoch: 0500 loss_train: 0.4835\n",
      "Epoch: 1000 loss_train: 0.2836\n",
      "Epoch: 1500 loss_train: 0.2124\n",
      "Epoch: 2000 loss_train: 0.1420\n",
      "Epoch: 2500 loss_train: 0.1251\n",
      "Epoch: 3000 loss_train: 0.0966\n",
      "Epoch: 3500 loss_train: 0.1100\n",
      "Epoch: 4000 loss_train: 0.0932\n",
      "Epoch: 4500 loss_train: 0.0762\n",
      "Epoch: 5000 loss_train: 0.0677\n",
      "Epoch: 5500 loss_train: 0.0897\n",
      " total time: 9.5750s\n",
      "0.032743096351623535\n",
      "Epoch: 0000 loss_train: 2.0860\n",
      "Epoch: 0500 loss_train: 0.4938\n",
      "Epoch: 1000 loss_train: 0.2574\n",
      "Epoch: 1500 loss_train: 0.1898\n",
      "Epoch: 2000 loss_train: 0.1036\n",
      "Epoch: 2500 loss_train: 0.1377\n",
      "Epoch: 3000 loss_train: 0.1014\n",
      "Epoch: 3500 loss_train: 0.0894\n",
      "Epoch: 4000 loss_train: 0.1081\n",
      "Epoch: 4500 loss_train: 0.0956\n",
      "Epoch: 5000 loss_train: 0.0866\n",
      "Epoch: 5500 loss_train: 0.0496\n",
      " total time: 9.5947s\n",
      "10.936683654785156\n",
      "Epoch: 0000 loss_train: 2.0795\n",
      "Epoch: 0500 loss_train: 0.4707\n",
      "Epoch: 1000 loss_train: 0.2748\n",
      "Epoch: 1500 loss_train: 0.1610\n",
      "Epoch: 2000 loss_train: 0.1298\n",
      "Epoch: 2500 loss_train: 0.1159\n",
      "Epoch: 3000 loss_train: 0.1092\n",
      "Epoch: 3500 loss_train: 0.1254\n",
      "Epoch: 4000 loss_train: 0.0764\n",
      "Epoch: 4500 loss_train: 0.0900\n",
      "Epoch: 5000 loss_train: 0.0686\n",
      "Epoch: 5500 loss_train: 0.1081\n",
      " total time: 9.5432s\n",
      "2.8687474727630615\n",
      "Epoch: 0000 loss_train: 2.0805\n",
      "Epoch: 0500 loss_train: 0.4415\n",
      "Epoch: 1000 loss_train: 0.2712\n",
      "Epoch: 1500 loss_train: 0.1669\n",
      "Epoch: 2000 loss_train: 0.1115\n",
      "Epoch: 2500 loss_train: 0.0971\n",
      "Epoch: 3000 loss_train: 0.0777\n",
      "Epoch: 3500 loss_train: 0.0992\n",
      "Epoch: 4000 loss_train: 0.0880\n",
      "Epoch: 4500 loss_train: 0.0783\n",
      "Epoch: 5000 loss_train: 0.0693\n",
      "Epoch: 5500 loss_train: 0.0863\n",
      " total time: 9.5273s\n",
      "0.0\n",
      "Epoch: 0000 loss_train: 2.0815\n",
      "Epoch: 0500 loss_train: 0.4756\n",
      "Epoch: 1000 loss_train: 0.2390\n",
      "Epoch: 1500 loss_train: 0.1782\n",
      "Epoch: 2000 loss_train: 0.1079\n",
      "Epoch: 2500 loss_train: 0.1147\n",
      "Epoch: 3000 loss_train: 0.1047\n",
      "Epoch: 3500 loss_train: 0.0960\n",
      "Epoch: 4000 loss_train: 0.0768\n",
      "Epoch: 4500 loss_train: 0.0695\n",
      "Epoch: 5000 loss_train: 0.0699\n",
      "Epoch: 5500 loss_train: 0.0747\n",
      " total time: 9.4796s\n",
      "56.42327117919922\n",
      "Epoch: 0000 loss_train: 2.0787\n",
      "Epoch: 0500 loss_train: 0.4553\n",
      "Epoch: 1000 loss_train: 0.2903\n",
      "Epoch: 1500 loss_train: 0.1633\n",
      "Epoch: 2000 loss_train: 0.1221\n",
      "Epoch: 2500 loss_train: 0.1077\n",
      "Epoch: 3000 loss_train: 0.0958\n",
      "Epoch: 3500 loss_train: 0.1195\n",
      "Epoch: 4000 loss_train: 0.0775\n",
      "Epoch: 4500 loss_train: 0.0883\n",
      "Epoch: 5000 loss_train: 0.0611\n",
      "Epoch: 5500 loss_train: 0.0752\n",
      " total time: 9.5073s\n",
      "21.027761459350586\n",
      "Epoch: 0000 loss_train: 2.0815\n",
      "Epoch: 0500 loss_train: 0.4333\n",
      "Epoch: 1000 loss_train: 0.2598\n",
      "Epoch: 1500 loss_train: 0.1718\n",
      "Epoch: 2000 loss_train: 0.1275\n",
      "Epoch: 2500 loss_train: 0.1900\n",
      "Epoch: 3000 loss_train: 0.1412\n",
      "Epoch: 3500 loss_train: 0.0935\n",
      "Epoch: 4000 loss_train: 0.0747\n",
      "Epoch: 4500 loss_train: 0.0689\n",
      "Epoch: 5000 loss_train: 0.0810\n",
      "Epoch: 5500 loss_train: 0.0458\n",
      " total time: 9.4979s\n",
      "6.647566318511963\n",
      "Epoch: 0000 loss_train: 2.0794\n",
      "Epoch: 0500 loss_train: 0.4605\n",
      "Epoch: 1000 loss_train: 0.2712\n",
      "Epoch: 1500 loss_train: 0.1613\n",
      "Epoch: 2000 loss_train: 0.1374\n",
      "Epoch: 2500 loss_train: 0.1275\n",
      "Epoch: 3000 loss_train: 0.0996\n",
      "Epoch: 3500 loss_train: 0.1138\n",
      "Epoch: 4000 loss_train: 0.0760\n",
      "Epoch: 4500 loss_train: 0.0651\n",
      "Epoch: 5000 loss_train: 0.0817\n",
      "Epoch: 5500 loss_train: 0.1159\n",
      " total time: 9.4409s\n",
      "17.242006301879883\n",
      "Epoch: 0000 loss_train: 2.0816\n",
      "Epoch: 0500 loss_train: 0.4685\n",
      "Epoch: 1000 loss_train: 0.2679\n",
      "Epoch: 1500 loss_train: 0.1969\n",
      "Epoch: 2000 loss_train: 0.1017\n",
      "Epoch: 2500 loss_train: 0.1175\n",
      "Epoch: 3000 loss_train: 0.0965\n",
      "Epoch: 3500 loss_train: 0.0756\n",
      "Epoch: 4000 loss_train: 0.0732\n",
      "Epoch: 4500 loss_train: 0.0629\n",
      "Epoch: 5000 loss_train: 0.0755\n",
      "Epoch: 5500 loss_train: 0.0771\n",
      " total time: 9.4259s\n",
      "1.9073476664743794e-07\n",
      "Epoch: 0000 loss_train: 2.0819\n",
      "Epoch: 0500 loss_train: 0.4478\n",
      "Epoch: 1000 loss_train: 0.2844\n",
      "Epoch: 1500 loss_train: 0.1890\n",
      "Epoch: 2000 loss_train: 0.1259\n",
      "Epoch: 2500 loss_train: 0.1036\n",
      "Epoch: 3000 loss_train: 0.1512\n",
      "Epoch: 3500 loss_train: 0.1175\n",
      "Epoch: 4000 loss_train: 0.0895\n",
      "Epoch: 4500 loss_train: 0.0798\n",
      "Epoch: 5000 loss_train: 0.0765\n",
      "Epoch: 5500 loss_train: 0.0741\n",
      " total time: 9.4375s\n",
      "49.62905502319336\n",
      "Epoch: 0000 loss_train: 2.0774\n",
      "Epoch: 0500 loss_train: 0.4270\n",
      "Epoch: 1000 loss_train: 0.2407\n",
      "Epoch: 1500 loss_train: 0.1514\n",
      "Epoch: 2000 loss_train: 0.1117\n",
      "Epoch: 2500 loss_train: 0.1199\n",
      "Epoch: 3000 loss_train: 0.1280\n",
      "Epoch: 3500 loss_train: 0.1383\n",
      "Epoch: 4000 loss_train: 0.0793\n",
      "Epoch: 4500 loss_train: 0.0473\n",
      "Epoch: 5000 loss_train: 0.0647\n",
      "Epoch: 5500 loss_train: 0.0565\n",
      " total time: 9.4428s\n",
      "17.605573654174805\n",
      "Epoch: 0000 loss_train: 2.0805\n",
      "Epoch: 0500 loss_train: 0.4350\n",
      "Epoch: 1000 loss_train: 0.2618\n",
      "Epoch: 1500 loss_train: 0.1619\n",
      "Epoch: 2000 loss_train: 0.1101\n",
      "Epoch: 2500 loss_train: 0.0970\n",
      "Epoch: 3000 loss_train: 0.0873\n",
      "Epoch: 3500 loss_train: 0.1002\n",
      "Epoch: 4000 loss_train: 0.0982\n",
      "Epoch: 4500 loss_train: 0.0646\n"
     ]
    }
   ],
   "source": [
    "inputAll=torch.tensor(inputAll).cuda().float()\n",
    "labelsAll=torch.tensor(labelsAll).cuda().long()\n",
    "\n",
    "testepoch=4800\n",
    "predtest=np.zeros((inputAll.shape[0],np.unique(labels_train).size))\n",
    "for patientIDX in range(np.unique(pIDList).size):\n",
    "    patientID=np.unique(pIDList)[patientIDX]\n",
    "    sampleIdx=np.arange(inputAll.shape[0])[pIDList==patientID]\n",
    "    trainIdx=np.arange(inputAll.shape[0])[pIDList!=patientID]\n",
    "    \n",
    "    seed=3\n",
    "    torch.manual_seed(seed)\n",
    "    nclasses=np.unique(labels_train).size\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    nfeatures=inputAll.shape[1]\n",
    "    if model_str=='fc3':\n",
    "        model = modelsCNN.FC_l3(nfeatures,fc_dim1,fc_dim2,fc_dim3,nclasses,0.5,regrs=False)\n",
    "        lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weights_train).cuda().float())\n",
    "    if model_str=='fc5':\n",
    "        model = modelsCNN.FC_l5(nfeatures,fc_dim1,fc_dim2,fc_dim3,fc_dim4,fc_dim5,nclasses,0.5,regrs=False)\n",
    "        lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weights_train).cuda().float())\n",
    "    if model_str=='fc1':\n",
    "        model = modelsCNN.FC_l1(nfeatures,fc_dim1,nclasses,regrs=False)\n",
    "        lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weights_train).cuda().float())\n",
    "    if model_str=='fc0':\n",
    "        model = modelsCNN.FC_l0(nfeatures,nclasses,regrs=False)\n",
    "        lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weights_train).cuda().float())\n",
    "\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_loss_ep=[None]*epochs\n",
    "    val_loss_ep=[None]*epochs\n",
    "    t_ep=time.time()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        train_loss_ep[ep]=train(ep,inputAll[trainIdx],labelsAll[trainIdx])\n",
    "\n",
    "\n",
    "        if ep%saveFreq == 0 and ep!=0:\n",
    "            torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,patientID+'_'+str(ep)+'.pt'))\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "            torch.cuda.empty_cache()\n",
    "    print(' total time: {:.4f}s'.format(time.time() - t_ep))\n",
    "\n",
    "    with open(os.path.join(logsavepath,patientID+'_train_loss'), 'wb') as output:\n",
    "        pickle.dump(train_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(modelsavepath,patientID+'_'+str(testepoch)+'.pt')))\n",
    "    with torch.no_grad():\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        pred = model(inputAll[[sampleIdx]])\n",
    "        predtest[sampleIdx]=pred.cpu().detach().numpy()\n",
    "\n",
    "        loss_test=lossCE(pred,labelsAll[[sampleIdx]]).item()\n",
    "\n",
    "    print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(logsavepath,'crossVal_loss'), 'wb') as output:\n",
    "    pickle.dump(predtest, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtest_label=np.argmax(predtest,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame({'sampleName':imgNamesAll,'true':progUnique[labelsAll.cpu().numpy()],'predicted':progUnique[predtest_label]})\n",
    "res.to_csv(os.path.join(plotsavepath,'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion\n",
    "def plotCTcomp(labels,ctlist,savepath,savenamecluster,byCT,addname='',order=progInclude):\n",
    "    res=np.zeros((order.size,order.size))\n",
    "    for li in range(res.shape[0]):\n",
    "        l=order[li]\n",
    "        nl=np.sum(labels==l)\n",
    "        ctlist_l=ctlist[labels==l]\n",
    "        for ci in range(res.shape[1]):\n",
    "            c=order[ci]\n",
    "            res[li,ci]=np.sum(ctlist_l==c)\n",
    "#             res[li,ci]=np.sum(ctlist_l==c)/nl\n",
    "    if not byCT:\n",
    "        addname+=''\n",
    "        for li in range(res.shape[0]):\n",
    "            l=order[li]\n",
    "            nl=np.sum(labels==l)\n",
    "            res[li]=res[li]/nl\n",
    "    else:\n",
    "        addname+='_normbyCT'\n",
    "        for ci in range(res.shape[1]):\n",
    "            c=order[ci]\n",
    "            nc=np.sum(ctlist==c)\n",
    "            res[:,ci]=res[:,ci]/nc\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(res,cmap='binary',vmin=0,vmax=1)\n",
    "    fig.colorbar(im)\n",
    "    ax.set_yticks(np.arange(order.size))\n",
    "    ax.set_yticklabels(order)\n",
    "    ax.set_xticks(np.arange(order.size))\n",
    "    ax.set_xticklabels(order)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(savepath,savenamecluster+addname+'.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "plotCTcomp(progUnique[labelsAll.cpu().numpy()],progUnique[predtest_label],plotsavepath,'confusion'+str(testepoch),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCTcomp(progUnique[labelsAll.cpu().numpy()][:sidx_start.size],progUnique[predtest_label][:sidx_start.size],plotsavepath,'confusion_excludeValSamples'+str(testepoch),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 65)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputAll_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
