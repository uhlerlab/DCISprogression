{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import scanpy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import models.loadImg as loadImg\n",
    "import models.modelsCNN as modelsCNN\n",
    "import models.optimizer as optimizer\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gc\n",
    "from skimage import io\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"2\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "use_cuda=True\n",
    "datadir='/media/xinyi/dcis2idc/data'\n",
    "name='exp0'\n",
    "plotsavepath='/media/xinyi/dcis2idc/plots/cnnvae'+name\n",
    "plottype='umap'\n",
    "sampledir=plotsavepath\n",
    "savedir=os.path.join(sampledir,'embedding_'+plottype)\n",
    "clustersavedir=os.path.join(sampledir,'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir,'processed','train_cnnvae_names'), 'rb') as input:\n",
    "    allImgNames=pickle.load(input)\n",
    "\n",
    "\n",
    "br1003aSpecs=pd.read_excel('/media/xinyi/dcis2idc/data/BR1003a specs.xlsx',header=10)\n",
    "br301Specs=pd.read_excel('/media/xinyi/dcis2idc/data/BR301 specs.xlsx',header=10)\n",
    "br8018aSpecs=pd.read_excel('/media/xinyi/dcis2idc/data/BR8018a specs.xlsx',header=10)\n",
    "br1003aSpecs.index=br1003aSpecs.loc[:,'Position']\n",
    "br301Specs.index=br301Specs.loc[:,'Position']\n",
    "br8018aSpecs.index=br8018aSpecs.loc[:,'Position']\n",
    "progList=np.copy(allImgNames)\n",
    "for s in np.unique(allImgNames):\n",
    "    ssplit=s.split('_')\n",
    "    if 'br1003a'==ssplit[0]:\n",
    "        prog_s=br1003aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br301'==ssplit[0]:\n",
    "        prog_s=br301Specs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    elif 'br8018a'==ssplit[0]:\n",
    "        prog_s=br8018aSpecs.loc[(ssplit[-1],'Pathology diagnosis')]\n",
    "    progList[allImgNames==s]=prog_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A1 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A2 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A4 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A5 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A6 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A7 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A8 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A9 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C1 Atypical hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C10 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C2 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C3 Atypical hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C4 Atypical hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C5 Atypical hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C6 Atypical hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C7 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C8 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C9 Hyperplasia\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I1 Breast tissue\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I10 Breast tissue\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I2 Breast tissue\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I3 Breast tissue\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I7 Breast tissue\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I8 Breast tissue\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I9 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_A1 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A2 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A3 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A5 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A6 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A7 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A8 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_A9 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C1 Atypical hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C10 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C2 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C3 Atypical hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C4 Atypical hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C5 Atypical hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C6 Atypical hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C7 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C8 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_C9 Hyperplasia\n",
      "br1003a_3_collagen1_647_hoechst_I1 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_I10 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_I2 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_I3 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_I7 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_I8 Breast tissue\n",
      "br1003a_3_collagen1_647_hoechst_I9 Breast tissue\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A1 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A10 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A2 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A3 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A4 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A5 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A6 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A7 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A8 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A9 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C1 Atypical hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C10 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C2 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C4 Atypical hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C5 Atypical hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C6 Atypical hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C7 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C8 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C9 Hyperplasia\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I1 Breast tissue\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I2 Breast tissue\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I3 Breast tissue\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I7 Breast tissue\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I8 Breast tissue\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I9 Breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A1 Ductal carcinoma in situ and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A3 Ductal carcinoma in situ\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A4 Ductal carcinoma in situ\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A5 Ductal carcinoma in situ and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A6 Ductal carcinoma in situ and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B1 Invasive ductal carcinoma\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B2 Ductal carcinoma in situ and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B3 Ductal carcinoma in situ and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B4 Ductal carcinoma in situ and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B5 Invasive ductal carcinoma and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B6 Invasive ductal carcinoma and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C1 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C3 Micropapillary type ductal carcinoma in situ wi\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C4 Micropapillary type ductal carcinoma in situ wi\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C5 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C6 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D1 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D3 Invasive ductal carcinoma and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D4 Invasive ductal carcinoma\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D5 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D6 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E1 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E3 Invasive ductal carcinoma and breast tissue\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E4 Invasive ductal carcinoma\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E5 Ductal carcinoma in situ with early infiltratio\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E6 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_A1 Ductal carcinoma in situ and breast tissue\n",
      "br301_6_collagen1_647_hoechst_A2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_A3 Ductal carcinoma in situ\n",
      "br301_6_collagen1_647_hoechst_A4 Ductal carcinoma in situ\n",
      "br301_6_collagen1_647_hoechst_A5 Ductal carcinoma in situ and breast tissue\n",
      "br301_6_collagen1_647_hoechst_A6 Ductal carcinoma in situ and breast tissue\n",
      "br301_6_collagen1_647_hoechst_B1 Invasive ductal carcinoma\n",
      "br301_6_collagen1_647_hoechst_B2 Ductal carcinoma in situ and breast tissue\n",
      "br301_6_collagen1_647_hoechst_B3 Ductal carcinoma in situ and breast tissue\n",
      "br301_6_collagen1_647_hoechst_B4 Ductal carcinoma in situ and breast tissue\n",
      "br301_6_collagen1_647_hoechst_B5 Invasive ductal carcinoma and breast tissue\n",
      "br301_6_collagen1_647_hoechst_B6 Invasive ductal carcinoma and breast tissue\n",
      "br301_6_collagen1_647_hoechst_C1 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_C2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_C3 Micropapillary type ductal carcinoma in situ wi\n",
      "br301_6_collagen1_647_hoechst_C4 Micropapillary type ductal carcinoma in situ wi\n",
      "br301_6_collagen1_647_hoechst_C5 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_C6 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_D1 Ductal carcinoma in situ with early infiltratio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br301_6_collagen1_647_hoechst_D2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_D3 Invasive ductal carcinoma and breast tissue\n",
      "br301_6_collagen1_647_hoechst_D4 Invasive ductal carcinoma\n",
      "br301_6_collagen1_647_hoechst_D5 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_D6 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_E1 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_E2 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_E3 Invasive ductal carcinoma and breast tissue\n",
      "br301_6_collagen1_647_hoechst_E4 Invasive ductal carcinoma\n",
      "br301_6_collagen1_647_hoechst_E5 Ductal carcinoma in situ with early infiltratio\n",
      "br301_6_collagen1_647_hoechst_E6 Ductal carcinoma in situ with early infiltratio\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A1 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A10 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A2 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A3 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A4 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A5 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A6 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A7 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A8 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A9 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B1 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B10 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B2 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B3 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B4 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B5 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B6 Invasive ductal carcinoma (breast tissue)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B7 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B8 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F1 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F10 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F2 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F3 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F4 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F5 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F6 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F7 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F8 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F9 Invasive ductal carcinoma\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H1 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H2 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H3 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H4 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H5 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H6 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H7 Cancer adjacent normal breast tissue\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H8 Cancer adjacent normal breast tissue\n",
      "br8018a_3_collagen1_647_hoechst_A1 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A2 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A3 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A4 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A6 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A7 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A8 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_A9 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B1 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B10 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B2 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B3 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B4 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B5 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B6 Invasive ductal carcinoma (breast tissue)\n",
      "br8018a_3_collagen1_647_hoechst_B7 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B8 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_B9 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F10 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F3 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F4 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F6 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F7 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F8 Invasive ductal carcinoma\n",
      "br8018a_3_collagen1_647_hoechst_F9 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A2 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A3 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A5 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A7 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A8 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B4 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B5 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B6 Invasive ductal carcinoma (breast tissue)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B7 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F3 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F4 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F7 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F8 Invasive ductal carcinoma\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H1 Cancer adjacent normal breast tissue\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H3 Cancer adjacent normal breast tissue\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H5 Cancer adjacent normal breast tissue\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H6 Cancer adjacent normal breast tissue\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H8 Cancer adjacent normal breast tissue\n"
     ]
    }
   ],
   "source": [
    "progNames,progCounts=np.unique(progList,return_counts=True)\n",
    "progSampleRate={}\n",
    "for p in range(progNames.size):\n",
    "    progSampleRate[progNames[p]]=np.min(progCounts)/progCounts[p]\n",
    "    \n",
    "ep=311\n",
    "np.random.seed(6)\n",
    "# plotPCT=4500\n",
    "plottingIdx_i=np.array([])\n",
    "n_pcs=50\n",
    "uniqueImgNames,imgNameIdx=np.unique(allImgNames,return_index=True)\n",
    "for i in range(1):\n",
    "    for sidx in range(uniqueImgNames.size):\n",
    "        s=uniqueImgNames[sidx]\n",
    "        p=progList[imgNameIdx[sidx]]\n",
    "        print(s+' '+p)\n",
    "        nsamples=int(np.sum(allImgNames==s)*progSampleRate[p])\n",
    "        plottingIdx_i=np.concatenate((plottingIdx_i,\n",
    "                                    np.random.choice(np.arange(allImgNames.shape[0])[allImgNames==s],nsamples,replace=False)))\n",
    "    \n",
    "ncluster=8\n",
    "savenamecluster='minibatchkmean_ncluster'+str(ncluster)+'n_pcs'+str(n_pcs)+'epoch'+str(ep)+'_plottingIdx_progBalanced_'+str(i)\n",
    "with open(os.path.join(clustersavedir,savenamecluster), 'rb') as output:\n",
    "    clusterRes=pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir,'processed','train_cnnvae_cellLabels'), 'rb') as output:\n",
    "    cellIDlist=pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A1\n",
      "(380, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A2\n",
      "(508, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A4\n",
      "(360, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A5\n",
      "(358, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A6\n",
      "(525, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A7\n",
      "(314, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A8\n",
      "(862, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_A9\n",
      "(484, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C1\n",
      "(2997, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C10\n",
      "(773, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C2\n",
      "(1286, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C3\n",
      "(1131, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C4\n",
      "(2694, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C5\n",
      "(954, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C6\n",
      "(780, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C7\n",
      "(667, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C8\n",
      "(392, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_C9\n",
      "(1112, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I1\n",
      "(1287, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I10\n",
      "(1495, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I2\n",
      "(1610, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I3\n",
      "(1532, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I7\n",
      "(1385, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I8\n",
      "(1789, 201)\n",
      "br1003a_1_cytokeratin_555_aSMA_647_hoechst_I9\n",
      "(856, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A1\n",
      "(459, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A2\n",
      "(664, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A3\n",
      "(475, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A5\n",
      "(507, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A6\n",
      "(361, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A7\n",
      "(716, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A8\n",
      "(481, 201)\n",
      "br1003a_3_collagen1_647_hoechst_A9\n",
      "(274, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C1\n",
      "(2720, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C10\n",
      "(779, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C2\n",
      "(1286, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C3\n",
      "(1172, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C4\n",
      "(2233, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C5\n",
      "(1350, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C6\n",
      "(1136, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C7\n",
      "(890, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C8\n",
      "(556, 201)\n",
      "br1003a_3_collagen1_647_hoechst_C9\n",
      "(824, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I1\n",
      "(1003, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I10\n",
      "(1564, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I2\n",
      "(1201, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I3\n",
      "(1379, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I7\n",
      "(1300, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I8\n",
      "(1609, 201)\n",
      "br1003a_3_collagen1_647_hoechst_I9\n",
      "(853, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A1\n",
      "(253, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A10\n",
      "(361, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A2\n",
      "(336, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A3\n",
      "(436, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A4\n",
      "(188, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A5\n",
      "(512, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A6\n",
      "(235, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A7\n",
      "(545, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A8\n",
      "(451, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_A9\n",
      "(138, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C1\n",
      "(2711, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C10\n",
      "(939, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C2\n",
      "(1111, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C4\n",
      "(1151, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C5\n",
      "(2269, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C6\n",
      "(920, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C7\n",
      "(1025, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C8\n",
      "(282, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_C9\n",
      "(1098, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I1\n",
      "(937, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I2\n",
      "(1149, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I3\n",
      "(1117, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I7\n",
      "(552, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I8\n",
      "(928, 201)\n",
      "br1003a_4_cytokeratin_555_gh2ax_647_hoechst_I9\n",
      "(670, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A1\n",
      "(3318, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A2\n",
      "(1504, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A3\n",
      "(9225, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A4\n",
      "(3724, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A5\n",
      "(915, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_A6\n",
      "(1187, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B1\n",
      "(241, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B2\n",
      "(2916, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B3\n",
      "(2204, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B4\n",
      "(2548, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B5\n",
      "(2173, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_B6\n",
      "(2702, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C1\n",
      "(851, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C2\n",
      "(1305, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C3\n",
      "(8036, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C4\n",
      "(4798, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C5\n",
      "(1260, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_C6\n",
      "(1578, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D1\n",
      "(634, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D2\n",
      "(617, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D3\n",
      "(1573, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D4\n",
      "(509, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D5\n",
      "(1106, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_D6\n",
      "(751, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E1\n",
      "(794, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E2\n",
      "(692, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E3\n",
      "(4444, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E4\n",
      "(887, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E5\n",
      "(784, 201)\n",
      "br301_4_cytokeratin_555_aSMA_647_hoechst_E6\n",
      "(674, 201)\n",
      "br301_6_collagen1_647_hoechst_A1\n",
      "(3422, 201)\n",
      "br301_6_collagen1_647_hoechst_A2\n",
      "(1052, 201)\n",
      "br301_6_collagen1_647_hoechst_A3\n",
      "(10088, 201)\n",
      "br301_6_collagen1_647_hoechst_A4\n",
      "(1184, 201)\n",
      "br301_6_collagen1_647_hoechst_A5\n",
      "(564, 201)\n",
      "br301_6_collagen1_647_hoechst_A6\n",
      "(1005, 201)\n",
      "br301_6_collagen1_647_hoechst_B1\n",
      "(342, 201)\n",
      "br301_6_collagen1_647_hoechst_B2\n",
      "(1725, 201)\n",
      "br301_6_collagen1_647_hoechst_B3\n",
      "(2325, 201)\n",
      "br301_6_collagen1_647_hoechst_B4\n",
      "(2088, 201)\n",
      "br301_6_collagen1_647_hoechst_B5\n",
      "(3392, 201)\n",
      "br301_6_collagen1_647_hoechst_B6\n",
      "(4106, 201)\n",
      "br301_6_collagen1_647_hoechst_C1\n",
      "(696, 201)\n",
      "br301_6_collagen1_647_hoechst_C2\n",
      "(1148, 201)\n",
      "br301_6_collagen1_647_hoechst_C3\n",
      "(5698, 201)\n",
      "br301_6_collagen1_647_hoechst_C4\n",
      "(5690, 201)\n",
      "br301_6_collagen1_647_hoechst_C5\n",
      "(664, 201)\n",
      "br301_6_collagen1_647_hoechst_C6\n",
      "(1198, 201)\n",
      "br301_6_collagen1_647_hoechst_D1\n",
      "(966, 201)\n",
      "br301_6_collagen1_647_hoechst_D2\n",
      "(699, 201)\n",
      "br301_6_collagen1_647_hoechst_D3\n",
      "(2081, 201)\n",
      "br301_6_collagen1_647_hoechst_D4\n",
      "(570, 201)\n",
      "br301_6_collagen1_647_hoechst_D5\n",
      "(1723, 201)\n",
      "br301_6_collagen1_647_hoechst_D6\n",
      "(650, 201)\n",
      "br301_6_collagen1_647_hoechst_E1\n",
      "(1006, 201)\n",
      "br301_6_collagen1_647_hoechst_E2\n",
      "(490, 201)\n",
      "br301_6_collagen1_647_hoechst_E3\n",
      "(3749, 201)\n",
      "br301_6_collagen1_647_hoechst_E4\n",
      "(918, 201)\n",
      "br301_6_collagen1_647_hoechst_E5\n",
      "(667, 201)\n",
      "br301_6_collagen1_647_hoechst_E6\n",
      "(704, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A1\n",
      "(212, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A10\n",
      "(135, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A2\n",
      "(252, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A3\n",
      "(457, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A4\n",
      "(356, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A5\n",
      "(165, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A6\n",
      "(416, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A7\n",
      "(537, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A8\n",
      "(203, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_A9\n",
      "(278, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B1\n",
      "(275, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B10\n",
      "(31, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B2\n",
      "(379, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B3\n",
      "(461, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B4\n",
      "(426, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B5\n",
      "(412, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B6\n",
      "(8109, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B7\n",
      "(299, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_B8\n",
      "(233, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F10\n",
      "(312, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F2\n",
      "(302, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F3\n",
      "(227, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F4\n",
      "(395, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F5\n",
      "(169, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F6\n",
      "(289, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F7\n",
      "(308, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F8\n",
      "(472, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_F9\n",
      "(358, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H1\n",
      "(816, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H2\n",
      "(3293, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H3\n",
      "(2464, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H4\n",
      "(1920, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H5\n",
      "(1439, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H6\n",
      "(2562, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H7\n",
      "(1716, 201)\n",
      "br8018a_1_cytokeratin_555_aSMA_647_hoechst_H8\n",
      "(2838, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A1\n",
      "(177, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A2\n",
      "(242, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A3\n",
      "(242, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A4\n",
      "(274, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A6\n",
      "(270, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A7\n",
      "(460, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A8\n",
      "(199, 201)\n",
      "br8018a_3_collagen1_647_hoechst_A9\n",
      "(182, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B1\n",
      "(283, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B10\n",
      "(309, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B2\n",
      "(389, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B3\n",
      "(522, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B4\n",
      "(475, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B5\n",
      "(355, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B6\n",
      "(7843, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B7\n",
      "(351, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B8\n",
      "(230, 201)\n",
      "br8018a_3_collagen1_647_hoechst_B9\n",
      "(339, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F10\n",
      "(467, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F3\n",
      "(202, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F4\n",
      "(362, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F6\n",
      "(297, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F7\n",
      "(341, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F8\n",
      "(476, 201)\n",
      "br8018a_3_collagen1_647_hoechst_F9\n",
      "(419, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A2\n",
      "(259, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A3\n",
      "(453, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A5\n",
      "(150, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A7\n",
      "(488, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_A8\n",
      "(211, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B4\n",
      "(484, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B5\n",
      "(276, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B6\n",
      "(8272, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_B7\n",
      "(356, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F3\n",
      "(228, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F4\n",
      "(409, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F7\n",
      "(308, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_F8\n",
      "(605, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H1\n",
      "(1764, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H3\n",
      "(1925, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H5\n",
      "(311, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H6\n",
      "(2234, 201)\n",
      "br8018a_4_cytokeratin_555_gh2ax_647_hoechst_H8\n",
      "(936, 201)\n"
     ]
    }
   ],
   "source": [
    "#load NMCO\n",
    "uniquenames,nameIdx=np.unique(allImgNames,return_index=True)\n",
    "\n",
    "allstats=None\n",
    "alllabels=None\n",
    "allvarnames=None\n",
    "for sidx in range(uniquenames.size):\n",
    "    s=np.unique(allImgNames)[sidx]\n",
    "\n",
    "    path_s=os.path.join(datadir,'_'.join(s.split('_')[:-1]),'nmco_features',s.split('_')[-1] +'.csv')\n",
    "    if not os.path.exists(path_s):\n",
    "        print('DNE '+path_s)\n",
    "    print(s)\n",
    "    plottingIdx_i_s=plottingIdx_i.astype(int)[allImgNames[plottingIdx_i.astype(int)]==s]-nameIdx[sidx]\n",
    "    assert np.min(plottingIdx_i_s)>=0\n",
    "\n",
    "    stats_s=pd.read_csv(path_s)\n",
    "    stats_s.index=stats_s.loc[:,'label']\n",
    "    stats_s=stats_s.loc[cellIDlist[s][plottingIdx_i_s]].to_numpy()[:,2:-2]\n",
    "    print(stats_s.shape)\n",
    "\n",
    "\n",
    "#         ssplit=s.split('_')\n",
    "    slabels=clusterRes[allImgNames[plottingIdx_i.astype(int)]==s]\n",
    "\n",
    "    if allstats is None:\n",
    "        allstats=stats_s\n",
    "        alllabels=np.copy(slabels)\n",
    "    else:\n",
    "        allstats=np.concatenate((allstats,stats_s),axis=0)\n",
    "        alllabels=np.concatenate((alllabels,np.copy(slabels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "np.random.seed(3)\n",
    "pctVal=0.05\n",
    "pctTest=0.1\n",
    "allIdx=np.arange(allstats.shape[0])\n",
    "np.random.shuffle(allIdx)\n",
    "valIdx=allIdx[:int(pctVal*allstats.shape[0])]\n",
    "testIdx=allIdx[int(pctVal*allstats.shape[0]):(int(pctVal*allstats.shape[0])+int(pctTest*allstats.shape[0]))]\n",
    "trainIdx=allIdx[(int(pctVal*allstats.shape[0])+int(pctTest*allstats.shape[0])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainLabels,traincounts=np.unique(alllabels[trainIdx],return_counts=True)\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(trainLabels,traincounts)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightClf=np.zeros(traincounts.size)\n",
    "for i in range(traincounts.size):\n",
    "    weightClf[i]=np.sum(traincounts)/traincounts[i]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=3\n",
    "epochs=6000\n",
    "saveFreq=200\n",
    "lr=0.001 #initial learning rate\n",
    "weight_decay=0 #Weight for L2 loss on embedding matrix.\n",
    "\n",
    "# batchsize=4\n",
    "batchsize=8000\n",
    "model_str='fc3'\n",
    "\n",
    "kernel_size=4\n",
    "stride=2\n",
    "padding=1\n",
    "\n",
    "fc_dim1=128\n",
    "fc_dim2=128\n",
    "fc_dim3=128\n",
    "\n",
    "\n",
    "dropout=0.01\n",
    "kl_weight=0.0000001\n",
    "\n",
    "name='exp0_clusterClf_nmco_'+savenamecluster+'fcl3'\n",
    "logsavepath='/media/xinyi/dcis2idc/log/cnnvae'+name\n",
    "modelsavepath='/media/xinyi/dcis2idc/models/cnnvae'+name\n",
    "plotsavepath='/media/xinyi/dcis2idc/plots/cnnvae'+name\n",
    "\n",
    "\n",
    "if not os.path.exists(logsavepath):\n",
    "    os.mkdir(logsavepath)\n",
    "if not os.path.exists(modelsavepath):\n",
    "    os.mkdir(modelsavepath)\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.mkdir(plotsavepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "seed=3\n",
    "torch.manual_seed(seed)\n",
    "nclasses=np.unique(alllabels).size\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "if model_str=='fc3':\n",
    "    model = modelsCNN.FC_l3(allstats.shape[1],fc_dim1,fc_dim2,fc_dim3,nclasses,0.5,regrs=False)\n",
    "    lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weightClf).cuda().float())\n",
    "if model_str=='fc5':\n",
    "    model = modelsCNN.FC_l5(allstats.shape[1],fc_dim1,fc_dim2,fc_dim3,fc_dim4,fc_dim5,nclasses,0.5,regrs=False)\n",
    "    lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weightClf).cuda().float())\n",
    "if model_str=='fc1':\n",
    "    model = modelsCNN.FC_l1(allstats.shape[1],fc_dim1,nclasses,regrs=False)\n",
    "    lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weightClf).cuda().float())\n",
    "if model_str=='fc0':\n",
    "    model = modelsCNN.FC_l0(allstats.shape[1],nclasses,regrs=False)\n",
    "    lossCE=torch.nn.CrossEntropyLoss(torch.tensor(weightClf).cuda().float())\n",
    "\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstats=allstats.astype(float)\n",
    "allstats=scipy.stats.zscore(allstats,axis=0,nan_policy='omit')\n",
    "allstats=np.nan_to_num(allstats,nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 loss_train: 1.6300 loss_val: 1.0792\n",
      "Epoch: 0001 loss_train: 0.8536 loss_val: 0.3962\n",
      "Epoch: 0002 loss_train: 0.4799 loss_val: 0.2305\n",
      "Epoch: 0003 loss_train: 0.3736 loss_val: 0.1922\n",
      "Epoch: 0004 loss_train: 0.3266 loss_val: 0.1767\n",
      "Epoch: 0005 loss_train: 0.2999 loss_val: 0.1592\n",
      "Epoch: 0006 loss_train: 0.2760 loss_val: 0.1486\n",
      "Epoch: 0007 loss_train: 0.2586 loss_val: 0.1413\n",
      "Epoch: 0008 loss_train: 0.2470 loss_val: 0.1375\n",
      "Epoch: 0009 loss_train: 0.2363 loss_val: 0.1293\n",
      "Epoch: 0010 loss_train: 0.2284 loss_val: 0.1259\n",
      "Epoch: 0011 loss_train: 0.2210 loss_val: 0.1225\n",
      "Epoch: 0012 loss_train: 0.2101 loss_val: 0.1177\n",
      "Epoch: 0013 loss_train: 0.2055 loss_val: 0.1158\n",
      "Epoch: 0014 loss_train: 0.2013 loss_val: 0.1138\n",
      "Epoch: 0015 loss_train: 0.1936 loss_val: 0.1120\n",
      "Epoch: 0016 loss_train: 0.1907 loss_val: 0.1096\n",
      "Epoch: 0017 loss_train: 0.1851 loss_val: 0.1075\n",
      "Epoch: 0018 loss_train: 0.1818 loss_val: 0.1060\n",
      "Epoch: 0019 loss_train: 0.1775 loss_val: 0.1025\n",
      "Epoch: 0020 loss_train: 0.1739 loss_val: 0.1020\n",
      "Epoch: 0021 loss_train: 0.1709 loss_val: 0.1033\n",
      "Epoch: 0022 loss_train: 0.1664 loss_val: 0.0991\n",
      "Epoch: 0023 loss_train: 0.1644 loss_val: 0.0974\n",
      "Epoch: 0024 loss_train: 0.1618 loss_val: 0.0965\n",
      "Epoch: 0025 loss_train: 0.1592 loss_val: 0.0958\n",
      "Epoch: 0026 loss_train: 0.1577 loss_val: 0.0946\n",
      "Epoch: 0027 loss_train: 0.1540 loss_val: 0.0939\n",
      "Epoch: 0028 loss_train: 0.1537 loss_val: 0.0951\n",
      "Epoch: 0029 loss_train: 0.1500 loss_val: 0.0909\n",
      "Epoch: 0030 loss_train: 0.1487 loss_val: 0.0916\n",
      "Epoch: 0031 loss_train: 0.1460 loss_val: 0.0899\n",
      "Epoch: 0032 loss_train: 0.1459 loss_val: 0.0890\n",
      "Epoch: 0033 loss_train: 0.1432 loss_val: 0.0877\n",
      "Epoch: 0034 loss_train: 0.1435 loss_val: 0.0895\n",
      "Epoch: 0035 loss_train: 0.1389 loss_val: 0.0865\n",
      "Epoch: 0036 loss_train: 0.1401 loss_val: 0.0858\n",
      "Epoch: 0037 loss_train: 0.1364 loss_val: 0.0841\n",
      "Epoch: 0038 loss_train: 0.1352 loss_val: 0.0845\n",
      "Epoch: 0039 loss_train: 0.1343 loss_val: 0.0840\n",
      "Epoch: 0040 loss_train: 0.1341 loss_val: 0.0836\n",
      "Epoch: 0041 loss_train: 0.1336 loss_val: 0.0826\n",
      "Epoch: 0042 loss_train: 0.1319 loss_val: 0.0819\n",
      "Epoch: 0043 loss_train: 0.1302 loss_val: 0.0819\n",
      "Epoch: 0044 loss_train: 0.1295 loss_val: 0.0797\n",
      "Epoch: 0045 loss_train: 0.1285 loss_val: 0.0832\n",
      "Epoch: 0046 loss_train: 0.1284 loss_val: 0.0803\n",
      "Epoch: 0047 loss_train: 0.1272 loss_val: 0.0794\n",
      "Epoch: 0048 loss_train: 0.1270 loss_val: 0.0787\n",
      "Epoch: 0049 loss_train: 0.1260 loss_val: 0.0791\n",
      "Epoch: 0050 loss_train: 0.1244 loss_val: 0.0775\n",
      "Epoch: 0051 loss_train: 0.1236 loss_val: 0.0758\n",
      "Epoch: 0052 loss_train: 0.1226 loss_val: 0.0764\n",
      "Epoch: 0053 loss_train: 0.1227 loss_val: 0.0775\n",
      "Epoch: 0054 loss_train: 0.1223 loss_val: 0.0775\n",
      "Epoch: 0055 loss_train: 0.1211 loss_val: 0.0767\n",
      "Epoch: 0056 loss_train: 0.1213 loss_val: 0.0760\n",
      "Epoch: 0057 loss_train: 0.1192 loss_val: 0.0770\n",
      "Epoch: 0058 loss_train: 0.1197 loss_val: 0.0763\n",
      "Epoch: 0059 loss_train: 0.1177 loss_val: 0.0757\n",
      "Epoch: 0060 loss_train: 0.1161 loss_val: 0.0746\n",
      "Epoch: 0061 loss_train: 0.1182 loss_val: 0.0736\n",
      "Epoch: 0062 loss_train: 0.1173 loss_val: 0.0722\n",
      "Epoch: 0063 loss_train: 0.1152 loss_val: 0.0755\n",
      "Epoch: 0064 loss_train: 0.1169 loss_val: 0.0755\n",
      "Epoch: 0065 loss_train: 0.1154 loss_val: 0.0729\n",
      "Epoch: 0066 loss_train: 0.1136 loss_val: 0.0736\n",
      "Epoch: 0067 loss_train: 0.1130 loss_val: 0.0740\n",
      "Epoch: 0068 loss_train: 0.1133 loss_val: 0.0744\n",
      "Epoch: 0069 loss_train: 0.1120 loss_val: 0.0737\n",
      "Epoch: 0070 loss_train: 0.1119 loss_val: 0.0733\n",
      "Epoch: 0071 loss_train: 0.1125 loss_val: 0.0722\n",
      "Epoch: 0072 loss_train: 0.1127 loss_val: 0.0729\n",
      "Epoch: 0073 loss_train: 0.1120 loss_val: 0.0719\n",
      "Epoch: 0074 loss_train: 0.1102 loss_val: 0.0741\n",
      "Epoch: 0075 loss_train: 0.1106 loss_val: 0.0713\n",
      "Epoch: 0076 loss_train: 0.1091 loss_val: 0.0709\n",
      "Epoch: 0077 loss_train: 0.1100 loss_val: 0.0712\n",
      "Epoch: 0078 loss_train: 0.1095 loss_val: 0.0703\n",
      "Epoch: 0079 loss_train: 0.1097 loss_val: 0.0707\n",
      "Epoch: 0080 loss_train: 0.1098 loss_val: 0.0713\n",
      "Epoch: 0081 loss_train: 0.1091 loss_val: 0.0692\n",
      "Epoch: 0082 loss_train: 0.1077 loss_val: 0.0727\n",
      "Epoch: 0083 loss_train: 0.1067 loss_val: 0.0700\n",
      "Epoch: 0084 loss_train: 0.1073 loss_val: 0.0688\n",
      "Epoch: 0085 loss_train: 0.1078 loss_val: 0.0713\n",
      "Epoch: 0086 loss_train: 0.1076 loss_val: 0.0693\n",
      "Epoch: 0087 loss_train: 0.1059 loss_val: 0.0692\n",
      "Epoch: 0088 loss_train: 0.1086 loss_val: 0.0716\n",
      "Epoch: 0089 loss_train: 0.1073 loss_val: 0.0710\n",
      "Epoch: 0090 loss_train: 0.1054 loss_val: 0.0686\n",
      "Epoch: 0091 loss_train: 0.1039 loss_val: 0.0700\n",
      "Epoch: 0092 loss_train: 0.1050 loss_val: 0.0702\n",
      "Epoch: 0093 loss_train: 0.1048 loss_val: 0.0693\n",
      "Epoch: 0094 loss_train: 0.1029 loss_val: 0.0692\n",
      "Epoch: 0095 loss_train: 0.1037 loss_val: 0.0698\n",
      "Epoch: 0096 loss_train: 0.1043 loss_val: 0.0694\n",
      "Epoch: 0097 loss_train: 0.1045 loss_val: 0.0690\n",
      "Epoch: 0098 loss_train: 0.1036 loss_val: 0.0685\n",
      "Epoch: 0099 loss_train: 0.1027 loss_val: 0.0684\n",
      "Epoch: 0100 loss_train: 0.1027 loss_val: 0.0690\n",
      "Epoch: 0101 loss_train: 0.1040 loss_val: 0.0710\n",
      "Epoch: 0102 loss_train: 0.1043 loss_val: 0.0696\n",
      "Epoch: 0103 loss_train: 0.1028 loss_val: 0.0691\n",
      "Epoch: 0104 loss_train: 0.1027 loss_val: 0.0680\n",
      "Epoch: 0105 loss_train: 0.1026 loss_val: 0.0670\n",
      "Epoch: 0106 loss_train: 0.1024 loss_val: 0.0692\n",
      "Epoch: 0107 loss_train: 0.1011 loss_val: 0.0730\n",
      "Epoch: 0108 loss_train: 0.1030 loss_val: 0.0692\n",
      "Epoch: 0109 loss_train: 0.1012 loss_val: 0.0679\n",
      "Epoch: 0110 loss_train: 0.1015 loss_val: 0.0678\n",
      "Epoch: 0111 loss_train: 0.1005 loss_val: 0.0682\n",
      "Epoch: 0112 loss_train: 0.1013 loss_val: 0.0705\n",
      "Epoch: 0113 loss_train: 0.1034 loss_val: 0.0686\n",
      "Epoch: 0114 loss_train: 0.1014 loss_val: 0.0695\n",
      "Epoch: 0115 loss_train: 0.1004 loss_val: 0.0702\n",
      "Epoch: 0116 loss_train: 0.1011 loss_val: 0.0681\n",
      "Epoch: 0117 loss_train: 0.1003 loss_val: 0.0688\n",
      "Epoch: 0118 loss_train: 0.1008 loss_val: 0.0709\n",
      "Epoch: 0119 loss_train: 0.1001 loss_val: 0.0691\n",
      "Epoch: 0120 loss_train: 0.0987 loss_val: 0.0675\n",
      "Epoch: 0121 loss_train: 0.0996 loss_val: 0.0677\n",
      "Epoch: 0122 loss_train: 0.0984 loss_val: 0.0682\n",
      "Epoch: 0123 loss_train: 0.0992 loss_val: 0.0671\n",
      "Epoch: 0124 loss_train: 0.0975 loss_val: 0.0672\n",
      "Epoch: 0125 loss_train: 0.0993 loss_val: 0.0678\n",
      "Epoch: 0126 loss_train: 0.0983 loss_val: 0.0717\n",
      "Epoch: 0127 loss_train: 0.0977 loss_val: 0.0656\n",
      "Epoch: 0128 loss_train: 0.0995 loss_val: 0.0688\n",
      "Epoch: 0129 loss_train: 0.0986 loss_val: 0.0676\n",
      "Epoch: 0130 loss_train: 0.0967 loss_val: 0.0703\n",
      "Epoch: 0131 loss_train: 0.0983 loss_val: 0.0658\n",
      "Epoch: 0132 loss_train: 0.0977 loss_val: 0.0667\n",
      "Epoch: 0133 loss_train: 0.0967 loss_val: 0.0657\n",
      "Epoch: 0134 loss_train: 0.0977 loss_val: 0.0659\n",
      "Epoch: 0135 loss_train: 0.0964 loss_val: 0.0647\n",
      "Epoch: 0136 loss_train: 0.0974 loss_val: 0.0657\n",
      "Epoch: 0137 loss_train: 0.0965 loss_val: 0.0662\n",
      "Epoch: 0138 loss_train: 0.0963 loss_val: 0.0665\n",
      "Epoch: 0139 loss_train: 0.0957 loss_val: 0.0657\n",
      "Epoch: 0140 loss_train: 0.0958 loss_val: 0.0653\n",
      "Epoch: 0141 loss_train: 0.0957 loss_val: 0.0659\n",
      "Epoch: 0142 loss_train: 0.0964 loss_val: 0.0644\n",
      "Epoch: 0143 loss_train: 0.0947 loss_val: 0.0670\n",
      "Epoch: 0144 loss_train: 0.0964 loss_val: 0.0650\n",
      "Epoch: 0145 loss_train: 0.0942 loss_val: 0.0648\n",
      "Epoch: 0146 loss_train: 0.0970 loss_val: 0.0664\n",
      "Epoch: 0147 loss_train: 0.0954 loss_val: 0.0652\n",
      "Epoch: 0148 loss_train: 0.0954 loss_val: 0.0652\n",
      "Epoch: 0149 loss_train: 0.0956 loss_val: 0.0655\n",
      "Epoch: 0150 loss_train: 0.0941 loss_val: 0.0627\n",
      "Epoch: 0151 loss_train: 0.0941 loss_val: 0.0657\n",
      "Epoch: 0152 loss_train: 0.0948 loss_val: 0.0635\n",
      "Epoch: 0153 loss_train: 0.0942 loss_val: 0.0644\n",
      "Epoch: 0154 loss_train: 0.0936 loss_val: 0.0645\n",
      "Epoch: 0155 loss_train: 0.0942 loss_val: 0.0659\n",
      "Epoch: 0156 loss_train: 0.0938 loss_val: 0.0653\n",
      "Epoch: 0157 loss_train: 0.0936 loss_val: 0.0631\n",
      "Epoch: 0158 loss_train: 0.0956 loss_val: 0.0642\n",
      "Epoch: 0159 loss_train: 0.0936 loss_val: 0.0643\n",
      "Epoch: 0160 loss_train: 0.0945 loss_val: 0.0669\n",
      "Epoch: 0161 loss_train: 0.0941 loss_val: 0.0667\n",
      "Epoch: 0162 loss_train: 0.0942 loss_val: 0.0646\n",
      "Epoch: 0163 loss_train: 0.0942 loss_val: 0.0640\n",
      "Epoch: 0164 loss_train: 0.0931 loss_val: 0.0656\n",
      "Epoch: 0165 loss_train: 0.0947 loss_val: 0.0654\n",
      "Epoch: 0166 loss_train: 0.0942 loss_val: 0.0646\n",
      "Epoch: 0167 loss_train: 0.0932 loss_val: 0.0636\n",
      "Epoch: 0168 loss_train: 0.0906 loss_val: 0.0638\n",
      "Epoch: 0169 loss_train: 0.0928 loss_val: 0.0665\n",
      "Epoch: 0170 loss_train: 0.0925 loss_val: 0.0644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0171 loss_train: 0.0925 loss_val: 0.0643\n",
      "Epoch: 0172 loss_train: 0.0921 loss_val: 0.0635\n",
      "Epoch: 0173 loss_train: 0.0925 loss_val: 0.0660\n",
      "Epoch: 0174 loss_train: 0.0925 loss_val: 0.0634\n",
      "Epoch: 0175 loss_train: 0.0921 loss_val: 0.0634\n",
      "Epoch: 0176 loss_train: 0.0924 loss_val: 0.0641\n",
      "Epoch: 0177 loss_train: 0.0915 loss_val: 0.0649\n",
      "Epoch: 0178 loss_train: 0.0921 loss_val: 0.0615\n",
      "Epoch: 0179 loss_train: 0.0911 loss_val: 0.0634\n",
      "Epoch: 0180 loss_train: 0.0923 loss_val: 0.0644\n",
      "Epoch: 0181 loss_train: 0.0895 loss_val: 0.0632\n",
      "Epoch: 0182 loss_train: 0.0926 loss_val: 0.0668\n",
      "Epoch: 0183 loss_train: 0.0915 loss_val: 0.0630\n",
      "Epoch: 0184 loss_train: 0.0907 loss_val: 0.0619\n",
      "Epoch: 0185 loss_train: 0.0910 loss_val: 0.0633\n",
      "Epoch: 0186 loss_train: 0.0924 loss_val: 0.0623\n",
      "Epoch: 0187 loss_train: 0.0914 loss_val: 0.0625\n",
      "Epoch: 0188 loss_train: 0.0914 loss_val: 0.0668\n",
      "Epoch: 0189 loss_train: 0.0916 loss_val: 0.0626\n",
      "Epoch: 0190 loss_train: 0.0898 loss_val: 0.0642\n",
      "Epoch: 0191 loss_train: 0.0909 loss_val: 0.0635\n",
      "Epoch: 0192 loss_train: 0.0911 loss_val: 0.0632\n",
      "Epoch: 0193 loss_train: 0.0895 loss_val: 0.0632\n",
      "Epoch: 0194 loss_train: 0.0901 loss_val: 0.0624\n",
      "Epoch: 0195 loss_train: 0.0904 loss_val: 0.0626\n",
      "Epoch: 0196 loss_train: 0.0904 loss_val: 0.0633\n",
      "Epoch: 0197 loss_train: 0.0912 loss_val: 0.0622\n",
      "Epoch: 0198 loss_train: 0.0900 loss_val: 0.0642\n",
      "Epoch: 0199 loss_train: 0.0894 loss_val: 0.0615\n",
      "Epoch: 0200 loss_train: 0.0897 loss_val: 0.0652\n",
      "Epoch: 0201 loss_train: 0.0900 loss_val: 0.0645\n",
      "Epoch: 0202 loss_train: 0.0897 loss_val: 0.0645\n",
      "Epoch: 0203 loss_train: 0.0906 loss_val: 0.0631\n",
      "Epoch: 0204 loss_train: 0.0894 loss_val: 0.0635\n",
      "Epoch: 0205 loss_train: 0.0905 loss_val: 0.0629\n",
      "Epoch: 0206 loss_train: 0.0884 loss_val: 0.0645\n",
      "Epoch: 0207 loss_train: 0.0899 loss_val: 0.0652\n",
      "Epoch: 0208 loss_train: 0.0897 loss_val: 0.0640\n",
      "Epoch: 0209 loss_train: 0.0889 loss_val: 0.0625\n",
      "Epoch: 0210 loss_train: 0.0889 loss_val: 0.0629\n",
      "Epoch: 0211 loss_train: 0.0883 loss_val: 0.0634\n",
      "Epoch: 0212 loss_train: 0.0893 loss_val: 0.0616\n",
      "Epoch: 0213 loss_train: 0.0869 loss_val: 0.0640\n",
      "Epoch: 0214 loss_train: 0.0897 loss_val: 0.0622\n",
      "Epoch: 0215 loss_train: 0.0896 loss_val: 0.0639\n",
      "Epoch: 0216 loss_train: 0.0894 loss_val: 0.0612\n",
      "Epoch: 0217 loss_train: 0.0896 loss_val: 0.0639\n",
      "Epoch: 0218 loss_train: 0.0873 loss_val: 0.0611\n",
      "Epoch: 0219 loss_train: 0.0878 loss_val: 0.0624\n",
      "Epoch: 0220 loss_train: 0.0894 loss_val: 0.0640\n",
      "Epoch: 0221 loss_train: 0.0888 loss_val: 0.0625\n",
      "Epoch: 0222 loss_train: 0.0888 loss_val: 0.0648\n",
      "Epoch: 0223 loss_train: 0.0893 loss_val: 0.0625\n",
      "Epoch: 0224 loss_train: 0.0879 loss_val: 0.0612\n",
      "Epoch: 0225 loss_train: 0.0876 loss_val: 0.0622\n",
      "Epoch: 0226 loss_train: 0.0881 loss_val: 0.0617\n",
      "Epoch: 0227 loss_train: 0.0870 loss_val: 0.0608\n",
      "Epoch: 0228 loss_train: 0.0863 loss_val: 0.0621\n",
      "Epoch: 0229 loss_train: 0.0866 loss_val: 0.0603\n",
      "Epoch: 0230 loss_train: 0.0861 loss_val: 0.0616\n",
      "Epoch: 0231 loss_train: 0.0881 loss_val: 0.0631\n",
      "Epoch: 0232 loss_train: 0.0876 loss_val: 0.0621\n",
      "Epoch: 0233 loss_train: 0.0870 loss_val: 0.0627\n",
      "Epoch: 0234 loss_train: 0.0863 loss_val: 0.0612\n",
      "Epoch: 0235 loss_train: 0.0876 loss_val: 0.0626\n",
      "Epoch: 0236 loss_train: 0.0875 loss_val: 0.0619\n",
      "Epoch: 0237 loss_train: 0.0863 loss_val: 0.0623\n",
      "Epoch: 0238 loss_train: 0.0881 loss_val: 0.0646\n",
      "Epoch: 0239 loss_train: 0.0884 loss_val: 0.0624\n",
      "Epoch: 0240 loss_train: 0.0865 loss_val: 0.0616\n",
      "Epoch: 0241 loss_train: 0.0868 loss_val: 0.0600\n",
      "Epoch: 0242 loss_train: 0.0867 loss_val: 0.0602\n",
      "Epoch: 0243 loss_train: 0.0876 loss_val: 0.0615\n",
      "Epoch: 0244 loss_train: 0.0858 loss_val: 0.0635\n",
      "Epoch: 0245 loss_train: 0.0892 loss_val: 0.0619\n",
      "Epoch: 0246 loss_train: 0.0862 loss_val: 0.0615\n",
      "Epoch: 0247 loss_train: 0.0853 loss_val: 0.0614\n",
      "Epoch: 0248 loss_train: 0.0872 loss_val: 0.0622\n",
      "Epoch: 0249 loss_train: 0.0861 loss_val: 0.0609\n",
      "Epoch: 0250 loss_train: 0.0864 loss_val: 0.0602\n",
      "Epoch: 0251 loss_train: 0.0863 loss_val: 0.0624\n",
      "Epoch: 0252 loss_train: 0.0858 loss_val: 0.0598\n",
      "Epoch: 0253 loss_train: 0.0862 loss_val: 0.0610\n",
      "Epoch: 0254 loss_train: 0.0870 loss_val: 0.0621\n",
      "Epoch: 0255 loss_train: 0.0871 loss_val: 0.0609\n",
      "Epoch: 0256 loss_train: 0.0851 loss_val: 0.0614\n",
      "Epoch: 0257 loss_train: 0.0848 loss_val: 0.0614\n",
      "Epoch: 0258 loss_train: 0.0865 loss_val: 0.0611\n",
      "Epoch: 0259 loss_train: 0.0868 loss_val: 0.0627\n",
      "Epoch: 0260 loss_train: 0.0859 loss_val: 0.0615\n",
      "Epoch: 0261 loss_train: 0.0852 loss_val: 0.0594\n",
      "Epoch: 0262 loss_train: 0.0858 loss_val: 0.0634\n",
      "Epoch: 0263 loss_train: 0.0860 loss_val: 0.0606\n",
      "Epoch: 0264 loss_train: 0.0858 loss_val: 0.0608\n",
      "Epoch: 0265 loss_train: 0.0845 loss_val: 0.0619\n",
      "Epoch: 0266 loss_train: 0.0855 loss_val: 0.0598\n",
      "Epoch: 0267 loss_train: 0.0852 loss_val: 0.0613\n",
      "Epoch: 0268 loss_train: 0.0855 loss_val: 0.0596\n",
      "Epoch: 0269 loss_train: 0.0844 loss_val: 0.0619\n",
      "Epoch: 0270 loss_train: 0.0861 loss_val: 0.0591\n",
      "Epoch: 0271 loss_train: 0.0846 loss_val: 0.0617\n",
      "Epoch: 0272 loss_train: 0.0848 loss_val: 0.0609\n",
      "Epoch: 0273 loss_train: 0.0838 loss_val: 0.0613\n",
      "Epoch: 0274 loss_train: 0.0843 loss_val: 0.0608\n",
      "Epoch: 0275 loss_train: 0.0836 loss_val: 0.0590\n",
      "Epoch: 0276 loss_train: 0.0854 loss_val: 0.0606\n",
      "Epoch: 0277 loss_train: 0.0857 loss_val: 0.0621\n",
      "Epoch: 0278 loss_train: 0.0859 loss_val: 0.0623\n",
      "Epoch: 0279 loss_train: 0.0856 loss_val: 0.0611\n",
      "Epoch: 0280 loss_train: 0.0852 loss_val: 0.0620\n",
      "Epoch: 0281 loss_train: 0.0831 loss_val: 0.0605\n",
      "Epoch: 0282 loss_train: 0.0848 loss_val: 0.0654\n",
      "Epoch: 0283 loss_train: 0.0859 loss_val: 0.0598\n",
      "Epoch: 0284 loss_train: 0.0844 loss_val: 0.0604\n",
      "Epoch: 0285 loss_train: 0.0835 loss_val: 0.0623\n",
      "Epoch: 0286 loss_train: 0.0860 loss_val: 0.0636\n",
      "Epoch: 0287 loss_train: 0.0850 loss_val: 0.0618\n",
      "Epoch: 0288 loss_train: 0.0839 loss_val: 0.0599\n",
      "Epoch: 0289 loss_train: 0.0838 loss_val: 0.0610\n",
      "Epoch: 0290 loss_train: 0.0839 loss_val: 0.0611\n",
      "Epoch: 0291 loss_train: 0.0832 loss_val: 0.0620\n",
      "Epoch: 0292 loss_train: 0.0848 loss_val: 0.0600\n",
      "Epoch: 0293 loss_train: 0.0825 loss_val: 0.0647\n",
      "Epoch: 0294 loss_train: 0.0836 loss_val: 0.0593\n",
      "Epoch: 0295 loss_train: 0.0835 loss_val: 0.0606\n",
      "Epoch: 0296 loss_train: 0.0836 loss_val: 0.0599\n",
      "Epoch: 0297 loss_train: 0.0835 loss_val: 0.0592\n",
      "Epoch: 0298 loss_train: 0.0831 loss_val: 0.0611\n",
      "Epoch: 0299 loss_train: 0.0836 loss_val: 0.0628\n",
      "Epoch: 0300 loss_train: 0.0842 loss_val: 0.0587\n",
      "Epoch: 0301 loss_train: 0.0842 loss_val: 0.0583\n",
      "Epoch: 0302 loss_train: 0.0828 loss_val: 0.0601\n",
      "Epoch: 0303 loss_train: 0.0830 loss_val: 0.0613\n",
      "Epoch: 0304 loss_train: 0.0833 loss_val: 0.0602\n",
      "Epoch: 0305 loss_train: 0.0833 loss_val: 0.0615\n",
      "Epoch: 0306 loss_train: 0.0833 loss_val: 0.0609\n",
      "Epoch: 0307 loss_train: 0.0830 loss_val: 0.0594\n",
      "Epoch: 0308 loss_train: 0.0828 loss_val: 0.0608\n",
      "Epoch: 0309 loss_train: 0.0832 loss_val: 0.0582\n",
      "Epoch: 0310 loss_train: 0.0823 loss_val: 0.0594\n",
      "Epoch: 0311 loss_train: 0.0839 loss_val: 0.0603\n",
      "Epoch: 0312 loss_train: 0.0832 loss_val: 0.0592\n",
      "Epoch: 0313 loss_train: 0.0838 loss_val: 0.0613\n",
      "Epoch: 0314 loss_train: 0.0833 loss_val: 0.0611\n",
      "Epoch: 0315 loss_train: 0.0834 loss_val: 0.0626\n",
      "Epoch: 0316 loss_train: 0.0837 loss_val: 0.0625\n",
      "Epoch: 0317 loss_train: 0.0835 loss_val: 0.0606\n",
      "Epoch: 0318 loss_train: 0.0844 loss_val: 0.0595\n",
      "Epoch: 0319 loss_train: 0.0822 loss_val: 0.0598\n",
      "Epoch: 0320 loss_train: 0.0832 loss_val: 0.0601\n",
      "Epoch: 0321 loss_train: 0.0803 loss_val: 0.0581\n",
      "Epoch: 0322 loss_train: 0.0827 loss_val: 0.0596\n",
      "Epoch: 0323 loss_train: 0.0824 loss_val: 0.0596\n",
      "Epoch: 0324 loss_train: 0.0841 loss_val: 0.0622\n",
      "Epoch: 0325 loss_train: 0.0820 loss_val: 0.0590\n",
      "Epoch: 0326 loss_train: 0.0837 loss_val: 0.0584\n",
      "Epoch: 0327 loss_train: 0.0818 loss_val: 0.0585\n",
      "Epoch: 0328 loss_train: 0.0820 loss_val: 0.0593\n",
      "Epoch: 0329 loss_train: 0.0832 loss_val: 0.0616\n",
      "Epoch: 0330 loss_train: 0.0826 loss_val: 0.0586\n",
      "Epoch: 0331 loss_train: 0.0825 loss_val: 0.0589\n",
      "Epoch: 0332 loss_train: 0.0819 loss_val: 0.0591\n",
      "Epoch: 0333 loss_train: 0.0814 loss_val: 0.0608\n",
      "Epoch: 0334 loss_train: 0.0806 loss_val: 0.0603\n",
      "Epoch: 0335 loss_train: 0.0810 loss_val: 0.0613\n",
      "Epoch: 0336 loss_train: 0.0823 loss_val: 0.0603\n",
      "Epoch: 0337 loss_train: 0.0822 loss_val: 0.0622\n",
      "Epoch: 0338 loss_train: 0.0825 loss_val: 0.0591\n",
      "Epoch: 0339 loss_train: 0.0815 loss_val: 0.0601\n",
      "Epoch: 0340 loss_train: 0.0824 loss_val: 0.0611\n",
      "Epoch: 0341 loss_train: 0.0811 loss_val: 0.0582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0342 loss_train: 0.0823 loss_val: 0.0579\n",
      "Epoch: 0343 loss_train: 0.0805 loss_val: 0.0587\n",
      "Epoch: 0344 loss_train: 0.0816 loss_val: 0.0584\n",
      "Epoch: 0345 loss_train: 0.0809 loss_val: 0.0588\n",
      "Epoch: 0346 loss_train: 0.0807 loss_val: 0.0582\n",
      "Epoch: 0347 loss_train: 0.0821 loss_val: 0.0582\n",
      "Epoch: 0348 loss_train: 0.0811 loss_val: 0.0591\n",
      "Epoch: 0349 loss_train: 0.0810 loss_val: 0.0595\n",
      "Epoch: 0350 loss_train: 0.0807 loss_val: 0.0594\n",
      "Epoch: 0351 loss_train: 0.0808 loss_val: 0.0590\n",
      "Epoch: 0352 loss_train: 0.0823 loss_val: 0.0594\n",
      "Epoch: 0353 loss_train: 0.0819 loss_val: 0.0601\n",
      "Epoch: 0354 loss_train: 0.0797 loss_val: 0.0586\n",
      "Epoch: 0355 loss_train: 0.0804 loss_val: 0.0573\n",
      "Epoch: 0356 loss_train: 0.0802 loss_val: 0.0581\n",
      "Epoch: 0357 loss_train: 0.0813 loss_val: 0.0573\n",
      "Epoch: 0358 loss_train: 0.0808 loss_val: 0.0600\n",
      "Epoch: 0359 loss_train: 0.0798 loss_val: 0.0590\n",
      "Epoch: 0360 loss_train: 0.0809 loss_val: 0.0605\n",
      "Epoch: 0361 loss_train: 0.0809 loss_val: 0.0592\n",
      "Epoch: 0362 loss_train: 0.0807 loss_val: 0.0580\n",
      "Epoch: 0363 loss_train: 0.0803 loss_val: 0.0593\n",
      "Epoch: 0364 loss_train: 0.0810 loss_val: 0.0593\n",
      "Epoch: 0365 loss_train: 0.0818 loss_val: 0.0595\n",
      "Epoch: 0366 loss_train: 0.0811 loss_val: 0.0593\n",
      "Epoch: 0367 loss_train: 0.0798 loss_val: 0.0585\n",
      "Epoch: 0368 loss_train: 0.0796 loss_val: 0.0579\n",
      "Epoch: 0369 loss_train: 0.0826 loss_val: 0.0590\n",
      "Epoch: 0370 loss_train: 0.0798 loss_val: 0.0581\n",
      "Epoch: 0371 loss_train: 0.0808 loss_val: 0.0585\n",
      "Epoch: 0372 loss_train: 0.0808 loss_val: 0.0596\n",
      "Epoch: 0373 loss_train: 0.0809 loss_val: 0.0574\n",
      "Epoch: 0374 loss_train: 0.0800 loss_val: 0.0610\n",
      "Epoch: 0375 loss_train: 0.0801 loss_val: 0.0585\n",
      "Epoch: 0376 loss_train: 0.0794 loss_val: 0.0592\n",
      "Epoch: 0377 loss_train: 0.0801 loss_val: 0.0593\n",
      "Epoch: 0378 loss_train: 0.0815 loss_val: 0.0592\n",
      "Epoch: 0379 loss_train: 0.0804 loss_val: 0.0596\n",
      "Epoch: 0380 loss_train: 0.0799 loss_val: 0.0604\n",
      "Epoch: 0381 loss_train: 0.0808 loss_val: 0.0594\n",
      "Epoch: 0382 loss_train: 0.0808 loss_val: 0.0586\n",
      "Epoch: 0383 loss_train: 0.0815 loss_val: 0.0586\n",
      "Epoch: 0384 loss_train: 0.0789 loss_val: 0.0592\n",
      "Epoch: 0385 loss_train: 0.0792 loss_val: 0.0585\n",
      "Epoch: 0386 loss_train: 0.0794 loss_val: 0.0594\n",
      "Epoch: 0387 loss_train: 0.0803 loss_val: 0.0597\n",
      "Epoch: 0388 loss_train: 0.0792 loss_val: 0.0589\n",
      "Epoch: 0389 loss_train: 0.0801 loss_val: 0.0585\n",
      "Epoch: 0390 loss_train: 0.0806 loss_val: 0.0585\n",
      "Epoch: 0391 loss_train: 0.0794 loss_val: 0.0585\n",
      "Epoch: 0392 loss_train: 0.0798 loss_val: 0.0581\n",
      "Epoch: 0393 loss_train: 0.0793 loss_val: 0.0579\n",
      "Epoch: 0394 loss_train: 0.0805 loss_val: 0.0583\n",
      "Epoch: 0395 loss_train: 0.0790 loss_val: 0.0577\n",
      "Epoch: 0396 loss_train: 0.0795 loss_val: 0.0606\n",
      "Epoch: 0397 loss_train: 0.0789 loss_val: 0.0603\n",
      "Epoch: 0398 loss_train: 0.0796 loss_val: 0.0584\n",
      "Epoch: 0399 loss_train: 0.0790 loss_val: 0.0599\n",
      "Epoch: 0400 loss_train: 0.0791 loss_val: 0.0582\n",
      "Epoch: 0401 loss_train: 0.0791 loss_val: 0.0589\n",
      "Epoch: 0402 loss_train: 0.0791 loss_val: 0.0565\n",
      "Epoch: 0403 loss_train: 0.0801 loss_val: 0.0584\n",
      "Epoch: 0404 loss_train: 0.0777 loss_val: 0.0586\n",
      "Epoch: 0405 loss_train: 0.0803 loss_val: 0.0609\n",
      "Epoch: 0406 loss_train: 0.0796 loss_val: 0.0583\n",
      "Epoch: 0407 loss_train: 0.0799 loss_val: 0.0570\n",
      "Epoch: 0408 loss_train: 0.0782 loss_val: 0.0567\n",
      "Epoch: 0409 loss_train: 0.0786 loss_val: 0.0572\n",
      "Epoch: 0410 loss_train: 0.0792 loss_val: 0.0597\n",
      "Epoch: 0411 loss_train: 0.0778 loss_val: 0.0566\n",
      "Epoch: 0412 loss_train: 0.0790 loss_val: 0.0572\n",
      "Epoch: 0413 loss_train: 0.0794 loss_val: 0.0581\n",
      "Epoch: 0414 loss_train: 0.0800 loss_val: 0.0583\n",
      "Epoch: 0415 loss_train: 0.0777 loss_val: 0.0582\n",
      "Epoch: 0416 loss_train: 0.0787 loss_val: 0.0594\n",
      "Epoch: 0417 loss_train: 0.0798 loss_val: 0.0590\n",
      "Epoch: 0418 loss_train: 0.0784 loss_val: 0.0570\n",
      "Epoch: 0419 loss_train: 0.0783 loss_val: 0.0572\n",
      "Epoch: 0420 loss_train: 0.0780 loss_val: 0.0564\n",
      "Epoch: 0421 loss_train: 0.0787 loss_val: 0.0572\n",
      "Epoch: 0422 loss_train: 0.0783 loss_val: 0.0590\n",
      "Epoch: 0423 loss_train: 0.0780 loss_val: 0.0577\n",
      "Epoch: 0424 loss_train: 0.0786 loss_val: 0.0605\n",
      "Epoch: 0425 loss_train: 0.0794 loss_val: 0.0589\n",
      "Epoch: 0426 loss_train: 0.0781 loss_val: 0.0597\n",
      "Epoch: 0427 loss_train: 0.0781 loss_val: 0.0590\n",
      "Epoch: 0428 loss_train: 0.0793 loss_val: 0.0588\n",
      "Epoch: 0429 loss_train: 0.0776 loss_val: 0.0584\n",
      "Epoch: 0430 loss_train: 0.0781 loss_val: 0.0613\n",
      "Epoch: 0431 loss_train: 0.0797 loss_val: 0.0591\n",
      "Epoch: 0432 loss_train: 0.0784 loss_val: 0.0580\n",
      "Epoch: 0433 loss_train: 0.0787 loss_val: 0.0594\n",
      "Epoch: 0434 loss_train: 0.0774 loss_val: 0.0587\n",
      "Epoch: 0435 loss_train: 0.0775 loss_val: 0.0585\n",
      "Epoch: 0436 loss_train: 0.0788 loss_val: 0.0593\n",
      "Epoch: 0437 loss_train: 0.0778 loss_val: 0.0590\n",
      "Epoch: 0438 loss_train: 0.0784 loss_val: 0.0572\n",
      "Epoch: 0439 loss_train: 0.0778 loss_val: 0.0570\n",
      "Epoch: 0440 loss_train: 0.0767 loss_val: 0.0593\n",
      "Epoch: 0441 loss_train: 0.0777 loss_val: 0.0577\n",
      "Epoch: 0442 loss_train: 0.0783 loss_val: 0.0586\n",
      "Epoch: 0443 loss_train: 0.0776 loss_val: 0.0582\n",
      "Epoch: 0444 loss_train: 0.0787 loss_val: 0.0591\n",
      "Epoch: 0445 loss_train: 0.0780 loss_val: 0.0579\n",
      "Epoch: 0446 loss_train: 0.0767 loss_val: 0.0572\n",
      "Epoch: 0447 loss_train: 0.0768 loss_val: 0.0599\n",
      "Epoch: 0448 loss_train: 0.0781 loss_val: 0.0609\n",
      "Epoch: 0449 loss_train: 0.0777 loss_val: 0.0590\n",
      "Epoch: 0450 loss_train: 0.0764 loss_val: 0.0582\n",
      "Epoch: 0451 loss_train: 0.0777 loss_val: 0.0566\n",
      "Epoch: 0452 loss_train: 0.0784 loss_val: 0.0588\n",
      "Epoch: 0453 loss_train: 0.0780 loss_val: 0.0579\n",
      "Epoch: 0454 loss_train: 0.0775 loss_val: 0.0583\n",
      "Epoch: 0455 loss_train: 0.0787 loss_val: 0.0569\n",
      "Epoch: 0456 loss_train: 0.0775 loss_val: 0.0580\n",
      "Epoch: 0457 loss_train: 0.0776 loss_val: 0.0594\n",
      "Epoch: 0458 loss_train: 0.0783 loss_val: 0.0591\n",
      "Epoch: 0459 loss_train: 0.0764 loss_val: 0.0581\n",
      "Epoch: 0460 loss_train: 0.0788 loss_val: 0.0569\n",
      "Epoch: 0461 loss_train: 0.0787 loss_val: 0.0589\n",
      "Epoch: 0462 loss_train: 0.0778 loss_val: 0.0584\n",
      "Epoch: 0463 loss_train: 0.0775 loss_val: 0.0565\n",
      "Epoch: 0464 loss_train: 0.0758 loss_val: 0.0598\n",
      "Epoch: 0465 loss_train: 0.0775 loss_val: 0.0584\n",
      "Epoch: 0466 loss_train: 0.0769 loss_val: 0.0577\n",
      "Epoch: 0467 loss_train: 0.0770 loss_val: 0.0581\n",
      "Epoch: 0468 loss_train: 0.0780 loss_val: 0.0593\n",
      "Epoch: 0469 loss_train: 0.0781 loss_val: 0.0592\n",
      "Epoch: 0470 loss_train: 0.0767 loss_val: 0.0598\n",
      "Epoch: 0471 loss_train: 0.0762 loss_val: 0.0549\n",
      "Epoch: 0472 loss_train: 0.0768 loss_val: 0.0582\n",
      "Epoch: 0473 loss_train: 0.0774 loss_val: 0.0597\n",
      "Epoch: 0474 loss_train: 0.0782 loss_val: 0.0581\n",
      "Epoch: 0475 loss_train: 0.0766 loss_val: 0.0565\n",
      "Epoch: 0476 loss_train: 0.0771 loss_val: 0.0574\n",
      "Epoch: 0477 loss_train: 0.0765 loss_val: 0.0567\n",
      "Epoch: 0478 loss_train: 0.0760 loss_val: 0.0562\n",
      "Epoch: 0479 loss_train: 0.0763 loss_val: 0.0592\n",
      "Epoch: 0480 loss_train: 0.0771 loss_val: 0.0575\n",
      "Epoch: 0481 loss_train: 0.0777 loss_val: 0.0575\n",
      "Epoch: 0482 loss_train: 0.0763 loss_val: 0.0565\n",
      "Epoch: 0483 loss_train: 0.0761 loss_val: 0.0581\n",
      "Epoch: 0484 loss_train: 0.0765 loss_val: 0.0572\n",
      "Epoch: 0485 loss_train: 0.0770 loss_val: 0.0588\n",
      "Epoch: 0486 loss_train: 0.0775 loss_val: 0.0577\n",
      "Epoch: 0487 loss_train: 0.0767 loss_val: 0.0569\n",
      "Epoch: 0488 loss_train: 0.0753 loss_val: 0.0571\n",
      "Epoch: 0489 loss_train: 0.0767 loss_val: 0.0583\n",
      "Epoch: 0490 loss_train: 0.0772 loss_val: 0.0569\n",
      "Epoch: 0491 loss_train: 0.0767 loss_val: 0.0574\n",
      "Epoch: 0492 loss_train: 0.0765 loss_val: 0.0575\n",
      "Epoch: 0493 loss_train: 0.0766 loss_val: 0.0580\n",
      "Epoch: 0494 loss_train: 0.0766 loss_val: 0.0583\n",
      "Epoch: 0495 loss_train: 0.0769 loss_val: 0.0565\n",
      "Epoch: 0496 loss_train: 0.0751 loss_val: 0.0572\n",
      "Epoch: 0497 loss_train: 0.0764 loss_val: 0.0570\n",
      "Epoch: 0498 loss_train: 0.0762 loss_val: 0.0592\n",
      "Epoch: 0499 loss_train: 0.0777 loss_val: 0.0591\n",
      "Epoch: 0500 loss_train: 0.0749 loss_val: 0.0575\n",
      "Epoch: 0501 loss_train: 0.0758 loss_val: 0.0577\n",
      "Epoch: 0502 loss_train: 0.0758 loss_val: 0.0569\n",
      "Epoch: 0503 loss_train: 0.0767 loss_val: 0.0580\n",
      "Epoch: 0504 loss_train: 0.0767 loss_val: 0.0566\n",
      "Epoch: 0505 loss_train: 0.0750 loss_val: 0.0576\n",
      "Epoch: 0506 loss_train: 0.0766 loss_val: 0.0594\n",
      "Epoch: 0507 loss_train: 0.0768 loss_val: 0.0576\n",
      "Epoch: 0508 loss_train: 0.0761 loss_val: 0.0576\n",
      "Epoch: 0509 loss_train: 0.0756 loss_val: 0.0590\n",
      "Epoch: 0510 loss_train: 0.0762 loss_val: 0.0573\n",
      "Epoch: 0511 loss_train: 0.0773 loss_val: 0.0562\n",
      "Epoch: 0512 loss_train: 0.0752 loss_val: 0.0564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0513 loss_train: 0.0761 loss_val: 0.0573\n",
      "Epoch: 0514 loss_train: 0.0750 loss_val: 0.0567\n",
      "Epoch: 0515 loss_train: 0.0765 loss_val: 0.0568\n",
      "Epoch: 0516 loss_train: 0.0756 loss_val: 0.0573\n",
      "Epoch: 0517 loss_train: 0.0763 loss_val: 0.0583\n",
      "Epoch: 0518 loss_train: 0.0757 loss_val: 0.0583\n",
      "Epoch: 0519 loss_train: 0.0754 loss_val: 0.0568\n",
      "Epoch: 0520 loss_train: 0.0765 loss_val: 0.0606\n",
      "Epoch: 0521 loss_train: 0.0755 loss_val: 0.0574\n",
      "Epoch: 0522 loss_train: 0.0752 loss_val: 0.0573\n",
      "Epoch: 0523 loss_train: 0.0740 loss_val: 0.0581\n",
      "Epoch: 0524 loss_train: 0.0752 loss_val: 0.0565\n",
      "Epoch: 0525 loss_train: 0.0759 loss_val: 0.0577\n",
      "Epoch: 0526 loss_train: 0.0754 loss_val: 0.0559\n",
      "Epoch: 0527 loss_train: 0.0751 loss_val: 0.0581\n",
      "Epoch: 0528 loss_train: 0.0763 loss_val: 0.0563\n",
      "Epoch: 0529 loss_train: 0.0764 loss_val: 0.0578\n",
      "Epoch: 0530 loss_train: 0.0777 loss_val: 0.0571\n",
      "Epoch: 0531 loss_train: 0.0760 loss_val: 0.0564\n",
      "Epoch: 0532 loss_train: 0.0754 loss_val: 0.0563\n",
      "Epoch: 0533 loss_train: 0.0751 loss_val: 0.0568\n",
      "Epoch: 0534 loss_train: 0.0753 loss_val: 0.0581\n",
      "Epoch: 0535 loss_train: 0.0766 loss_val: 0.0575\n",
      "Epoch: 0536 loss_train: 0.0757 loss_val: 0.0551\n",
      "Epoch: 0537 loss_train: 0.0770 loss_val: 0.0570\n",
      "Epoch: 0538 loss_train: 0.0767 loss_val: 0.0564\n",
      "Epoch: 0539 loss_train: 0.0761 loss_val: 0.0596\n",
      "Epoch: 0540 loss_train: 0.0743 loss_val: 0.0564\n",
      "Epoch: 0541 loss_train: 0.0736 loss_val: 0.0574\n",
      "Epoch: 0542 loss_train: 0.0758 loss_val: 0.0579\n",
      "Epoch: 0543 loss_train: 0.0763 loss_val: 0.0589\n",
      "Epoch: 0544 loss_train: 0.0747 loss_val: 0.0572\n",
      "Epoch: 0545 loss_train: 0.0746 loss_val: 0.0574\n",
      "Epoch: 0546 loss_train: 0.0761 loss_val: 0.0565\n",
      "Epoch: 0547 loss_train: 0.0745 loss_val: 0.0566\n",
      "Epoch: 0548 loss_train: 0.0762 loss_val: 0.0571\n",
      "Epoch: 0549 loss_train: 0.0769 loss_val: 0.0563\n",
      "Epoch: 0550 loss_train: 0.0749 loss_val: 0.0568\n",
      "Epoch: 0551 loss_train: 0.0743 loss_val: 0.0553\n",
      "Epoch: 0552 loss_train: 0.0750 loss_val: 0.0562\n",
      "Epoch: 0553 loss_train: 0.0751 loss_val: 0.0569\n",
      "Epoch: 0554 loss_train: 0.0758 loss_val: 0.0553\n",
      "Epoch: 0555 loss_train: 0.0757 loss_val: 0.0562\n",
      "Epoch: 0556 loss_train: 0.0749 loss_val: 0.0583\n",
      "Epoch: 0557 loss_train: 0.0762 loss_val: 0.0565\n",
      "Epoch: 0558 loss_train: 0.0761 loss_val: 0.0568\n",
      "Epoch: 0559 loss_train: 0.0757 loss_val: 0.0577\n",
      "Epoch: 0560 loss_train: 0.0754 loss_val: 0.0554\n",
      "Epoch: 0561 loss_train: 0.0754 loss_val: 0.0553\n",
      "Epoch: 0562 loss_train: 0.0744 loss_val: 0.0567\n",
      "Epoch: 0563 loss_train: 0.0746 loss_val: 0.0551\n",
      "Epoch: 0564 loss_train: 0.0731 loss_val: 0.0556\n",
      "Epoch: 0565 loss_train: 0.0760 loss_val: 0.0569\n",
      "Epoch: 0566 loss_train: 0.0749 loss_val: 0.0572\n",
      "Epoch: 0567 loss_train: 0.0750 loss_val: 0.0562\n",
      "Epoch: 0568 loss_train: 0.0737 loss_val: 0.0562\n",
      "Epoch: 0569 loss_train: 0.0763 loss_val: 0.0566\n",
      "Epoch: 0570 loss_train: 0.0740 loss_val: 0.0545\n",
      "Epoch: 0571 loss_train: 0.0760 loss_val: 0.0563\n",
      "Epoch: 0572 loss_train: 0.0754 loss_val: 0.0570\n",
      "Epoch: 0573 loss_train: 0.0741 loss_val: 0.0552\n",
      "Epoch: 0574 loss_train: 0.0733 loss_val: 0.0565\n",
      "Epoch: 0575 loss_train: 0.0758 loss_val: 0.0554\n",
      "Epoch: 0576 loss_train: 0.0738 loss_val: 0.0557\n",
      "Epoch: 0577 loss_train: 0.0741 loss_val: 0.0560\n",
      "Epoch: 0578 loss_train: 0.0741 loss_val: 0.0551\n",
      "Epoch: 0579 loss_train: 0.0751 loss_val: 0.0545\n",
      "Epoch: 0580 loss_train: 0.0732 loss_val: 0.0571\n",
      "Epoch: 0581 loss_train: 0.0749 loss_val: 0.0563\n",
      "Epoch: 0582 loss_train: 0.0737 loss_val: 0.0582\n",
      "Epoch: 0583 loss_train: 0.0741 loss_val: 0.0559\n",
      "Epoch: 0584 loss_train: 0.0749 loss_val: 0.0581\n",
      "Epoch: 0585 loss_train: 0.0752 loss_val: 0.0553\n",
      "Epoch: 0586 loss_train: 0.0753 loss_val: 0.0555\n",
      "Epoch: 0587 loss_train: 0.0742 loss_val: 0.0559\n",
      "Epoch: 0588 loss_train: 0.0746 loss_val: 0.0568\n",
      "Epoch: 0589 loss_train: 0.0755 loss_val: 0.0566\n",
      "Epoch: 0590 loss_train: 0.0750 loss_val: 0.0561\n",
      "Epoch: 0591 loss_train: 0.0740 loss_val: 0.0561\n",
      "Epoch: 0592 loss_train: 0.0745 loss_val: 0.0544\n",
      "Epoch: 0593 loss_train: 0.0745 loss_val: 0.0576\n",
      "Epoch: 0594 loss_train: 0.0738 loss_val: 0.0562\n",
      "Epoch: 0595 loss_train: 0.0752 loss_val: 0.0565\n",
      "Epoch: 0596 loss_train: 0.0745 loss_val: 0.0553\n",
      "Epoch: 0597 loss_train: 0.0747 loss_val: 0.0565\n",
      "Epoch: 0598 loss_train: 0.0738 loss_val: 0.0580\n",
      "Epoch: 0599 loss_train: 0.0750 loss_val: 0.0575\n",
      "Epoch: 0600 loss_train: 0.0762 loss_val: 0.0584\n",
      "Epoch: 0601 loss_train: 0.0755 loss_val: 0.0587\n",
      "Epoch: 0602 loss_train: 0.0749 loss_val: 0.0559\n",
      "Epoch: 0603 loss_train: 0.0728 loss_val: 0.0562\n",
      "Epoch: 0604 loss_train: 0.0741 loss_val: 0.0572\n",
      "Epoch: 0605 loss_train: 0.0747 loss_val: 0.0576\n",
      "Epoch: 0606 loss_train: 0.0752 loss_val: 0.0561\n",
      "Epoch: 0607 loss_train: 0.0737 loss_val: 0.0554\n",
      "Epoch: 0608 loss_train: 0.0734 loss_val: 0.0561\n",
      "Epoch: 0609 loss_train: 0.0742 loss_val: 0.0563\n",
      "Epoch: 0610 loss_train: 0.0737 loss_val: 0.0560\n",
      "Epoch: 0611 loss_train: 0.0738 loss_val: 0.0548\n",
      "Epoch: 0612 loss_train: 0.0736 loss_val: 0.0561\n",
      "Epoch: 0613 loss_train: 0.0731 loss_val: 0.0548\n",
      "Epoch: 0614 loss_train: 0.0745 loss_val: 0.0549\n",
      "Epoch: 0615 loss_train: 0.0736 loss_val: 0.0561\n",
      "Epoch: 0616 loss_train: 0.0739 loss_val: 0.0556\n",
      "Epoch: 0617 loss_train: 0.0758 loss_val: 0.0566\n",
      "Epoch: 0618 loss_train: 0.0728 loss_val: 0.0562\n",
      "Epoch: 0619 loss_train: 0.0740 loss_val: 0.0573\n",
      "Epoch: 0620 loss_train: 0.0739 loss_val: 0.0562\n",
      "Epoch: 0621 loss_train: 0.0733 loss_val: 0.0539\n",
      "Epoch: 0622 loss_train: 0.0734 loss_val: 0.0540\n",
      "Epoch: 0623 loss_train: 0.0730 loss_val: 0.0550\n",
      "Epoch: 0624 loss_train: 0.0748 loss_val: 0.0563\n",
      "Epoch: 0625 loss_train: 0.0741 loss_val: 0.0550\n",
      "Epoch: 0626 loss_train: 0.0729 loss_val: 0.0561\n",
      "Epoch: 0627 loss_train: 0.0742 loss_val: 0.0568\n",
      "Epoch: 0628 loss_train: 0.0737 loss_val: 0.0576\n",
      "Epoch: 0629 loss_train: 0.0738 loss_val: 0.0561\n",
      "Epoch: 0630 loss_train: 0.0737 loss_val: 0.0563\n",
      "Epoch: 0631 loss_train: 0.0728 loss_val: 0.0561\n",
      "Epoch: 0632 loss_train: 0.0745 loss_val: 0.0559\n",
      "Epoch: 0633 loss_train: 0.0746 loss_val: 0.0555\n",
      "Epoch: 0634 loss_train: 0.0745 loss_val: 0.0558\n",
      "Epoch: 0635 loss_train: 0.0753 loss_val: 0.0564\n",
      "Epoch: 0636 loss_train: 0.0739 loss_val: 0.0558\n",
      "Epoch: 0637 loss_train: 0.0739 loss_val: 0.0569\n",
      "Epoch: 0638 loss_train: 0.0727 loss_val: 0.0564\n",
      "Epoch: 0639 loss_train: 0.0735 loss_val: 0.0549\n",
      "Epoch: 0640 loss_train: 0.0732 loss_val: 0.0552\n",
      "Epoch: 0641 loss_train: 0.0739 loss_val: 0.0573\n",
      "Epoch: 0642 loss_train: 0.0734 loss_val: 0.0584\n",
      "Epoch: 0643 loss_train: 0.0742 loss_val: 0.0586\n",
      "Epoch: 0644 loss_train: 0.0732 loss_val: 0.0547\n",
      "Epoch: 0645 loss_train: 0.0730 loss_val: 0.0586\n",
      "Epoch: 0646 loss_train: 0.0743 loss_val: 0.0563\n",
      "Epoch: 0647 loss_train: 0.0744 loss_val: 0.0564\n",
      "Epoch: 0648 loss_train: 0.0728 loss_val: 0.0567\n",
      "Epoch: 0649 loss_train: 0.0740 loss_val: 0.0552\n",
      "Epoch: 0650 loss_train: 0.0734 loss_val: 0.0563\n",
      "Epoch: 0651 loss_train: 0.0723 loss_val: 0.0569\n",
      "Epoch: 0652 loss_train: 0.0733 loss_val: 0.0554\n",
      "Epoch: 0653 loss_train: 0.0739 loss_val: 0.0553\n",
      "Epoch: 0654 loss_train: 0.0741 loss_val: 0.0565\n",
      "Epoch: 0655 loss_train: 0.0736 loss_val: 0.0561\n",
      "Epoch: 0656 loss_train: 0.0729 loss_val: 0.0579\n",
      "Epoch: 0657 loss_train: 0.0725 loss_val: 0.0550\n",
      "Epoch: 0658 loss_train: 0.0729 loss_val: 0.0570\n",
      "Epoch: 0659 loss_train: 0.0738 loss_val: 0.0568\n",
      "Epoch: 0660 loss_train: 0.0725 loss_val: 0.0573\n",
      "Epoch: 0661 loss_train: 0.0742 loss_val: 0.0570\n",
      "Epoch: 0662 loss_train: 0.0735 loss_val: 0.0548\n",
      "Epoch: 0663 loss_train: 0.0731 loss_val: 0.0561\n",
      "Epoch: 0664 loss_train: 0.0724 loss_val: 0.0549\n",
      "Epoch: 0665 loss_train: 0.0728 loss_val: 0.0563\n",
      "Epoch: 0666 loss_train: 0.0732 loss_val: 0.0563\n",
      "Epoch: 0667 loss_train: 0.0737 loss_val: 0.0562\n",
      "Epoch: 0668 loss_train: 0.0728 loss_val: 0.0562\n",
      "Epoch: 0669 loss_train: 0.0724 loss_val: 0.0555\n",
      "Epoch: 0670 loss_train: 0.0715 loss_val: 0.0545\n",
      "Epoch: 0671 loss_train: 0.0721 loss_val: 0.0559\n",
      "Epoch: 0672 loss_train: 0.0726 loss_val: 0.0570\n",
      "Epoch: 0673 loss_train: 0.0735 loss_val: 0.0571\n",
      "Epoch: 0674 loss_train: 0.0750 loss_val: 0.0592\n",
      "Epoch: 0675 loss_train: 0.0735 loss_val: 0.0566\n",
      "Epoch: 0676 loss_train: 0.0730 loss_val: 0.0547\n",
      "Epoch: 0677 loss_train: 0.0731 loss_val: 0.0571\n",
      "Epoch: 0678 loss_train: 0.0717 loss_val: 0.0575\n",
      "Epoch: 0679 loss_train: 0.0732 loss_val: 0.0557\n",
      "Epoch: 0680 loss_train: 0.0732 loss_val: 0.0560\n",
      "Epoch: 0681 loss_train: 0.0733 loss_val: 0.0561\n",
      "Epoch: 0682 loss_train: 0.0728 loss_val: 0.0558\n",
      "Epoch: 0683 loss_train: 0.0722 loss_val: 0.0559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0684 loss_train: 0.0732 loss_val: 0.0547\n",
      "Epoch: 0685 loss_train: 0.0717 loss_val: 0.0569\n",
      "Epoch: 0686 loss_train: 0.0729 loss_val: 0.0591\n",
      "Epoch: 0687 loss_train: 0.0731 loss_val: 0.0569\n",
      "Epoch: 0688 loss_train: 0.0717 loss_val: 0.0547\n",
      "Epoch: 0689 loss_train: 0.0721 loss_val: 0.0571\n",
      "Epoch: 0690 loss_train: 0.0721 loss_val: 0.0556\n",
      "Epoch: 0691 loss_train: 0.0713 loss_val: 0.0585\n",
      "Epoch: 0692 loss_train: 0.0734 loss_val: 0.0574\n",
      "Epoch: 0693 loss_train: 0.0730 loss_val: 0.0575\n",
      "Epoch: 0694 loss_train: 0.0727 loss_val: 0.0544\n",
      "Epoch: 0695 loss_train: 0.0733 loss_val: 0.0558\n",
      "Epoch: 0696 loss_train: 0.0726 loss_val: 0.0557\n",
      "Epoch: 0697 loss_train: 0.0734 loss_val: 0.0578\n",
      "Epoch: 0698 loss_train: 0.0737 loss_val: 0.0558\n",
      "Epoch: 0699 loss_train: 0.0724 loss_val: 0.0544\n",
      "Epoch: 0700 loss_train: 0.0727 loss_val: 0.0562\n",
      "Epoch: 0701 loss_train: 0.0728 loss_val: 0.0570\n",
      "Epoch: 0702 loss_train: 0.0728 loss_val: 0.0551\n",
      "Epoch: 0703 loss_train: 0.0720 loss_val: 0.0555\n",
      "Epoch: 0704 loss_train: 0.0744 loss_val: 0.0550\n",
      "Epoch: 0705 loss_train: 0.0735 loss_val: 0.0552\n",
      "Epoch: 0706 loss_train: 0.0716 loss_val: 0.0556\n",
      "Epoch: 0707 loss_train: 0.0726 loss_val: 0.0574\n",
      "Epoch: 0708 loss_train: 0.0726 loss_val: 0.0547\n",
      "Epoch: 0709 loss_train: 0.0707 loss_val: 0.0542\n",
      "Epoch: 0710 loss_train: 0.0714 loss_val: 0.0540\n",
      "Epoch: 0711 loss_train: 0.0724 loss_val: 0.0558\n",
      "Epoch: 0712 loss_train: 0.0735 loss_val: 0.0570\n",
      "Epoch: 0713 loss_train: 0.0722 loss_val: 0.0548\n",
      "Epoch: 0714 loss_train: 0.0718 loss_val: 0.0567\n",
      "Epoch: 0715 loss_train: 0.0730 loss_val: 0.0557\n",
      "Epoch: 0716 loss_train: 0.0733 loss_val: 0.0562\n",
      "Epoch: 0717 loss_train: 0.0731 loss_val: 0.0551\n",
      "Epoch: 0718 loss_train: 0.0727 loss_val: 0.0573\n",
      "Epoch: 0719 loss_train: 0.0725 loss_val: 0.0553\n",
      "Epoch: 0720 loss_train: 0.0733 loss_val: 0.0562\n",
      "Epoch: 0721 loss_train: 0.0723 loss_val: 0.0560\n",
      "Epoch: 0722 loss_train: 0.0728 loss_val: 0.0560\n",
      "Epoch: 0723 loss_train: 0.0717 loss_val: 0.0558\n",
      "Epoch: 0724 loss_train: 0.0726 loss_val: 0.0583\n",
      "Epoch: 0725 loss_train: 0.0713 loss_val: 0.0559\n",
      "Epoch: 0726 loss_train: 0.0735 loss_val: 0.0578\n",
      "Epoch: 0727 loss_train: 0.0731 loss_val: 0.0566\n",
      "Epoch: 0728 loss_train: 0.0726 loss_val: 0.0544\n",
      "Epoch: 0729 loss_train: 0.0704 loss_val: 0.0557\n",
      "Epoch: 0730 loss_train: 0.0721 loss_val: 0.0557\n",
      "Epoch: 0731 loss_train: 0.0730 loss_val: 0.0567\n",
      "Epoch: 0732 loss_train: 0.0736 loss_val: 0.0548\n",
      "Epoch: 0733 loss_train: 0.0718 loss_val: 0.0539\n",
      "Epoch: 0734 loss_train: 0.0721 loss_val: 0.0550\n",
      "Epoch: 0735 loss_train: 0.0725 loss_val: 0.0573\n",
      "Epoch: 0736 loss_train: 0.0720 loss_val: 0.0574\n",
      "Epoch: 0737 loss_train: 0.0723 loss_val: 0.0540\n",
      "Epoch: 0738 loss_train: 0.0713 loss_val: 0.0554\n",
      "Epoch: 0739 loss_train: 0.0726 loss_val: 0.0559\n",
      "Epoch: 0740 loss_train: 0.0718 loss_val: 0.0547\n",
      "Epoch: 0741 loss_train: 0.0726 loss_val: 0.0563\n",
      "Epoch: 0742 loss_train: 0.0730 loss_val: 0.0555\n",
      "Epoch: 0743 loss_train: 0.0723 loss_val: 0.0559\n",
      "Epoch: 0744 loss_train: 0.0720 loss_val: 0.0559\n",
      "Epoch: 0745 loss_train: 0.0727 loss_val: 0.0559\n",
      "Epoch: 0746 loss_train: 0.0716 loss_val: 0.0555\n",
      "Epoch: 0747 loss_train: 0.0719 loss_val: 0.0551\n",
      "Epoch: 0748 loss_train: 0.0712 loss_val: 0.0557\n",
      "Epoch: 0749 loss_train: 0.0730 loss_val: 0.0574\n",
      "Epoch: 0750 loss_train: 0.0734 loss_val: 0.0550\n",
      "Epoch: 0751 loss_train: 0.0720 loss_val: 0.0548\n",
      "Epoch: 0752 loss_train: 0.0723 loss_val: 0.0551\n",
      "Epoch: 0753 loss_train: 0.0722 loss_val: 0.0565\n",
      "Epoch: 0754 loss_train: 0.0731 loss_val: 0.0557\n",
      "Epoch: 0755 loss_train: 0.0718 loss_val: 0.0589\n",
      "Epoch: 0756 loss_train: 0.0710 loss_val: 0.0561\n",
      "Epoch: 0757 loss_train: 0.0719 loss_val: 0.0549\n",
      "Epoch: 0758 loss_train: 0.0717 loss_val: 0.0557\n",
      "Epoch: 0759 loss_train: 0.0710 loss_val: 0.0544\n",
      "Epoch: 0760 loss_train: 0.0712 loss_val: 0.0538\n",
      "Epoch: 0761 loss_train: 0.0716 loss_val: 0.0544\n",
      "Epoch: 0762 loss_train: 0.0714 loss_val: 0.0580\n",
      "Epoch: 0763 loss_train: 0.0716 loss_val: 0.0566\n",
      "Epoch: 0764 loss_train: 0.0722 loss_val: 0.0544\n",
      "Epoch: 0765 loss_train: 0.0713 loss_val: 0.0548\n",
      "Epoch: 0766 loss_train: 0.0723 loss_val: 0.0546\n",
      "Epoch: 0767 loss_train: 0.0717 loss_val: 0.0550\n",
      "Epoch: 0768 loss_train: 0.0717 loss_val: 0.0569\n",
      "Epoch: 0769 loss_train: 0.0717 loss_val: 0.0576\n",
      "Epoch: 0770 loss_train: 0.0729 loss_val: 0.0564\n",
      "Epoch: 0771 loss_train: 0.0727 loss_val: 0.0553\n",
      "Epoch: 0772 loss_train: 0.0733 loss_val: 0.0567\n",
      "Epoch: 0773 loss_train: 0.0722 loss_val: 0.0567\n",
      "Epoch: 0774 loss_train: 0.0725 loss_val: 0.0568\n",
      "Epoch: 0775 loss_train: 0.0728 loss_val: 0.0562\n",
      "Epoch: 0776 loss_train: 0.0717 loss_val: 0.0554\n",
      "Epoch: 0777 loss_train: 0.0713 loss_val: 0.0545\n",
      "Epoch: 0778 loss_train: 0.0707 loss_val: 0.0551\n",
      "Epoch: 0779 loss_train: 0.0723 loss_val: 0.0550\n",
      "Epoch: 0780 loss_train: 0.0718 loss_val: 0.0568\n",
      "Epoch: 0781 loss_train: 0.0711 loss_val: 0.0569\n",
      "Epoch: 0782 loss_train: 0.0711 loss_val: 0.0567\n",
      "Epoch: 0783 loss_train: 0.0712 loss_val: 0.0563\n",
      "Epoch: 0784 loss_train: 0.0725 loss_val: 0.0571\n",
      "Epoch: 0785 loss_train: 0.0718 loss_val: 0.0563\n",
      "Epoch: 0786 loss_train: 0.0724 loss_val: 0.0580\n",
      "Epoch: 0787 loss_train: 0.0721 loss_val: 0.0545\n",
      "Epoch: 0788 loss_train: 0.0711 loss_val: 0.0566\n",
      "Epoch: 0789 loss_train: 0.0710 loss_val: 0.0568\n",
      "Epoch: 0790 loss_train: 0.0719 loss_val: 0.0548\n",
      "Epoch: 0791 loss_train: 0.0705 loss_val: 0.0558\n",
      "Epoch: 0792 loss_train: 0.0706 loss_val: 0.0555\n",
      "Epoch: 0793 loss_train: 0.0689 loss_val: 0.0585\n",
      "Epoch: 0794 loss_train: 0.0704 loss_val: 0.0552\n",
      "Epoch: 0795 loss_train: 0.0719 loss_val: 0.0553\n",
      "Epoch: 0796 loss_train: 0.0714 loss_val: 0.0557\n",
      "Epoch: 0797 loss_train: 0.0721 loss_val: 0.0586\n",
      "Epoch: 0798 loss_train: 0.0704 loss_val: 0.0577\n",
      "Epoch: 0799 loss_train: 0.0713 loss_val: 0.0571\n",
      "Epoch: 0800 loss_train: 0.0705 loss_val: 0.0562\n",
      "Epoch: 0801 loss_train: 0.0704 loss_val: 0.0556\n",
      "Epoch: 0802 loss_train: 0.0715 loss_val: 0.0563\n",
      "Epoch: 0803 loss_train: 0.0701 loss_val: 0.0558\n",
      "Epoch: 0804 loss_train: 0.0704 loss_val: 0.0544\n",
      "Epoch: 0805 loss_train: 0.0713 loss_val: 0.0547\n",
      "Epoch: 0806 loss_train: 0.0715 loss_val: 0.0560\n",
      "Epoch: 0807 loss_train: 0.0714 loss_val: 0.0558\n",
      "Epoch: 0808 loss_train: 0.0711 loss_val: 0.0557\n",
      "Epoch: 0809 loss_train: 0.0695 loss_val: 0.0562\n",
      "Epoch: 0810 loss_train: 0.0705 loss_val: 0.0554\n",
      "Epoch: 0811 loss_train: 0.0713 loss_val: 0.0543\n",
      "Epoch: 0812 loss_train: 0.0706 loss_val: 0.0555\n",
      "Epoch: 0813 loss_train: 0.0711 loss_val: 0.0579\n",
      "Epoch: 0814 loss_train: 0.0710 loss_val: 0.0571\n",
      "Epoch: 0815 loss_train: 0.0711 loss_val: 0.0561\n",
      "Epoch: 0816 loss_train: 0.0714 loss_val: 0.0566\n",
      "Epoch: 0817 loss_train: 0.0707 loss_val: 0.0564\n",
      "Epoch: 0818 loss_train: 0.0706 loss_val: 0.0559\n",
      "Epoch: 0819 loss_train: 0.0703 loss_val: 0.0556\n",
      "Epoch: 0820 loss_train: 0.0717 loss_val: 0.0559\n",
      "Epoch: 0821 loss_train: 0.0706 loss_val: 0.0563\n",
      "Epoch: 0822 loss_train: 0.0710 loss_val: 0.0559\n",
      "Epoch: 0823 loss_train: 0.0710 loss_val: 0.0554\n",
      "Epoch: 0824 loss_train: 0.0711 loss_val: 0.0541\n",
      "Epoch: 0825 loss_train: 0.0715 loss_val: 0.0554\n",
      "Epoch: 0826 loss_train: 0.0710 loss_val: 0.0556\n",
      "Epoch: 0827 loss_train: 0.0704 loss_val: 0.0543\n",
      "Epoch: 0828 loss_train: 0.0706 loss_val: 0.0561\n",
      "Epoch: 0829 loss_train: 0.0715 loss_val: 0.0572\n",
      "Epoch: 0830 loss_train: 0.0698 loss_val: 0.0549\n",
      "Epoch: 0831 loss_train: 0.0726 loss_val: 0.0552\n",
      "Epoch: 0832 loss_train: 0.0713 loss_val: 0.0555\n",
      "Epoch: 0833 loss_train: 0.0709 loss_val: 0.0546\n",
      "Epoch: 0834 loss_train: 0.0707 loss_val: 0.0561\n",
      "Epoch: 0835 loss_train: 0.0714 loss_val: 0.0562\n",
      "Epoch: 0836 loss_train: 0.0716 loss_val: 0.0560\n",
      "Epoch: 0837 loss_train: 0.0719 loss_val: 0.0584\n",
      "Epoch: 0838 loss_train: 0.0710 loss_val: 0.0549\n",
      "Epoch: 0839 loss_train: 0.0710 loss_val: 0.0533\n",
      "Epoch: 0840 loss_train: 0.0706 loss_val: 0.0564\n",
      "Epoch: 0841 loss_train: 0.0701 loss_val: 0.0548\n",
      "Epoch: 0842 loss_train: 0.0705 loss_val: 0.0532\n",
      "Epoch: 0843 loss_train: 0.0700 loss_val: 0.0557\n",
      "Epoch: 0844 loss_train: 0.0725 loss_val: 0.0559\n",
      "Epoch: 0845 loss_train: 0.0719 loss_val: 0.0530\n",
      "Epoch: 0846 loss_train: 0.0714 loss_val: 0.0538\n",
      "Epoch: 0847 loss_train: 0.0710 loss_val: 0.0563\n",
      "Epoch: 0848 loss_train: 0.0708 loss_val: 0.0549\n",
      "Epoch: 0849 loss_train: 0.0720 loss_val: 0.0553\n",
      "Epoch: 0850 loss_train: 0.0710 loss_val: 0.0541\n",
      "Epoch: 0851 loss_train: 0.0703 loss_val: 0.0537\n",
      "Epoch: 0852 loss_train: 0.0700 loss_val: 0.0563\n",
      "Epoch: 0853 loss_train: 0.0712 loss_val: 0.0546\n",
      "Epoch: 0854 loss_train: 0.0695 loss_val: 0.0530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0855 loss_train: 0.0714 loss_val: 0.0554\n",
      "Epoch: 0856 loss_train: 0.0719 loss_val: 0.0550\n",
      "Epoch: 0857 loss_train: 0.0707 loss_val: 0.0550\n",
      "Epoch: 0858 loss_train: 0.0706 loss_val: 0.0552\n",
      "Epoch: 0859 loss_train: 0.0697 loss_val: 0.0527\n",
      "Epoch: 0860 loss_train: 0.0702 loss_val: 0.0549\n",
      "Epoch: 0861 loss_train: 0.0701 loss_val: 0.0543\n",
      "Epoch: 0862 loss_train: 0.0695 loss_val: 0.0554\n",
      "Epoch: 0863 loss_train: 0.0719 loss_val: 0.0561\n",
      "Epoch: 0864 loss_train: 0.0703 loss_val: 0.0554\n",
      "Epoch: 0865 loss_train: 0.0693 loss_val: 0.0531\n",
      "Epoch: 0866 loss_train: 0.0696 loss_val: 0.0551\n",
      "Epoch: 0867 loss_train: 0.0712 loss_val: 0.0548\n",
      "Epoch: 0868 loss_train: 0.0705 loss_val: 0.0562\n",
      "Epoch: 0869 loss_train: 0.0705 loss_val: 0.0555\n",
      "Epoch: 0870 loss_train: 0.0693 loss_val: 0.0544\n",
      "Epoch: 0871 loss_train: 0.0696 loss_val: 0.0559\n",
      "Epoch: 0872 loss_train: 0.0706 loss_val: 0.0543\n",
      "Epoch: 0873 loss_train: 0.0709 loss_val: 0.0554\n",
      "Epoch: 0874 loss_train: 0.0706 loss_val: 0.0564\n",
      "Epoch: 0875 loss_train: 0.0704 loss_val: 0.0564\n",
      "Epoch: 0876 loss_train: 0.0699 loss_val: 0.0565\n",
      "Epoch: 0877 loss_train: 0.0697 loss_val: 0.0548\n",
      "Epoch: 0878 loss_train: 0.0700 loss_val: 0.0560\n",
      "Epoch: 0879 loss_train: 0.0694 loss_val: 0.0550\n",
      "Epoch: 0880 loss_train: 0.0700 loss_val: 0.0571\n",
      "Epoch: 0881 loss_train: 0.0704 loss_val: 0.0571\n",
      "Epoch: 0882 loss_train: 0.0694 loss_val: 0.0559\n",
      "Epoch: 0883 loss_train: 0.0695 loss_val: 0.0549\n",
      "Epoch: 0884 loss_train: 0.0701 loss_val: 0.0553\n",
      "Epoch: 0885 loss_train: 0.0710 loss_val: 0.0540\n",
      "Epoch: 0886 loss_train: 0.0703 loss_val: 0.0556\n",
      "Epoch: 0887 loss_train: 0.0710 loss_val: 0.0546\n",
      "Epoch: 0888 loss_train: 0.0706 loss_val: 0.0560\n",
      "Epoch: 0889 loss_train: 0.0717 loss_val: 0.0561\n",
      "Epoch: 0890 loss_train: 0.0701 loss_val: 0.0555\n",
      "Epoch: 0891 loss_train: 0.0689 loss_val: 0.0564\n",
      "Epoch: 0892 loss_train: 0.0727 loss_val: 0.0548\n",
      "Epoch: 0893 loss_train: 0.0715 loss_val: 0.0564\n",
      "Epoch: 0894 loss_train: 0.0704 loss_val: 0.0567\n",
      "Epoch: 0895 loss_train: 0.0712 loss_val: 0.0556\n",
      "Epoch: 0896 loss_train: 0.0687 loss_val: 0.0555\n",
      "Epoch: 0897 loss_train: 0.0701 loss_val: 0.0554\n",
      "Epoch: 0898 loss_train: 0.0703 loss_val: 0.0547\n",
      "Epoch: 0899 loss_train: 0.0704 loss_val: 0.0566\n",
      "Epoch: 0900 loss_train: 0.0700 loss_val: 0.0573\n",
      "Epoch: 0901 loss_train: 0.0719 loss_val: 0.0578\n",
      "Epoch: 0902 loss_train: 0.0698 loss_val: 0.0567\n",
      "Epoch: 0903 loss_train: 0.0692 loss_val: 0.0564\n",
      "Epoch: 0904 loss_train: 0.0699 loss_val: 0.0585\n",
      "Epoch: 0905 loss_train: 0.0697 loss_val: 0.0575\n",
      "Epoch: 0906 loss_train: 0.0698 loss_val: 0.0571\n",
      "Epoch: 0907 loss_train: 0.0700 loss_val: 0.0569\n",
      "Epoch: 0908 loss_train: 0.0711 loss_val: 0.0554\n",
      "Epoch: 0909 loss_train: 0.0691 loss_val: 0.0564\n",
      "Epoch: 0910 loss_train: 0.0703 loss_val: 0.0570\n",
      "Epoch: 0911 loss_train: 0.0693 loss_val: 0.0555\n",
      "Epoch: 0912 loss_train: 0.0695 loss_val: 0.0566\n",
      "Epoch: 0913 loss_train: 0.0707 loss_val: 0.0559\n",
      "Epoch: 0914 loss_train: 0.0695 loss_val: 0.0556\n",
      "Epoch: 0915 loss_train: 0.0717 loss_val: 0.0575\n",
      "Epoch: 0916 loss_train: 0.0693 loss_val: 0.0545\n",
      "Epoch: 0917 loss_train: 0.0698 loss_val: 0.0547\n",
      "Epoch: 0918 loss_train: 0.0700 loss_val: 0.0554\n",
      "Epoch: 0919 loss_train: 0.0699 loss_val: 0.0556\n",
      "Epoch: 0920 loss_train: 0.0707 loss_val: 0.0537\n",
      "Epoch: 0921 loss_train: 0.0710 loss_val: 0.0551\n",
      "Epoch: 0922 loss_train: 0.0700 loss_val: 0.0562\n",
      "Epoch: 0923 loss_train: 0.0682 loss_val: 0.0565\n",
      "Epoch: 0924 loss_train: 0.0695 loss_val: 0.0559\n",
      "Epoch: 0925 loss_train: 0.0708 loss_val: 0.0562\n",
      "Epoch: 0926 loss_train: 0.0700 loss_val: 0.0567\n",
      "Epoch: 0927 loss_train: 0.0710 loss_val: 0.0557\n",
      "Epoch: 0928 loss_train: 0.0700 loss_val: 0.0554\n",
      "Epoch: 0929 loss_train: 0.0693 loss_val: 0.0565\n",
      "Epoch: 0930 loss_train: 0.0705 loss_val: 0.0550\n",
      "Epoch: 0931 loss_train: 0.0697 loss_val: 0.0554\n",
      "Epoch: 0932 loss_train: 0.0694 loss_val: 0.0552\n",
      "Epoch: 0933 loss_train: 0.0701 loss_val: 0.0543\n",
      "Epoch: 0934 loss_train: 0.0685 loss_val: 0.0535\n",
      "Epoch: 0935 loss_train: 0.0700 loss_val: 0.0550\n",
      "Epoch: 0936 loss_train: 0.0701 loss_val: 0.0549\n",
      "Epoch: 0937 loss_train: 0.0696 loss_val: 0.0551\n",
      "Epoch: 0938 loss_train: 0.0696 loss_val: 0.0571\n",
      "Epoch: 0939 loss_train: 0.0692 loss_val: 0.0572\n",
      "Epoch: 0940 loss_train: 0.0696 loss_val: 0.0590\n",
      "Epoch: 0941 loss_train: 0.0704 loss_val: 0.0558\n",
      "Epoch: 0942 loss_train: 0.0703 loss_val: 0.0575\n",
      "Epoch: 0943 loss_train: 0.0697 loss_val: 0.0559\n",
      "Epoch: 0944 loss_train: 0.0700 loss_val: 0.0549\n",
      "Epoch: 0945 loss_train: 0.0693 loss_val: 0.0583\n",
      "Epoch: 0946 loss_train: 0.0708 loss_val: 0.0573\n",
      "Epoch: 0947 loss_train: 0.0703 loss_val: 0.0563\n",
      "Epoch: 0948 loss_train: 0.0687 loss_val: 0.0551\n",
      "Epoch: 0949 loss_train: 0.0707 loss_val: 0.0550\n",
      "Epoch: 0950 loss_train: 0.0700 loss_val: 0.0559\n",
      "Epoch: 0951 loss_train: 0.0699 loss_val: 0.0543\n",
      "Epoch: 0952 loss_train: 0.0696 loss_val: 0.0555\n",
      "Epoch: 0953 loss_train: 0.0688 loss_val: 0.0532\n",
      "Epoch: 0954 loss_train: 0.0695 loss_val: 0.0546\n",
      "Epoch: 0955 loss_train: 0.0693 loss_val: 0.0557\n",
      "Epoch: 0956 loss_train: 0.0705 loss_val: 0.0553\n",
      "Epoch: 0957 loss_train: 0.0695 loss_val: 0.0551\n",
      "Epoch: 0958 loss_train: 0.0699 loss_val: 0.0566\n",
      "Epoch: 0959 loss_train: 0.0697 loss_val: 0.0576\n",
      "Epoch: 0960 loss_train: 0.0700 loss_val: 0.0549\n",
      "Epoch: 0961 loss_train: 0.0689 loss_val: 0.0553\n",
      "Epoch: 0962 loss_train: 0.0687 loss_val: 0.0544\n",
      "Epoch: 0963 loss_train: 0.0688 loss_val: 0.0568\n",
      "Epoch: 0964 loss_train: 0.0682 loss_val: 0.0583\n",
      "Epoch: 0965 loss_train: 0.0704 loss_val: 0.0553\n",
      "Epoch: 0966 loss_train: 0.0699 loss_val: 0.0579\n",
      "Epoch: 0967 loss_train: 0.0693 loss_val: 0.0572\n",
      "Epoch: 0968 loss_train: 0.0697 loss_val: 0.0548\n",
      "Epoch: 0969 loss_train: 0.0699 loss_val: 0.0559\n",
      "Epoch: 0970 loss_train: 0.0681 loss_val: 0.0566\n",
      "Epoch: 0971 loss_train: 0.0687 loss_val: 0.0556\n",
      "Epoch: 0972 loss_train: 0.0680 loss_val: 0.0552\n",
      "Epoch: 0973 loss_train: 0.0687 loss_val: 0.0578\n",
      "Epoch: 0974 loss_train: 0.0701 loss_val: 0.0558\n",
      "Epoch: 0975 loss_train: 0.0694 loss_val: 0.0549\n",
      "Epoch: 0976 loss_train: 0.0705 loss_val: 0.0553\n",
      "Epoch: 0977 loss_train: 0.0692 loss_val: 0.0566\n",
      "Epoch: 0978 loss_train: 0.0703 loss_val: 0.0551\n",
      "Epoch: 0979 loss_train: 0.0689 loss_val: 0.0559\n",
      "Epoch: 0980 loss_train: 0.0692 loss_val: 0.0560\n",
      "Epoch: 0981 loss_train: 0.0691 loss_val: 0.0571\n",
      "Epoch: 0982 loss_train: 0.0700 loss_val: 0.0562\n",
      "Epoch: 0983 loss_train: 0.0696 loss_val: 0.0571\n",
      "Epoch: 0984 loss_train: 0.0685 loss_val: 0.0544\n",
      "Epoch: 0985 loss_train: 0.0689 loss_val: 0.0562\n",
      "Epoch: 0986 loss_train: 0.0696 loss_val: 0.0561\n",
      "Epoch: 0987 loss_train: 0.0698 loss_val: 0.0557\n",
      "Epoch: 0988 loss_train: 0.0691 loss_val: 0.0575\n",
      "Epoch: 0989 loss_train: 0.0699 loss_val: 0.0551\n",
      "Epoch: 0990 loss_train: 0.0691 loss_val: 0.0557\n",
      "Epoch: 0991 loss_train: 0.0689 loss_val: 0.0544\n",
      "Epoch: 0992 loss_train: 0.0689 loss_val: 0.0550\n",
      "Epoch: 0993 loss_train: 0.0704 loss_val: 0.0569\n",
      "Epoch: 0994 loss_train: 0.0689 loss_val: 0.0559\n",
      "Epoch: 0995 loss_train: 0.0705 loss_val: 0.0560\n",
      "Epoch: 0996 loss_train: 0.0689 loss_val: 0.0562\n",
      "Epoch: 0997 loss_train: 0.0686 loss_val: 0.0566\n",
      "Epoch: 0998 loss_train: 0.0686 loss_val: 0.0558\n",
      "Epoch: 0999 loss_train: 0.0676 loss_val: 0.0555\n",
      "Epoch: 1000 loss_train: 0.0680 loss_val: 0.0560\n",
      "Epoch: 1001 loss_train: 0.0687 loss_val: 0.0573\n",
      "Epoch: 1002 loss_train: 0.0688 loss_val: 0.0545\n",
      "Epoch: 1003 loss_train: 0.0694 loss_val: 0.0567\n",
      "Epoch: 1004 loss_train: 0.0701 loss_val: 0.0579\n",
      "Epoch: 1005 loss_train: 0.0699 loss_val: 0.0558\n",
      "Epoch: 1006 loss_train: 0.0688 loss_val: 0.0580\n",
      "Epoch: 1007 loss_train: 0.0689 loss_val: 0.0566\n",
      "Epoch: 1008 loss_train: 0.0692 loss_val: 0.0558\n",
      "Epoch: 1009 loss_train: 0.0697 loss_val: 0.0548\n",
      "Epoch: 1010 loss_train: 0.0703 loss_val: 0.0546\n",
      "Epoch: 1011 loss_train: 0.0703 loss_val: 0.0571\n",
      "Epoch: 1012 loss_train: 0.0697 loss_val: 0.0557\n",
      "Epoch: 1013 loss_train: 0.0693 loss_val: 0.0528\n",
      "Epoch: 1014 loss_train: 0.0693 loss_val: 0.0550\n",
      "Epoch: 1015 loss_train: 0.0706 loss_val: 0.0559\n",
      "Epoch: 1016 loss_train: 0.0683 loss_val: 0.0573\n",
      "Epoch: 1017 loss_train: 0.0694 loss_val: 0.0575\n",
      "Epoch: 1018 loss_train: 0.0695 loss_val: 0.0566\n",
      "Epoch: 1019 loss_train: 0.0694 loss_val: 0.0589\n",
      "Epoch: 1020 loss_train: 0.0697 loss_val: 0.0570\n",
      "Epoch: 1021 loss_train: 0.0686 loss_val: 0.0565\n",
      "Epoch: 1022 loss_train: 0.0679 loss_val: 0.0580\n",
      "Epoch: 1023 loss_train: 0.0696 loss_val: 0.0563\n",
      "Epoch: 1024 loss_train: 0.0681 loss_val: 0.0545\n",
      "Epoch: 1025 loss_train: 0.0689 loss_val: 0.0581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1026 loss_train: 0.0685 loss_val: 0.0562\n",
      "Epoch: 1027 loss_train: 0.0686 loss_val: 0.0562\n",
      "Epoch: 1028 loss_train: 0.0680 loss_val: 0.0579\n",
      "Epoch: 1029 loss_train: 0.0700 loss_val: 0.0569\n",
      "Epoch: 1030 loss_train: 0.0684 loss_val: 0.0546\n",
      "Epoch: 1031 loss_train: 0.0675 loss_val: 0.0571\n",
      "Epoch: 1032 loss_train: 0.0692 loss_val: 0.0549\n",
      "Epoch: 1033 loss_train: 0.0697 loss_val: 0.0565\n",
      "Epoch: 1034 loss_train: 0.0695 loss_val: 0.0556\n",
      "Epoch: 1035 loss_train: 0.0683 loss_val: 0.0543\n",
      "Epoch: 1036 loss_train: 0.0676 loss_val: 0.0539\n",
      "Epoch: 1037 loss_train: 0.0686 loss_val: 0.0550\n",
      "Epoch: 1038 loss_train: 0.0684 loss_val: 0.0542\n",
      "Epoch: 1039 loss_train: 0.0690 loss_val: 0.0558\n",
      "Epoch: 1040 loss_train: 0.0676 loss_val: 0.0560\n",
      "Epoch: 1041 loss_train: 0.0691 loss_val: 0.0553\n",
      "Epoch: 1042 loss_train: 0.0696 loss_val: 0.0560\n",
      "Epoch: 1043 loss_train: 0.0700 loss_val: 0.0569\n",
      "Epoch: 1044 loss_train: 0.0698 loss_val: 0.0543\n",
      "Epoch: 1045 loss_train: 0.0691 loss_val: 0.0556\n",
      "Epoch: 1046 loss_train: 0.0682 loss_val: 0.0551\n",
      "Epoch: 1047 loss_train: 0.0691 loss_val: 0.0565\n",
      "Epoch: 1048 loss_train: 0.0695 loss_val: 0.0559\n",
      "Epoch: 1049 loss_train: 0.0698 loss_val: 0.0558\n",
      "Epoch: 1050 loss_train: 0.0689 loss_val: 0.0573\n",
      "Epoch: 1051 loss_train: 0.0689 loss_val: 0.0554\n",
      "Epoch: 1052 loss_train: 0.0687 loss_val: 0.0560\n",
      "Epoch: 1053 loss_train: 0.0686 loss_val: 0.0552\n",
      "Epoch: 1054 loss_train: 0.0690 loss_val: 0.0552\n",
      "Epoch: 1055 loss_train: 0.0683 loss_val: 0.0556\n",
      "Epoch: 1056 loss_train: 0.0685 loss_val: 0.0561\n",
      "Epoch: 1057 loss_train: 0.0681 loss_val: 0.0565\n",
      "Epoch: 1058 loss_train: 0.0679 loss_val: 0.0558\n",
      "Epoch: 1059 loss_train: 0.0683 loss_val: 0.0556\n",
      "Epoch: 1060 loss_train: 0.0672 loss_val: 0.0569\n",
      "Epoch: 1061 loss_train: 0.0683 loss_val: 0.0576\n",
      "Epoch: 1062 loss_train: 0.0674 loss_val: 0.0549\n",
      "Epoch: 1063 loss_train: 0.0677 loss_val: 0.0559\n",
      "Epoch: 1064 loss_train: 0.0684 loss_val: 0.0564\n",
      "Epoch: 1065 loss_train: 0.0671 loss_val: 0.0578\n",
      "Epoch: 1066 loss_train: 0.0681 loss_val: 0.0550\n",
      "Epoch: 1067 loss_train: 0.0702 loss_val: 0.0577\n",
      "Epoch: 1068 loss_train: 0.0694 loss_val: 0.0558\n",
      "Epoch: 1069 loss_train: 0.0700 loss_val: 0.0560\n",
      "Epoch: 1070 loss_train: 0.0684 loss_val: 0.0560\n",
      "Epoch: 1071 loss_train: 0.0694 loss_val: 0.0550\n",
      "Epoch: 1072 loss_train: 0.0690 loss_val: 0.0544\n",
      "Epoch: 1073 loss_train: 0.0683 loss_val: 0.0550\n",
      "Epoch: 1074 loss_train: 0.0686 loss_val: 0.0563\n",
      "Epoch: 1075 loss_train: 0.0688 loss_val: 0.0581\n",
      "Epoch: 1076 loss_train: 0.0692 loss_val: 0.0574\n",
      "Epoch: 1077 loss_train: 0.0684 loss_val: 0.0548\n",
      "Epoch: 1078 loss_train: 0.0691 loss_val: 0.0546\n",
      "Epoch: 1079 loss_train: 0.0688 loss_val: 0.0573\n",
      "Epoch: 1080 loss_train: 0.0681 loss_val: 0.0545\n",
      "Epoch: 1081 loss_train: 0.0690 loss_val: 0.0548\n",
      "Epoch: 1082 loss_train: 0.0675 loss_val: 0.0543\n",
      "Epoch: 1083 loss_train: 0.0682 loss_val: 0.0552\n",
      "Epoch: 1084 loss_train: 0.0687 loss_val: 0.0565\n",
      "Epoch: 1085 loss_train: 0.0689 loss_val: 0.0549\n",
      "Epoch: 1086 loss_train: 0.0687 loss_val: 0.0555\n",
      "Epoch: 1087 loss_train: 0.0678 loss_val: 0.0570\n",
      "Epoch: 1088 loss_train: 0.0688 loss_val: 0.0550\n",
      "Epoch: 1089 loss_train: 0.0698 loss_val: 0.0558\n",
      "Epoch: 1090 loss_train: 0.0707 loss_val: 0.0538\n",
      "Epoch: 1091 loss_train: 0.0690 loss_val: 0.0570\n",
      "Epoch: 1092 loss_train: 0.0680 loss_val: 0.0564\n",
      "Epoch: 1093 loss_train: 0.0685 loss_val: 0.0578\n",
      "Epoch: 1094 loss_train: 0.0690 loss_val: 0.0575\n",
      "Epoch: 1095 loss_train: 0.0682 loss_val: 0.0550\n",
      "Epoch: 1096 loss_train: 0.0689 loss_val: 0.0561\n",
      "Epoch: 1097 loss_train: 0.0690 loss_val: 0.0581\n",
      "Epoch: 1098 loss_train: 0.0687 loss_val: 0.0574\n",
      "Epoch: 1099 loss_train: 0.0684 loss_val: 0.0574\n",
      "Epoch: 1100 loss_train: 0.0687 loss_val: 0.0580\n",
      "Epoch: 1101 loss_train: 0.0685 loss_val: 0.0571\n",
      "Epoch: 1102 loss_train: 0.0679 loss_val: 0.0564\n",
      "Epoch: 1103 loss_train: 0.0694 loss_val: 0.0552\n",
      "Epoch: 1104 loss_train: 0.0683 loss_val: 0.0554\n",
      "Epoch: 1105 loss_train: 0.0658 loss_val: 0.0551\n",
      "Epoch: 1106 loss_train: 0.0676 loss_val: 0.0549\n",
      "Epoch: 1107 loss_train: 0.0693 loss_val: 0.0536\n",
      "Epoch: 1108 loss_train: 0.0679 loss_val: 0.0544\n",
      "Epoch: 1109 loss_train: 0.0677 loss_val: 0.0561\n",
      "Epoch: 1110 loss_train: 0.0687 loss_val: 0.0547\n",
      "Epoch: 1111 loss_train: 0.0672 loss_val: 0.0572\n",
      "Epoch: 1112 loss_train: 0.0693 loss_val: 0.0574\n",
      "Epoch: 1113 loss_train: 0.0680 loss_val: 0.0570\n",
      "Epoch: 1114 loss_train: 0.0677 loss_val: 0.0564\n",
      "Epoch: 1115 loss_train: 0.0688 loss_val: 0.0566\n",
      "Epoch: 1116 loss_train: 0.0687 loss_val: 0.0558\n",
      "Epoch: 1117 loss_train: 0.0676 loss_val: 0.0559\n",
      "Epoch: 1118 loss_train: 0.0685 loss_val: 0.0554\n",
      "Epoch: 1119 loss_train: 0.0684 loss_val: 0.0552\n",
      "Epoch: 1120 loss_train: 0.0685 loss_val: 0.0560\n",
      "Epoch: 1121 loss_train: 0.0685 loss_val: 0.0549\n",
      "Epoch: 1122 loss_train: 0.0684 loss_val: 0.0549\n",
      "Epoch: 1123 loss_train: 0.0677 loss_val: 0.0578\n",
      "Epoch: 1124 loss_train: 0.0668 loss_val: 0.0544\n",
      "Epoch: 1125 loss_train: 0.0669 loss_val: 0.0541\n",
      "Epoch: 1126 loss_train: 0.0674 loss_val: 0.0549\n",
      "Epoch: 1127 loss_train: 0.0678 loss_val: 0.0545\n",
      "Epoch: 1128 loss_train: 0.0682 loss_val: 0.0571\n",
      "Epoch: 1129 loss_train: 0.0692 loss_val: 0.0565\n",
      "Epoch: 1130 loss_train: 0.0667 loss_val: 0.0545\n",
      "Epoch: 1131 loss_train: 0.0674 loss_val: 0.0553\n",
      "Epoch: 1132 loss_train: 0.0677 loss_val: 0.0546\n",
      "Epoch: 1133 loss_train: 0.0670 loss_val: 0.0557\n",
      "Epoch: 1134 loss_train: 0.0686 loss_val: 0.0562\n",
      "Epoch: 1135 loss_train: 0.0684 loss_val: 0.0553\n",
      "Epoch: 1136 loss_train: 0.0680 loss_val: 0.0552\n",
      "Epoch: 1137 loss_train: 0.0687 loss_val: 0.0556\n",
      "Epoch: 1138 loss_train: 0.0687 loss_val: 0.0565\n",
      "Epoch: 1139 loss_train: 0.0676 loss_val: 0.0571\n",
      "Epoch: 1140 loss_train: 0.0677 loss_val: 0.0562\n",
      "Epoch: 1141 loss_train: 0.0677 loss_val: 0.0553\n",
      "Epoch: 1142 loss_train: 0.0678 loss_val: 0.0560\n",
      "Epoch: 1143 loss_train: 0.0679 loss_val: 0.0546\n",
      "Epoch: 1144 loss_train: 0.0675 loss_val: 0.0555\n",
      "Epoch: 1145 loss_train: 0.0686 loss_val: 0.0566\n",
      "Epoch: 1146 loss_train: 0.0690 loss_val: 0.0556\n",
      "Epoch: 1147 loss_train: 0.0697 loss_val: 0.0577\n",
      "Epoch: 1148 loss_train: 0.0702 loss_val: 0.0576\n",
      "Epoch: 1149 loss_train: 0.0684 loss_val: 0.0569\n",
      "Epoch: 1150 loss_train: 0.0677 loss_val: 0.0570\n",
      "Epoch: 1151 loss_train: 0.0680 loss_val: 0.0575\n",
      "Epoch: 1152 loss_train: 0.0685 loss_val: 0.0554\n",
      "Epoch: 1153 loss_train: 0.0669 loss_val: 0.0562\n",
      "Epoch: 1154 loss_train: 0.0675 loss_val: 0.0579\n",
      "Epoch: 1155 loss_train: 0.0686 loss_val: 0.0567\n",
      "Epoch: 1156 loss_train: 0.0671 loss_val: 0.0563\n",
      "Epoch: 1157 loss_train: 0.0680 loss_val: 0.0560\n",
      "Epoch: 1158 loss_train: 0.0676 loss_val: 0.0553\n",
      "Epoch: 1159 loss_train: 0.0676 loss_val: 0.0548\n",
      "Epoch: 1160 loss_train: 0.0673 loss_val: 0.0557\n",
      "Epoch: 1161 loss_train: 0.0688 loss_val: 0.0553\n",
      "Epoch: 1162 loss_train: 0.0677 loss_val: 0.0552\n",
      "Epoch: 1163 loss_train: 0.0672 loss_val: 0.0558\n",
      "Epoch: 1164 loss_train: 0.0665 loss_val: 0.0563\n",
      "Epoch: 1165 loss_train: 0.0687 loss_val: 0.0552\n",
      "Epoch: 1166 loss_train: 0.0669 loss_val: 0.0557\n",
      "Epoch: 1167 loss_train: 0.0672 loss_val: 0.0564\n",
      "Epoch: 1168 loss_train: 0.0686 loss_val: 0.0575\n",
      "Epoch: 1169 loss_train: 0.0680 loss_val: 0.0558\n",
      "Epoch: 1170 loss_train: 0.0668 loss_val: 0.0561\n",
      "Epoch: 1171 loss_train: 0.0666 loss_val: 0.0545\n",
      "Epoch: 1172 loss_train: 0.0667 loss_val: 0.0558\n",
      "Epoch: 1173 loss_train: 0.0676 loss_val: 0.0544\n",
      "Epoch: 1174 loss_train: 0.0671 loss_val: 0.0548\n",
      "Epoch: 1175 loss_train: 0.0687 loss_val: 0.0564\n",
      "Epoch: 1176 loss_train: 0.0681 loss_val: 0.0559\n",
      "Epoch: 1177 loss_train: 0.0686 loss_val: 0.0556\n",
      "Epoch: 1178 loss_train: 0.0676 loss_val: 0.0556\n",
      "Epoch: 1179 loss_train: 0.0684 loss_val: 0.0551\n",
      "Epoch: 1180 loss_train: 0.0675 loss_val: 0.0566\n",
      "Epoch: 1181 loss_train: 0.0686 loss_val: 0.0553\n",
      "Epoch: 1182 loss_train: 0.0665 loss_val: 0.0544\n",
      "Epoch: 1183 loss_train: 0.0666 loss_val: 0.0549\n",
      "Epoch: 1184 loss_train: 0.0674 loss_val: 0.0548\n",
      "Epoch: 1185 loss_train: 0.0672 loss_val: 0.0567\n",
      "Epoch: 1186 loss_train: 0.0667 loss_val: 0.0551\n",
      "Epoch: 1187 loss_train: 0.0663 loss_val: 0.0524\n",
      "Epoch: 1188 loss_train: 0.0657 loss_val: 0.0550\n",
      "Epoch: 1189 loss_train: 0.0675 loss_val: 0.0545\n",
      "Epoch: 1190 loss_train: 0.0677 loss_val: 0.0556\n",
      "Epoch: 1191 loss_train: 0.0666 loss_val: 0.0559\n",
      "Epoch: 1192 loss_train: 0.0676 loss_val: 0.0556\n",
      "Epoch: 1193 loss_train: 0.0666 loss_val: 0.0536\n",
      "Epoch: 1194 loss_train: 0.0671 loss_val: 0.0550\n",
      "Epoch: 1195 loss_train: 0.0670 loss_val: 0.0543\n",
      "Epoch: 1196 loss_train: 0.0664 loss_val: 0.0553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1197 loss_train: 0.0664 loss_val: 0.0552\n",
      "Epoch: 1198 loss_train: 0.0674 loss_val: 0.0530\n",
      "Epoch: 1199 loss_train: 0.0686 loss_val: 0.0558\n",
      "Epoch: 1200 loss_train: 0.0672 loss_val: 0.0546\n",
      "Epoch: 1201 loss_train: 0.0690 loss_val: 0.0566\n",
      "Epoch: 1202 loss_train: 0.0675 loss_val: 0.0548\n",
      "Epoch: 1203 loss_train: 0.0675 loss_val: 0.0546\n",
      "Epoch: 1204 loss_train: 0.0684 loss_val: 0.0555\n",
      "Epoch: 1205 loss_train: 0.0674 loss_val: 0.0539\n",
      "Epoch: 1206 loss_train: 0.0681 loss_val: 0.0571\n",
      "Epoch: 1207 loss_train: 0.0679 loss_val: 0.0568\n",
      "Epoch: 1208 loss_train: 0.0668 loss_val: 0.0547\n",
      "Epoch: 1209 loss_train: 0.0664 loss_val: 0.0547\n",
      "Epoch: 1210 loss_train: 0.0676 loss_val: 0.0541\n",
      "Epoch: 1211 loss_train: 0.0672 loss_val: 0.0554\n",
      "Epoch: 1212 loss_train: 0.0677 loss_val: 0.0561\n",
      "Epoch: 1213 loss_train: 0.0687 loss_val: 0.0555\n",
      "Epoch: 1214 loss_train: 0.0670 loss_val: 0.0556\n",
      "Epoch: 1215 loss_train: 0.0673 loss_val: 0.0565\n",
      "Epoch: 1216 loss_train: 0.0686 loss_val: 0.0562\n",
      "Epoch: 1217 loss_train: 0.0679 loss_val: 0.0566\n",
      "Epoch: 1218 loss_train: 0.0682 loss_val: 0.0560\n",
      "Epoch: 1219 loss_train: 0.0658 loss_val: 0.0568\n",
      "Epoch: 1220 loss_train: 0.0675 loss_val: 0.0556\n",
      "Epoch: 1221 loss_train: 0.0669 loss_val: 0.0561\n",
      "Epoch: 1222 loss_train: 0.0684 loss_val: 0.0526\n",
      "Epoch: 1223 loss_train: 0.0679 loss_val: 0.0552\n",
      "Epoch: 1224 loss_train: 0.0679 loss_val: 0.0565\n",
      "Epoch: 1225 loss_train: 0.0677 loss_val: 0.0556\n",
      "Epoch: 1226 loss_train: 0.0679 loss_val: 0.0544\n",
      "Epoch: 1227 loss_train: 0.0675 loss_val: 0.0557\n",
      "Epoch: 1228 loss_train: 0.0679 loss_val: 0.0553\n",
      "Epoch: 1229 loss_train: 0.0679 loss_val: 0.0555\n",
      "Epoch: 1230 loss_train: 0.0669 loss_val: 0.0560\n",
      "Epoch: 1231 loss_train: 0.0675 loss_val: 0.0564\n",
      "Epoch: 1232 loss_train: 0.0667 loss_val: 0.0548\n",
      "Epoch: 1233 loss_train: 0.0668 loss_val: 0.0557\n",
      "Epoch: 1234 loss_train: 0.0664 loss_val: 0.0584\n",
      "Epoch: 1235 loss_train: 0.0684 loss_val: 0.0553\n",
      "Epoch: 1236 loss_train: 0.0675 loss_val: 0.0553\n",
      "Epoch: 1237 loss_train: 0.0661 loss_val: 0.0548\n",
      "Epoch: 1238 loss_train: 0.0670 loss_val: 0.0555\n",
      "Epoch: 1239 loss_train: 0.0670 loss_val: 0.0541\n",
      "Epoch: 1240 loss_train: 0.0667 loss_val: 0.0583\n",
      "Epoch: 1241 loss_train: 0.0682 loss_val: 0.0542\n",
      "Epoch: 1242 loss_train: 0.0672 loss_val: 0.0555\n",
      "Epoch: 1243 loss_train: 0.0675 loss_val: 0.0546\n",
      "Epoch: 1244 loss_train: 0.0670 loss_val: 0.0540\n",
      "Epoch: 1245 loss_train: 0.0673 loss_val: 0.0579\n",
      "Epoch: 1246 loss_train: 0.0668 loss_val: 0.0569\n",
      "Epoch: 1247 loss_train: 0.0680 loss_val: 0.0567\n",
      "Epoch: 1248 loss_train: 0.0664 loss_val: 0.0557\n",
      "Epoch: 1249 loss_train: 0.0676 loss_val: 0.0559\n",
      "Epoch: 1250 loss_train: 0.0667 loss_val: 0.0549\n",
      "Epoch: 1251 loss_train: 0.0678 loss_val: 0.0546\n",
      "Epoch: 1252 loss_train: 0.0660 loss_val: 0.0566\n",
      "Epoch: 1253 loss_train: 0.0674 loss_val: 0.0543\n",
      "Epoch: 1254 loss_train: 0.0669 loss_val: 0.0567\n",
      "Epoch: 1255 loss_train: 0.0670 loss_val: 0.0569\n",
      "Epoch: 1256 loss_train: 0.0677 loss_val: 0.0577\n",
      "Epoch: 1257 loss_train: 0.0679 loss_val: 0.0555\n",
      "Epoch: 1258 loss_train: 0.0667 loss_val: 0.0549\n",
      "Epoch: 1259 loss_train: 0.0665 loss_val: 0.0553\n",
      "Epoch: 1260 loss_train: 0.0671 loss_val: 0.0560\n",
      "Epoch: 1261 loss_train: 0.0665 loss_val: 0.0549\n",
      "Epoch: 1262 loss_train: 0.0673 loss_val: 0.0548\n",
      "Epoch: 1263 loss_train: 0.0667 loss_val: 0.0554\n",
      "Epoch: 1264 loss_train: 0.0658 loss_val: 0.0555\n",
      "Epoch: 1265 loss_train: 0.0671 loss_val: 0.0566\n",
      "Epoch: 1266 loss_train: 0.0674 loss_val: 0.0554\n",
      "Epoch: 1267 loss_train: 0.0660 loss_val: 0.0531\n",
      "Epoch: 1268 loss_train: 0.0660 loss_val: 0.0545\n",
      "Epoch: 1269 loss_train: 0.0667 loss_val: 0.0550\n",
      "Epoch: 1270 loss_train: 0.0661 loss_val: 0.0550\n",
      "Epoch: 1271 loss_train: 0.0675 loss_val: 0.0559\n",
      "Epoch: 1272 loss_train: 0.0668 loss_val: 0.0564\n",
      "Epoch: 1273 loss_train: 0.0672 loss_val: 0.0538\n",
      "Epoch: 1274 loss_train: 0.0662 loss_val: 0.0532\n",
      "Epoch: 1275 loss_train: 0.0672 loss_val: 0.0548\n",
      "Epoch: 1276 loss_train: 0.0665 loss_val: 0.0557\n",
      "Epoch: 1277 loss_train: 0.0667 loss_val: 0.0552\n",
      "Epoch: 1278 loss_train: 0.0682 loss_val: 0.0553\n",
      "Epoch: 1279 loss_train: 0.0673 loss_val: 0.0582\n",
      "Epoch: 1280 loss_train: 0.0670 loss_val: 0.0560\n",
      "Epoch: 1281 loss_train: 0.0678 loss_val: 0.0560\n",
      "Epoch: 1282 loss_train: 0.0671 loss_val: 0.0548\n",
      "Epoch: 1283 loss_train: 0.0673 loss_val: 0.0555\n",
      "Epoch: 1284 loss_train: 0.0658 loss_val: 0.0544\n",
      "Epoch: 1285 loss_train: 0.0661 loss_val: 0.0567\n",
      "Epoch: 1286 loss_train: 0.0673 loss_val: 0.0559\n",
      "Epoch: 1287 loss_train: 0.0671 loss_val: 0.0554\n",
      "Epoch: 1288 loss_train: 0.0657 loss_val: 0.0550\n",
      "Epoch: 1289 loss_train: 0.0666 loss_val: 0.0548\n",
      "Epoch: 1290 loss_train: 0.0649 loss_val: 0.0548\n",
      "Epoch: 1291 loss_train: 0.0666 loss_val: 0.0567\n",
      "Epoch: 1292 loss_train: 0.0654 loss_val: 0.0557\n",
      "Epoch: 1293 loss_train: 0.0657 loss_val: 0.0552\n",
      "Epoch: 1294 loss_train: 0.0664 loss_val: 0.0558\n",
      "Epoch: 1295 loss_train: 0.0664 loss_val: 0.0576\n",
      "Epoch: 1296 loss_train: 0.0664 loss_val: 0.0554\n",
      "Epoch: 1297 loss_train: 0.0667 loss_val: 0.0587\n",
      "Epoch: 1298 loss_train: 0.0663 loss_val: 0.0552\n",
      "Epoch: 1299 loss_train: 0.0666 loss_val: 0.0544\n",
      "Epoch: 1300 loss_train: 0.0650 loss_val: 0.0561\n",
      "Epoch: 1301 loss_train: 0.0683 loss_val: 0.0566\n",
      "Epoch: 1302 loss_train: 0.0663 loss_val: 0.0553\n",
      "Epoch: 1303 loss_train: 0.0659 loss_val: 0.0563\n",
      "Epoch: 1304 loss_train: 0.0663 loss_val: 0.0563\n",
      "Epoch: 1305 loss_train: 0.0665 loss_val: 0.0560\n",
      "Epoch: 1306 loss_train: 0.0662 loss_val: 0.0570\n",
      "Epoch: 1307 loss_train: 0.0660 loss_val: 0.0539\n",
      "Epoch: 1308 loss_train: 0.0660 loss_val: 0.0541\n",
      "Epoch: 1309 loss_train: 0.0666 loss_val: 0.0550\n",
      "Epoch: 1310 loss_train: 0.0682 loss_val: 0.0555\n",
      "Epoch: 1311 loss_train: 0.0666 loss_val: 0.0554\n",
      "Epoch: 1312 loss_train: 0.0653 loss_val: 0.0567\n",
      "Epoch: 1313 loss_train: 0.0680 loss_val: 0.0560\n",
      "Epoch: 1314 loss_train: 0.0660 loss_val: 0.0555\n",
      "Epoch: 1315 loss_train: 0.0668 loss_val: 0.0568\n",
      "Epoch: 1316 loss_train: 0.0673 loss_val: 0.0559\n",
      "Epoch: 1317 loss_train: 0.0671 loss_val: 0.0571\n",
      "Epoch: 1318 loss_train: 0.0666 loss_val: 0.0556\n",
      "Epoch: 1319 loss_train: 0.0667 loss_val: 0.0548\n",
      "Epoch: 1320 loss_train: 0.0652 loss_val: 0.0545\n",
      "Epoch: 1321 loss_train: 0.0654 loss_val: 0.0559\n",
      "Epoch: 1322 loss_train: 0.0666 loss_val: 0.0554\n",
      "Epoch: 1323 loss_train: 0.0670 loss_val: 0.0552\n",
      "Epoch: 1324 loss_train: 0.0660 loss_val: 0.0570\n",
      "Epoch: 1325 loss_train: 0.0666 loss_val: 0.0563\n",
      "Epoch: 1326 loss_train: 0.0671 loss_val: 0.0563\n",
      "Epoch: 1327 loss_train: 0.0665 loss_val: 0.0546\n",
      "Epoch: 1328 loss_train: 0.0653 loss_val: 0.0547\n",
      "Epoch: 1329 loss_train: 0.0654 loss_val: 0.0556\n",
      "Epoch: 1330 loss_train: 0.0668 loss_val: 0.0544\n",
      "Epoch: 1331 loss_train: 0.0662 loss_val: 0.0566\n",
      "Epoch: 1332 loss_train: 0.0666 loss_val: 0.0567\n",
      "Epoch: 1333 loss_train: 0.0665 loss_val: 0.0553\n",
      "Epoch: 1334 loss_train: 0.0657 loss_val: 0.0547\n",
      "Epoch: 1335 loss_train: 0.0666 loss_val: 0.0548\n",
      "Epoch: 1336 loss_train: 0.0668 loss_val: 0.0548\n",
      "Epoch: 1337 loss_train: 0.0674 loss_val: 0.0536\n",
      "Epoch: 1338 loss_train: 0.0674 loss_val: 0.0556\n",
      "Epoch: 1339 loss_train: 0.0683 loss_val: 0.0562\n",
      "Epoch: 1340 loss_train: 0.0680 loss_val: 0.0557\n",
      "Epoch: 1341 loss_train: 0.0663 loss_val: 0.0548\n",
      "Epoch: 1342 loss_train: 0.0661 loss_val: 0.0562\n",
      "Epoch: 1343 loss_train: 0.0675 loss_val: 0.0564\n",
      "Epoch: 1344 loss_train: 0.0663 loss_val: 0.0569\n",
      "Epoch: 1345 loss_train: 0.0666 loss_val: 0.0550\n",
      "Epoch: 1346 loss_train: 0.0662 loss_val: 0.0548\n",
      "Epoch: 1347 loss_train: 0.0660 loss_val: 0.0545\n",
      "Epoch: 1348 loss_train: 0.0660 loss_val: 0.0558\n",
      "Epoch: 1349 loss_train: 0.0661 loss_val: 0.0552\n",
      "Epoch: 1350 loss_train: 0.0661 loss_val: 0.0564\n",
      "Epoch: 1351 loss_train: 0.0654 loss_val: 0.0568\n",
      "Epoch: 1352 loss_train: 0.0669 loss_val: 0.0561\n",
      "Epoch: 1353 loss_train: 0.0675 loss_val: 0.0550\n",
      "Epoch: 1354 loss_train: 0.0653 loss_val: 0.0552\n",
      "Epoch: 1355 loss_train: 0.0657 loss_val: 0.0554\n",
      "Epoch: 1356 loss_train: 0.0671 loss_val: 0.0557\n",
      "Epoch: 1357 loss_train: 0.0671 loss_val: 0.0576\n",
      "Epoch: 1358 loss_train: 0.0669 loss_val: 0.0559\n",
      "Epoch: 1359 loss_train: 0.0674 loss_val: 0.0572\n",
      "Epoch: 1360 loss_train: 0.0653 loss_val: 0.0544\n",
      "Epoch: 1361 loss_train: 0.0657 loss_val: 0.0553\n",
      "Epoch: 1362 loss_train: 0.0667 loss_val: 0.0556\n",
      "Epoch: 1363 loss_train: 0.0655 loss_val: 0.0555\n",
      "Epoch: 1364 loss_train: 0.0655 loss_val: 0.0556\n",
      "Epoch: 1365 loss_train: 0.0651 loss_val: 0.0567\n",
      "Epoch: 1366 loss_train: 0.0662 loss_val: 0.0559\n",
      "Epoch: 1367 loss_train: 0.0658 loss_val: 0.0554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1368 loss_train: 0.0656 loss_val: 0.0565\n",
      "Epoch: 1369 loss_train: 0.0664 loss_val: 0.0559\n",
      "Epoch: 1370 loss_train: 0.0665 loss_val: 0.0564\n",
      "Epoch: 1371 loss_train: 0.0662 loss_val: 0.0562\n",
      "Epoch: 1372 loss_train: 0.0666 loss_val: 0.0565\n",
      "Epoch: 1373 loss_train: 0.0660 loss_val: 0.0554\n",
      "Epoch: 1374 loss_train: 0.0675 loss_val: 0.0559\n",
      "Epoch: 1375 loss_train: 0.0658 loss_val: 0.0568\n",
      "Epoch: 1376 loss_train: 0.0667 loss_val: 0.0563\n",
      "Epoch: 1377 loss_train: 0.0667 loss_val: 0.0564\n",
      "Epoch: 1378 loss_train: 0.0654 loss_val: 0.0557\n",
      "Epoch: 1379 loss_train: 0.0665 loss_val: 0.0544\n",
      "Epoch: 1380 loss_train: 0.0654 loss_val: 0.0551\n",
      "Epoch: 1381 loss_train: 0.0658 loss_val: 0.0569\n",
      "Epoch: 1382 loss_train: 0.0670 loss_val: 0.0569\n",
      "Epoch: 1383 loss_train: 0.0669 loss_val: 0.0565\n",
      "Epoch: 1384 loss_train: 0.0654 loss_val: 0.0571\n",
      "Epoch: 1385 loss_train: 0.0670 loss_val: 0.0565\n",
      "Epoch: 1386 loss_train: 0.0668 loss_val: 0.0558\n",
      "Epoch: 1387 loss_train: 0.0672 loss_val: 0.0554\n",
      "Epoch: 1388 loss_train: 0.0664 loss_val: 0.0562\n",
      "Epoch: 1389 loss_train: 0.0660 loss_val: 0.0584\n",
      "Epoch: 1390 loss_train: 0.0655 loss_val: 0.0550\n",
      "Epoch: 1391 loss_train: 0.0662 loss_val: 0.0556\n",
      "Epoch: 1392 loss_train: 0.0665 loss_val: 0.0549\n",
      "Epoch: 1393 loss_train: 0.0660 loss_val: 0.0540\n",
      "Epoch: 1394 loss_train: 0.0660 loss_val: 0.0555\n",
      "Epoch: 1395 loss_train: 0.0655 loss_val: 0.0553\n",
      "Epoch: 1396 loss_train: 0.0662 loss_val: 0.0563\n",
      "Epoch: 1397 loss_train: 0.0671 loss_val: 0.0565\n",
      "Epoch: 1398 loss_train: 0.0652 loss_val: 0.0563\n",
      "Epoch: 1399 loss_train: 0.0661 loss_val: 0.0552\n",
      "Epoch: 1400 loss_train: 0.0665 loss_val: 0.0542\n",
      "Epoch: 1401 loss_train: 0.0653 loss_val: 0.0576\n",
      "Epoch: 1402 loss_train: 0.0654 loss_val: 0.0577\n",
      "Epoch: 1403 loss_train: 0.0652 loss_val: 0.0556\n",
      "Epoch: 1404 loss_train: 0.0655 loss_val: 0.0554\n",
      "Epoch: 1405 loss_train: 0.0647 loss_val: 0.0563\n",
      "Epoch: 1406 loss_train: 0.0665 loss_val: 0.0584\n",
      "Epoch: 1407 loss_train: 0.0663 loss_val: 0.0574\n",
      "Epoch: 1408 loss_train: 0.0670 loss_val: 0.0575\n",
      "Epoch: 1409 loss_train: 0.0666 loss_val: 0.0575\n",
      "Epoch: 1410 loss_train: 0.0662 loss_val: 0.0572\n",
      "Epoch: 1411 loss_train: 0.0661 loss_val: 0.0564\n",
      "Epoch: 1412 loss_train: 0.0663 loss_val: 0.0574\n",
      "Epoch: 1413 loss_train: 0.0660 loss_val: 0.0579\n",
      "Epoch: 1414 loss_train: 0.0671 loss_val: 0.0562\n",
      "Epoch: 1415 loss_train: 0.0667 loss_val: 0.0561\n",
      "Epoch: 1416 loss_train: 0.0658 loss_val: 0.0558\n",
      "Epoch: 1417 loss_train: 0.0659 loss_val: 0.0557\n",
      "Epoch: 1418 loss_train: 0.0658 loss_val: 0.0561\n",
      "Epoch: 1419 loss_train: 0.0657 loss_val: 0.0574\n",
      "Epoch: 1420 loss_train: 0.0655 loss_val: 0.0553\n",
      "Epoch: 1421 loss_train: 0.0656 loss_val: 0.0565\n",
      "Epoch: 1422 loss_train: 0.0663 loss_val: 0.0579\n",
      "Epoch: 1423 loss_train: 0.0657 loss_val: 0.0576\n",
      "Epoch: 1424 loss_train: 0.0652 loss_val: 0.0548\n",
      "Epoch: 1425 loss_train: 0.0653 loss_val: 0.0554\n",
      "Epoch: 1426 loss_train: 0.0653 loss_val: 0.0562\n",
      "Epoch: 1427 loss_train: 0.0667 loss_val: 0.0559\n",
      "Epoch: 1428 loss_train: 0.0657 loss_val: 0.0541\n",
      "Epoch: 1429 loss_train: 0.0652 loss_val: 0.0540\n",
      "Epoch: 1430 loss_train: 0.0657 loss_val: 0.0557\n",
      "Epoch: 1431 loss_train: 0.0649 loss_val: 0.0552\n",
      "Epoch: 1432 loss_train: 0.0660 loss_val: 0.0584\n",
      "Epoch: 1433 loss_train: 0.0659 loss_val: 0.0582\n",
      "Epoch: 1434 loss_train: 0.0661 loss_val: 0.0558\n",
      "Epoch: 1435 loss_train: 0.0643 loss_val: 0.0565\n",
      "Epoch: 1436 loss_train: 0.0668 loss_val: 0.0566\n",
      "Epoch: 1437 loss_train: 0.0659 loss_val: 0.0549\n",
      "Epoch: 1438 loss_train: 0.0671 loss_val: 0.0567\n",
      "Epoch: 1439 loss_train: 0.0660 loss_val: 0.0562\n",
      "Epoch: 1440 loss_train: 0.0645 loss_val: 0.0548\n",
      "Epoch: 1441 loss_train: 0.0646 loss_val: 0.0546\n",
      "Epoch: 1442 loss_train: 0.0650 loss_val: 0.0545\n",
      "Epoch: 1443 loss_train: 0.0648 loss_val: 0.0560\n",
      "Epoch: 1444 loss_train: 0.0648 loss_val: 0.0569\n",
      "Epoch: 1445 loss_train: 0.0664 loss_val: 0.0580\n",
      "Epoch: 1446 loss_train: 0.0659 loss_val: 0.0551\n",
      "Epoch: 1447 loss_train: 0.0662 loss_val: 0.0552\n",
      "Epoch: 1448 loss_train: 0.0659 loss_val: 0.0563\n",
      "Epoch: 1449 loss_train: 0.0657 loss_val: 0.0548\n",
      "Epoch: 1450 loss_train: 0.0654 loss_val: 0.0553\n",
      "Epoch: 1451 loss_train: 0.0655 loss_val: 0.0570\n",
      "Epoch: 1452 loss_train: 0.0662 loss_val: 0.0566\n",
      "Epoch: 1453 loss_train: 0.0655 loss_val: 0.0561\n",
      "Epoch: 1454 loss_train: 0.0656 loss_val: 0.0551\n",
      "Epoch: 1455 loss_train: 0.0649 loss_val: 0.0557\n",
      "Epoch: 1456 loss_train: 0.0663 loss_val: 0.0564\n",
      "Epoch: 1457 loss_train: 0.0653 loss_val: 0.0549\n",
      "Epoch: 1458 loss_train: 0.0660 loss_val: 0.0547\n",
      "Epoch: 1459 loss_train: 0.0669 loss_val: 0.0571\n",
      "Epoch: 1460 loss_train: 0.0669 loss_val: 0.0551\n",
      "Epoch: 1461 loss_train: 0.0675 loss_val: 0.0534\n",
      "Epoch: 1462 loss_train: 0.0669 loss_val: 0.0568\n",
      "Epoch: 1463 loss_train: 0.0655 loss_val: 0.0574\n",
      "Epoch: 1464 loss_train: 0.0663 loss_val: 0.0557\n",
      "Epoch: 1465 loss_train: 0.0657 loss_val: 0.0537\n",
      "Epoch: 1466 loss_train: 0.0653 loss_val: 0.0549\n",
      "Epoch: 1467 loss_train: 0.0657 loss_val: 0.0560\n",
      "Epoch: 1468 loss_train: 0.0650 loss_val: 0.0543\n",
      "Epoch: 1469 loss_train: 0.0647 loss_val: 0.0560\n",
      "Epoch: 1470 loss_train: 0.0645 loss_val: 0.0558\n",
      "Epoch: 1471 loss_train: 0.0651 loss_val: 0.0565\n",
      "Epoch: 1472 loss_train: 0.0659 loss_val: 0.0551\n",
      "Epoch: 1473 loss_train: 0.0645 loss_val: 0.0551\n",
      "Epoch: 1474 loss_train: 0.0654 loss_val: 0.0538\n",
      "Epoch: 1475 loss_train: 0.0667 loss_val: 0.0541\n",
      "Epoch: 1476 loss_train: 0.0650 loss_val: 0.0540\n",
      "Epoch: 1477 loss_train: 0.0649 loss_val: 0.0543\n",
      "Epoch: 1478 loss_train: 0.0648 loss_val: 0.0546\n",
      "Epoch: 1479 loss_train: 0.0663 loss_val: 0.0566\n",
      "Epoch: 1480 loss_train: 0.0659 loss_val: 0.0570\n",
      "Epoch: 1481 loss_train: 0.0654 loss_val: 0.0563\n",
      "Epoch: 1482 loss_train: 0.0655 loss_val: 0.0564\n",
      "Epoch: 1483 loss_train: 0.0648 loss_val: 0.0557\n",
      "Epoch: 1484 loss_train: 0.0665 loss_val: 0.0558\n",
      "Epoch: 1485 loss_train: 0.0672 loss_val: 0.0557\n",
      "Epoch: 1486 loss_train: 0.0657 loss_val: 0.0532\n",
      "Epoch: 1487 loss_train: 0.0649 loss_val: 0.0528\n",
      "Epoch: 1488 loss_train: 0.0643 loss_val: 0.0552\n",
      "Epoch: 1489 loss_train: 0.0654 loss_val: 0.0552\n",
      "Epoch: 1490 loss_train: 0.0653 loss_val: 0.0542\n",
      "Epoch: 1491 loss_train: 0.0652 loss_val: 0.0549\n",
      "Epoch: 1492 loss_train: 0.0662 loss_val: 0.0550\n",
      "Epoch: 1493 loss_train: 0.0656 loss_val: 0.0563\n",
      "Epoch: 1494 loss_train: 0.0667 loss_val: 0.0563\n",
      "Epoch: 1495 loss_train: 0.0649 loss_val: 0.0564\n",
      "Epoch: 1496 loss_train: 0.0658 loss_val: 0.0567\n",
      "Epoch: 1497 loss_train: 0.0662 loss_val: 0.0547\n",
      "Epoch: 1498 loss_train: 0.0645 loss_val: 0.0543\n",
      "Epoch: 1499 loss_train: 0.0649 loss_val: 0.0557\n",
      "Epoch: 1500 loss_train: 0.0657 loss_val: 0.0547\n",
      "Epoch: 1501 loss_train: 0.0659 loss_val: 0.0547\n",
      "Epoch: 1502 loss_train: 0.0668 loss_val: 0.0558\n",
      "Epoch: 1503 loss_train: 0.0651 loss_val: 0.0567\n",
      "Epoch: 1504 loss_train: 0.0656 loss_val: 0.0555\n",
      "Epoch: 1505 loss_train: 0.0655 loss_val: 0.0539\n",
      "Epoch: 1506 loss_train: 0.0656 loss_val: 0.0554\n",
      "Epoch: 1507 loss_train: 0.0655 loss_val: 0.0559\n",
      "Epoch: 1508 loss_train: 0.0663 loss_val: 0.0563\n",
      "Epoch: 1509 loss_train: 0.0654 loss_val: 0.0563\n",
      "Epoch: 1510 loss_train: 0.0654 loss_val: 0.0565\n",
      "Epoch: 1511 loss_train: 0.0638 loss_val: 0.0559\n",
      "Epoch: 1512 loss_train: 0.0655 loss_val: 0.0544\n",
      "Epoch: 1513 loss_train: 0.0657 loss_val: 0.0545\n",
      "Epoch: 1514 loss_train: 0.0651 loss_val: 0.0545\n",
      "Epoch: 1515 loss_train: 0.0654 loss_val: 0.0548\n",
      "Epoch: 1516 loss_train: 0.0653 loss_val: 0.0554\n",
      "Epoch: 1517 loss_train: 0.0662 loss_val: 0.0558\n",
      "Epoch: 1518 loss_train: 0.0660 loss_val: 0.0551\n",
      "Epoch: 1519 loss_train: 0.0664 loss_val: 0.0578\n",
      "Epoch: 1520 loss_train: 0.0663 loss_val: 0.0561\n",
      "Epoch: 1521 loss_train: 0.0649 loss_val: 0.0564\n",
      "Epoch: 1522 loss_train: 0.0651 loss_val: 0.0570\n",
      "Epoch: 1523 loss_train: 0.0652 loss_val: 0.0585\n",
      "Epoch: 1524 loss_train: 0.0654 loss_val: 0.0564\n",
      "Epoch: 1525 loss_train: 0.0660 loss_val: 0.0559\n",
      "Epoch: 1526 loss_train: 0.0661 loss_val: 0.0558\n",
      "Epoch: 1527 loss_train: 0.0658 loss_val: 0.0545\n",
      "Epoch: 1528 loss_train: 0.0656 loss_val: 0.0579\n",
      "Epoch: 1529 loss_train: 0.0661 loss_val: 0.0546\n",
      "Epoch: 1530 loss_train: 0.0648 loss_val: 0.0536\n",
      "Epoch: 1531 loss_train: 0.0647 loss_val: 0.0548\n",
      "Epoch: 1532 loss_train: 0.0655 loss_val: 0.0546\n",
      "Epoch: 1533 loss_train: 0.0665 loss_val: 0.0571\n",
      "Epoch: 1534 loss_train: 0.0674 loss_val: 0.0556\n",
      "Epoch: 1535 loss_train: 0.0655 loss_val: 0.0573\n",
      "Epoch: 1536 loss_train: 0.0652 loss_val: 0.0566\n",
      "Epoch: 1537 loss_train: 0.0650 loss_val: 0.0564\n",
      "Epoch: 1538 loss_train: 0.0641 loss_val: 0.0561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1539 loss_train: 0.0654 loss_val: 0.0569\n",
      "Epoch: 1540 loss_train: 0.0660 loss_val: 0.0551\n",
      "Epoch: 1541 loss_train: 0.0657 loss_val: 0.0573\n",
      "Epoch: 1542 loss_train: 0.0660 loss_val: 0.0560\n",
      "Epoch: 1543 loss_train: 0.0655 loss_val: 0.0537\n",
      "Epoch: 1544 loss_train: 0.0647 loss_val: 0.0536\n",
      "Epoch: 1545 loss_train: 0.0645 loss_val: 0.0543\n",
      "Epoch: 1546 loss_train: 0.0644 loss_val: 0.0550\n",
      "Epoch: 1547 loss_train: 0.0640 loss_val: 0.0529\n",
      "Epoch: 1548 loss_train: 0.0643 loss_val: 0.0551\n",
      "Epoch: 1549 loss_train: 0.0653 loss_val: 0.0538\n",
      "Epoch: 1550 loss_train: 0.0646 loss_val: 0.0541\n",
      "Epoch: 1551 loss_train: 0.0649 loss_val: 0.0546\n",
      "Epoch: 1552 loss_train: 0.0652 loss_val: 0.0540\n",
      "Epoch: 1553 loss_train: 0.0641 loss_val: 0.0555\n",
      "Epoch: 1554 loss_train: 0.0650 loss_val: 0.0549\n",
      "Epoch: 1555 loss_train: 0.0648 loss_val: 0.0544\n",
      "Epoch: 1556 loss_train: 0.0644 loss_val: 0.0566\n",
      "Epoch: 1557 loss_train: 0.0656 loss_val: 0.0555\n",
      "Epoch: 1558 loss_train: 0.0666 loss_val: 0.0551\n",
      "Epoch: 1559 loss_train: 0.0646 loss_val: 0.0566\n",
      "Epoch: 1560 loss_train: 0.0655 loss_val: 0.0550\n",
      "Epoch: 1561 loss_train: 0.0660 loss_val: 0.0541\n",
      "Epoch: 1562 loss_train: 0.0662 loss_val: 0.0558\n",
      "Epoch: 1563 loss_train: 0.0647 loss_val: 0.0554\n",
      "Epoch: 1564 loss_train: 0.0650 loss_val: 0.0561\n",
      "Epoch: 1565 loss_train: 0.0645 loss_val: 0.0549\n",
      "Epoch: 1566 loss_train: 0.0652 loss_val: 0.0566\n",
      "Epoch: 1567 loss_train: 0.0647 loss_val: 0.0577\n",
      "Epoch: 1568 loss_train: 0.0647 loss_val: 0.0540\n",
      "Epoch: 1569 loss_train: 0.0647 loss_val: 0.0539\n",
      "Epoch: 1570 loss_train: 0.0642 loss_val: 0.0544\n",
      "Epoch: 1571 loss_train: 0.0655 loss_val: 0.0547\n",
      "Epoch: 1572 loss_train: 0.0647 loss_val: 0.0546\n",
      "Epoch: 1573 loss_train: 0.0639 loss_val: 0.0557\n",
      "Epoch: 1574 loss_train: 0.0638 loss_val: 0.0563\n",
      "Epoch: 1575 loss_train: 0.0654 loss_val: 0.0545\n",
      "Epoch: 1576 loss_train: 0.0654 loss_val: 0.0551\n",
      "Epoch: 1577 loss_train: 0.0652 loss_val: 0.0555\n",
      "Epoch: 1578 loss_train: 0.0649 loss_val: 0.0553\n",
      "Epoch: 1579 loss_train: 0.0653 loss_val: 0.0561\n",
      "Epoch: 1580 loss_train: 0.0641 loss_val: 0.0571\n",
      "Epoch: 1581 loss_train: 0.0653 loss_val: 0.0541\n",
      "Epoch: 1582 loss_train: 0.0659 loss_val: 0.0560\n",
      "Epoch: 1583 loss_train: 0.0641 loss_val: 0.0545\n",
      "Epoch: 1584 loss_train: 0.0638 loss_val: 0.0548\n",
      "Epoch: 1585 loss_train: 0.0647 loss_val: 0.0546\n",
      "Epoch: 1586 loss_train: 0.0651 loss_val: 0.0564\n",
      "Epoch: 1587 loss_train: 0.0651 loss_val: 0.0579\n",
      "Epoch: 1588 loss_train: 0.0642 loss_val: 0.0563\n",
      "Epoch: 1589 loss_train: 0.0659 loss_val: 0.0568\n",
      "Epoch: 1590 loss_train: 0.0638 loss_val: 0.0576\n",
      "Epoch: 1591 loss_train: 0.0649 loss_val: 0.0588\n",
      "Epoch: 1592 loss_train: 0.0645 loss_val: 0.0573\n",
      "Epoch: 1593 loss_train: 0.0649 loss_val: 0.0548\n",
      "Epoch: 1594 loss_train: 0.0648 loss_val: 0.0556\n",
      "Epoch: 1595 loss_train: 0.0668 loss_val: 0.0573\n",
      "Epoch: 1596 loss_train: 0.0653 loss_val: 0.0575\n",
      "Epoch: 1597 loss_train: 0.0660 loss_val: 0.0566\n",
      "Epoch: 1598 loss_train: 0.0642 loss_val: 0.0561\n",
      "Epoch: 1599 loss_train: 0.0639 loss_val: 0.0564\n",
      "Epoch: 1600 loss_train: 0.0653 loss_val: 0.0568\n",
      "Epoch: 1601 loss_train: 0.0646 loss_val: 0.0567\n",
      "Epoch: 1602 loss_train: 0.0647 loss_val: 0.0556\n",
      "Epoch: 1603 loss_train: 0.0647 loss_val: 0.0566\n",
      "Epoch: 1604 loss_train: 0.0640 loss_val: 0.0554\n",
      "Epoch: 1605 loss_train: 0.0639 loss_val: 0.0580\n",
      "Epoch: 1606 loss_train: 0.0660 loss_val: 0.0582\n",
      "Epoch: 1607 loss_train: 0.0653 loss_val: 0.0573\n",
      "Epoch: 1608 loss_train: 0.0660 loss_val: 0.0565\n",
      "Epoch: 1609 loss_train: 0.0634 loss_val: 0.0553\n",
      "Epoch: 1610 loss_train: 0.0636 loss_val: 0.0554\n",
      "Epoch: 1611 loss_train: 0.0643 loss_val: 0.0564\n",
      "Epoch: 1612 loss_train: 0.0661 loss_val: 0.0578\n",
      "Epoch: 1613 loss_train: 0.0653 loss_val: 0.0544\n",
      "Epoch: 1614 loss_train: 0.0655 loss_val: 0.0549\n",
      "Epoch: 1615 loss_train: 0.0645 loss_val: 0.0551\n",
      "Epoch: 1616 loss_train: 0.0651 loss_val: 0.0538\n",
      "Epoch: 1617 loss_train: 0.0648 loss_val: 0.0553\n",
      "Epoch: 1618 loss_train: 0.0654 loss_val: 0.0565\n",
      "Epoch: 1619 loss_train: 0.0655 loss_val: 0.0557\n",
      "Epoch: 1620 loss_train: 0.0654 loss_val: 0.0560\n",
      "Epoch: 1621 loss_train: 0.0650 loss_val: 0.0579\n",
      "Epoch: 1622 loss_train: 0.0655 loss_val: 0.0564\n",
      "Epoch: 1623 loss_train: 0.0648 loss_val: 0.0572\n",
      "Epoch: 1624 loss_train: 0.0657 loss_val: 0.0577\n",
      "Epoch: 1625 loss_train: 0.0642 loss_val: 0.0559\n",
      "Epoch: 1626 loss_train: 0.0651 loss_val: 0.0587\n",
      "Epoch: 1627 loss_train: 0.0658 loss_val: 0.0551\n",
      "Epoch: 1628 loss_train: 0.0637 loss_val: 0.0560\n",
      "Epoch: 1629 loss_train: 0.0637 loss_val: 0.0563\n",
      "Epoch: 1630 loss_train: 0.0644 loss_val: 0.0544\n",
      "Epoch: 1631 loss_train: 0.0647 loss_val: 0.0569\n",
      "Epoch: 1632 loss_train: 0.0639 loss_val: 0.0549\n",
      "Epoch: 1633 loss_train: 0.0650 loss_val: 0.0549\n",
      "Epoch: 1634 loss_train: 0.0636 loss_val: 0.0559\n",
      "Epoch: 1635 loss_train: 0.0634 loss_val: 0.0563\n",
      "Epoch: 1636 loss_train: 0.0639 loss_val: 0.0543\n",
      "Epoch: 1637 loss_train: 0.0639 loss_val: 0.0573\n",
      "Epoch: 1638 loss_train: 0.0653 loss_val: 0.0560\n",
      "Epoch: 1639 loss_train: 0.0651 loss_val: 0.0557\n",
      "Epoch: 1640 loss_train: 0.0641 loss_val: 0.0558\n",
      "Epoch: 1641 loss_train: 0.0654 loss_val: 0.0571\n",
      "Epoch: 1642 loss_train: 0.0639 loss_val: 0.0563\n",
      "Epoch: 1643 loss_train: 0.0634 loss_val: 0.0547\n",
      "Epoch: 1644 loss_train: 0.0648 loss_val: 0.0571\n",
      "Epoch: 1645 loss_train: 0.0651 loss_val: 0.0580\n",
      "Epoch: 1646 loss_train: 0.0644 loss_val: 0.0554\n",
      "Epoch: 1647 loss_train: 0.0624 loss_val: 0.0566\n",
      "Epoch: 1648 loss_train: 0.0653 loss_val: 0.0558\n",
      "Epoch: 1649 loss_train: 0.0646 loss_val: 0.0548\n",
      "Epoch: 1650 loss_train: 0.0658 loss_val: 0.0536\n",
      "Epoch: 1651 loss_train: 0.0642 loss_val: 0.0544\n",
      "Epoch: 1652 loss_train: 0.0640 loss_val: 0.0550\n",
      "Epoch: 1653 loss_train: 0.0634 loss_val: 0.0574\n",
      "Epoch: 1654 loss_train: 0.0643 loss_val: 0.0566\n",
      "Epoch: 1655 loss_train: 0.0653 loss_val: 0.0529\n",
      "Epoch: 1656 loss_train: 0.0629 loss_val: 0.0534\n",
      "Epoch: 1657 loss_train: 0.0646 loss_val: 0.0558\n",
      "Epoch: 1658 loss_train: 0.0650 loss_val: 0.0530\n",
      "Epoch: 1659 loss_train: 0.0630 loss_val: 0.0529\n",
      "Epoch: 1660 loss_train: 0.0639 loss_val: 0.0545\n",
      "Epoch: 1661 loss_train: 0.0662 loss_val: 0.0542\n",
      "Epoch: 1662 loss_train: 0.0649 loss_val: 0.0562\n",
      "Epoch: 1663 loss_train: 0.0645 loss_val: 0.0567\n",
      "Epoch: 1664 loss_train: 0.0642 loss_val: 0.0559\n",
      "Epoch: 1665 loss_train: 0.0644 loss_val: 0.0540\n",
      "Epoch: 1666 loss_train: 0.0639 loss_val: 0.0553\n",
      "Epoch: 1667 loss_train: 0.0662 loss_val: 0.0544\n",
      "Epoch: 1668 loss_train: 0.0660 loss_val: 0.0551\n",
      "Epoch: 1669 loss_train: 0.0641 loss_val: 0.0567\n",
      "Epoch: 1670 loss_train: 0.0643 loss_val: 0.0556\n",
      "Epoch: 1671 loss_train: 0.0649 loss_val: 0.0550\n",
      "Epoch: 1672 loss_train: 0.0649 loss_val: 0.0550\n",
      "Epoch: 1673 loss_train: 0.0642 loss_val: 0.0562\n",
      "Epoch: 1674 loss_train: 0.0634 loss_val: 0.0553\n",
      "Epoch: 1675 loss_train: 0.0658 loss_val: 0.0564\n",
      "Epoch: 1676 loss_train: 0.0647 loss_val: 0.0552\n",
      "Epoch: 1677 loss_train: 0.0642 loss_val: 0.0557\n",
      "Epoch: 1678 loss_train: 0.0636 loss_val: 0.0542\n",
      "Epoch: 1679 loss_train: 0.0637 loss_val: 0.0563\n",
      "Epoch: 1680 loss_train: 0.0644 loss_val: 0.0546\n",
      "Epoch: 1681 loss_train: 0.0639 loss_val: 0.0542\n",
      "Epoch: 1682 loss_train: 0.0645 loss_val: 0.0547\n",
      "Epoch: 1683 loss_train: 0.0649 loss_val: 0.0542\n",
      "Epoch: 1684 loss_train: 0.0635 loss_val: 0.0564\n",
      "Epoch: 1685 loss_train: 0.0640 loss_val: 0.0553\n",
      "Epoch: 1686 loss_train: 0.0648 loss_val: 0.0564\n",
      "Epoch: 1687 loss_train: 0.0639 loss_val: 0.0559\n",
      "Epoch: 1688 loss_train: 0.0647 loss_val: 0.0561\n",
      "Epoch: 1689 loss_train: 0.0642 loss_val: 0.0562\n",
      "Epoch: 1690 loss_train: 0.0653 loss_val: 0.0555\n",
      "Epoch: 1691 loss_train: 0.0645 loss_val: 0.0534\n",
      "Epoch: 1692 loss_train: 0.0648 loss_val: 0.0542\n",
      "Epoch: 1693 loss_train: 0.0646 loss_val: 0.0566\n",
      "Epoch: 1694 loss_train: 0.0644 loss_val: 0.0561\n",
      "Epoch: 1695 loss_train: 0.0642 loss_val: 0.0549\n",
      "Epoch: 1696 loss_train: 0.0647 loss_val: 0.0548\n",
      "Epoch: 1697 loss_train: 0.0647 loss_val: 0.0554\n",
      "Epoch: 1698 loss_train: 0.0660 loss_val: 0.0563\n",
      "Epoch: 1699 loss_train: 0.0648 loss_val: 0.0578\n",
      "Epoch: 1700 loss_train: 0.0658 loss_val: 0.0546\n",
      "Epoch: 1701 loss_train: 0.0651 loss_val: 0.0563\n",
      "Epoch: 1702 loss_train: 0.0640 loss_val: 0.0563\n",
      "Epoch: 1703 loss_train: 0.0642 loss_val: 0.0560\n",
      "Epoch: 1704 loss_train: 0.0650 loss_val: 0.0549\n",
      "Epoch: 1705 loss_train: 0.0656 loss_val: 0.0546\n",
      "Epoch: 1706 loss_train: 0.0651 loss_val: 0.0545\n",
      "Epoch: 1707 loss_train: 0.0643 loss_val: 0.0549\n",
      "Epoch: 1708 loss_train: 0.0651 loss_val: 0.0541\n",
      "Epoch: 1709 loss_train: 0.0652 loss_val: 0.0541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1710 loss_train: 0.0653 loss_val: 0.0531\n",
      "Epoch: 1711 loss_train: 0.0659 loss_val: 0.0545\n",
      "Epoch: 1712 loss_train: 0.0658 loss_val: 0.0549\n",
      "Epoch: 1713 loss_train: 0.0662 loss_val: 0.0551\n",
      "Epoch: 1714 loss_train: 0.0651 loss_val: 0.0553\n",
      "Epoch: 1715 loss_train: 0.0637 loss_val: 0.0533\n",
      "Epoch: 1716 loss_train: 0.0636 loss_val: 0.0530\n",
      "Epoch: 1717 loss_train: 0.0644 loss_val: 0.0525\n",
      "Epoch: 1718 loss_train: 0.0647 loss_val: 0.0560\n",
      "Epoch: 1719 loss_train: 0.0639 loss_val: 0.0544\n",
      "Epoch: 1720 loss_train: 0.0651 loss_val: 0.0539\n",
      "Epoch: 1721 loss_train: 0.0647 loss_val: 0.0544\n",
      "Epoch: 1722 loss_train: 0.0655 loss_val: 0.0538\n",
      "Epoch: 1723 loss_train: 0.0649 loss_val: 0.0544\n",
      "Epoch: 1724 loss_train: 0.0655 loss_val: 0.0543\n",
      "Epoch: 1725 loss_train: 0.0647 loss_val: 0.0528\n",
      "Epoch: 1726 loss_train: 0.0649 loss_val: 0.0551\n",
      "Epoch: 1727 loss_train: 0.0649 loss_val: 0.0548\n",
      "Epoch: 1728 loss_train: 0.0636 loss_val: 0.0548\n",
      "Epoch: 1729 loss_train: 0.0655 loss_val: 0.0567\n",
      "Epoch: 1730 loss_train: 0.0635 loss_val: 0.0559\n",
      "Epoch: 1731 loss_train: 0.0639 loss_val: 0.0543\n",
      "Epoch: 1732 loss_train: 0.0652 loss_val: 0.0539\n",
      "Epoch: 1733 loss_train: 0.0627 loss_val: 0.0545\n",
      "Epoch: 1734 loss_train: 0.0639 loss_val: 0.0554\n",
      "Epoch: 1735 loss_train: 0.0651 loss_val: 0.0550\n",
      "Epoch: 1736 loss_train: 0.0640 loss_val: 0.0565\n",
      "Epoch: 1737 loss_train: 0.0655 loss_val: 0.0549\n",
      "Epoch: 1738 loss_train: 0.0646 loss_val: 0.0548\n",
      "Epoch: 1739 loss_train: 0.0632 loss_val: 0.0529\n",
      "Epoch: 1740 loss_train: 0.0636 loss_val: 0.0535\n",
      "Epoch: 1741 loss_train: 0.0656 loss_val: 0.0551\n",
      "Epoch: 1742 loss_train: 0.0642 loss_val: 0.0539\n",
      "Epoch: 1743 loss_train: 0.0634 loss_val: 0.0551\n",
      "Epoch: 1744 loss_train: 0.0636 loss_val: 0.0538\n",
      "Epoch: 1745 loss_train: 0.0636 loss_val: 0.0533\n",
      "Epoch: 1746 loss_train: 0.0634 loss_val: 0.0535\n",
      "Epoch: 1747 loss_train: 0.0645 loss_val: 0.0541\n",
      "Epoch: 1748 loss_train: 0.0629 loss_val: 0.0547\n",
      "Epoch: 1749 loss_train: 0.0642 loss_val: 0.0542\n",
      "Epoch: 1750 loss_train: 0.0638 loss_val: 0.0553\n",
      "Epoch: 1751 loss_train: 0.0646 loss_val: 0.0542\n",
      "Epoch: 1752 loss_train: 0.0625 loss_val: 0.0536\n",
      "Epoch: 1753 loss_train: 0.0642 loss_val: 0.0543\n",
      "Epoch: 1754 loss_train: 0.0648 loss_val: 0.0552\n",
      "Epoch: 1755 loss_train: 0.0640 loss_val: 0.0543\n",
      "Epoch: 1756 loss_train: 0.0630 loss_val: 0.0542\n",
      "Epoch: 1757 loss_train: 0.0641 loss_val: 0.0538\n",
      "Epoch: 1758 loss_train: 0.0637 loss_val: 0.0531\n",
      "Epoch: 1759 loss_train: 0.0644 loss_val: 0.0536\n",
      "Epoch: 1760 loss_train: 0.0639 loss_val: 0.0550\n",
      "Epoch: 1761 loss_train: 0.0621 loss_val: 0.0534\n",
      "Epoch: 1762 loss_train: 0.0626 loss_val: 0.0534\n",
      "Epoch: 1763 loss_train: 0.0623 loss_val: 0.0556\n",
      "Epoch: 1764 loss_train: 0.0635 loss_val: 0.0548\n",
      "Epoch: 1765 loss_train: 0.0643 loss_val: 0.0533\n",
      "Epoch: 1766 loss_train: 0.0632 loss_val: 0.0546\n",
      "Epoch: 1767 loss_train: 0.0629 loss_val: 0.0541\n",
      "Epoch: 1768 loss_train: 0.0640 loss_val: 0.0537\n",
      "Epoch: 1769 loss_train: 0.0642 loss_val: 0.0576\n",
      "Epoch: 1770 loss_train: 0.0644 loss_val: 0.0547\n",
      "Epoch: 1771 loss_train: 0.0630 loss_val: 0.0553\n",
      "Epoch: 1772 loss_train: 0.0629 loss_val: 0.0549\n",
      "Epoch: 1773 loss_train: 0.0627 loss_val: 0.0544\n",
      "Epoch: 1774 loss_train: 0.0648 loss_val: 0.0556\n",
      "Epoch: 1775 loss_train: 0.0635 loss_val: 0.0545\n",
      "Epoch: 1776 loss_train: 0.0651 loss_val: 0.0546\n",
      "Epoch: 1777 loss_train: 0.0634 loss_val: 0.0561\n",
      "Epoch: 1778 loss_train: 0.0635 loss_val: 0.0545\n",
      "Epoch: 1779 loss_train: 0.0640 loss_val: 0.0569\n",
      "Epoch: 1780 loss_train: 0.0652 loss_val: 0.0553\n",
      "Epoch: 1781 loss_train: 0.0644 loss_val: 0.0564\n",
      "Epoch: 1782 loss_train: 0.0650 loss_val: 0.0561\n",
      "Epoch: 1783 loss_train: 0.0654 loss_val: 0.0580\n",
      "Epoch: 1784 loss_train: 0.0651 loss_val: 0.0567\n",
      "Epoch: 1785 loss_train: 0.0630 loss_val: 0.0541\n",
      "Epoch: 1786 loss_train: 0.0636 loss_val: 0.0553\n",
      "Epoch: 1787 loss_train: 0.0632 loss_val: 0.0538\n",
      "Epoch: 1788 loss_train: 0.0624 loss_val: 0.0559\n",
      "Epoch: 1789 loss_train: 0.0637 loss_val: 0.0567\n",
      "Epoch: 1790 loss_train: 0.0620 loss_val: 0.0546\n",
      "Epoch: 1791 loss_train: 0.0630 loss_val: 0.0544\n",
      "Epoch: 1792 loss_train: 0.0626 loss_val: 0.0571\n",
      "Epoch: 1793 loss_train: 0.0636 loss_val: 0.0552\n",
      "Epoch: 1794 loss_train: 0.0641 loss_val: 0.0563\n",
      "Epoch: 1795 loss_train: 0.0643 loss_val: 0.0571\n",
      "Epoch: 1796 loss_train: 0.0643 loss_val: 0.0553\n",
      "Epoch: 1797 loss_train: 0.0642 loss_val: 0.0569\n",
      "Epoch: 1798 loss_train: 0.0651 loss_val: 0.0572\n",
      "Epoch: 1799 loss_train: 0.0647 loss_val: 0.0547\n",
      "Epoch: 1800 loss_train: 0.0641 loss_val: 0.0547\n",
      "Epoch: 1801 loss_train: 0.0635 loss_val: 0.0560\n",
      "Epoch: 1802 loss_train: 0.0640 loss_val: 0.0544\n",
      "Epoch: 1803 loss_train: 0.0640 loss_val: 0.0563\n",
      "Epoch: 1804 loss_train: 0.0648 loss_val: 0.0552\n",
      "Epoch: 1805 loss_train: 0.0645 loss_val: 0.0560\n",
      "Epoch: 1806 loss_train: 0.0649 loss_val: 0.0541\n",
      "Epoch: 1807 loss_train: 0.0645 loss_val: 0.0542\n",
      "Epoch: 1808 loss_train: 0.0632 loss_val: 0.0558\n",
      "Epoch: 1809 loss_train: 0.0638 loss_val: 0.0552\n",
      "Epoch: 1810 loss_train: 0.0645 loss_val: 0.0549\n",
      "Epoch: 1811 loss_train: 0.0627 loss_val: 0.0554\n",
      "Epoch: 1812 loss_train: 0.0632 loss_val: 0.0554\n",
      "Epoch: 1813 loss_train: 0.0640 loss_val: 0.0562\n",
      "Epoch: 1814 loss_train: 0.0659 loss_val: 0.0561\n",
      "Epoch: 1815 loss_train: 0.0650 loss_val: 0.0546\n",
      "Epoch: 1816 loss_train: 0.0633 loss_val: 0.0566\n",
      "Epoch: 1817 loss_train: 0.0643 loss_val: 0.0555\n",
      "Epoch: 1818 loss_train: 0.0620 loss_val: 0.0547\n",
      "Epoch: 1819 loss_train: 0.0637 loss_val: 0.0551\n",
      "Epoch: 1820 loss_train: 0.0649 loss_val: 0.0539\n",
      "Epoch: 1821 loss_train: 0.0623 loss_val: 0.0544\n",
      "Epoch: 1822 loss_train: 0.0655 loss_val: 0.0582\n",
      "Epoch: 1823 loss_train: 0.0641 loss_val: 0.0571\n",
      "Epoch: 1824 loss_train: 0.0640 loss_val: 0.0567\n",
      "Epoch: 1825 loss_train: 0.0647 loss_val: 0.0561\n",
      "Epoch: 1826 loss_train: 0.0630 loss_val: 0.0551\n",
      "Epoch: 1827 loss_train: 0.0642 loss_val: 0.0553\n",
      "Epoch: 1828 loss_train: 0.0642 loss_val: 0.0548\n",
      "Epoch: 1829 loss_train: 0.0643 loss_val: 0.0549\n",
      "Epoch: 1830 loss_train: 0.0649 loss_val: 0.0543\n",
      "Epoch: 1831 loss_train: 0.0642 loss_val: 0.0543\n",
      "Epoch: 1832 loss_train: 0.0654 loss_val: 0.0548\n",
      "Epoch: 1833 loss_train: 0.0647 loss_val: 0.0529\n",
      "Epoch: 1834 loss_train: 0.0632 loss_val: 0.0558\n",
      "Epoch: 1835 loss_train: 0.0635 loss_val: 0.0557\n",
      "Epoch: 1836 loss_train: 0.0642 loss_val: 0.0550\n",
      "Epoch: 1837 loss_train: 0.0639 loss_val: 0.0548\n",
      "Epoch: 1838 loss_train: 0.0651 loss_val: 0.0550\n",
      "Epoch: 1839 loss_train: 0.0640 loss_val: 0.0578\n",
      "Epoch: 1840 loss_train: 0.0646 loss_val: 0.0545\n",
      "Epoch: 1841 loss_train: 0.0648 loss_val: 0.0568\n",
      "Epoch: 1842 loss_train: 0.0634 loss_val: 0.0569\n",
      "Epoch: 1843 loss_train: 0.0630 loss_val: 0.0557\n",
      "Epoch: 1844 loss_train: 0.0634 loss_val: 0.0556\n",
      "Epoch: 1845 loss_train: 0.0640 loss_val: 0.0556\n",
      "Epoch: 1846 loss_train: 0.0638 loss_val: 0.0576\n",
      "Epoch: 1847 loss_train: 0.0636 loss_val: 0.0565\n",
      "Epoch: 1848 loss_train: 0.0624 loss_val: 0.0541\n",
      "Epoch: 1849 loss_train: 0.0624 loss_val: 0.0543\n",
      "Epoch: 1850 loss_train: 0.0627 loss_val: 0.0553\n",
      "Epoch: 1851 loss_train: 0.0631 loss_val: 0.0545\n",
      "Epoch: 1852 loss_train: 0.0622 loss_val: 0.0567\n",
      "Epoch: 1853 loss_train: 0.0619 loss_val: 0.0557\n",
      "Epoch: 1854 loss_train: 0.0625 loss_val: 0.0545\n",
      "Epoch: 1855 loss_train: 0.0618 loss_val: 0.0552\n",
      "Epoch: 1856 loss_train: 0.0633 loss_val: 0.0549\n",
      "Epoch: 1857 loss_train: 0.0646 loss_val: 0.0562\n",
      "Epoch: 1858 loss_train: 0.0649 loss_val: 0.0570\n",
      "Epoch: 1859 loss_train: 0.0646 loss_val: 0.0565\n",
      "Epoch: 1860 loss_train: 0.0639 loss_val: 0.0558\n",
      "Epoch: 1861 loss_train: 0.0633 loss_val: 0.0562\n",
      "Epoch: 1862 loss_train: 0.0622 loss_val: 0.0562\n",
      "Epoch: 1863 loss_train: 0.0636 loss_val: 0.0544\n",
      "Epoch: 1864 loss_train: 0.0635 loss_val: 0.0559\n",
      "Epoch: 1865 loss_train: 0.0624 loss_val: 0.0560\n",
      "Epoch: 1866 loss_train: 0.0635 loss_val: 0.0561\n",
      "Epoch: 1867 loss_train: 0.0644 loss_val: 0.0551\n",
      "Epoch: 1868 loss_train: 0.0630 loss_val: 0.0539\n",
      "Epoch: 1869 loss_train: 0.0630 loss_val: 0.0552\n",
      "Epoch: 1870 loss_train: 0.0638 loss_val: 0.0569\n",
      "Epoch: 1871 loss_train: 0.0628 loss_val: 0.0544\n",
      "Epoch: 1872 loss_train: 0.0633 loss_val: 0.0547\n",
      "Epoch: 1873 loss_train: 0.0620 loss_val: 0.0549\n",
      "Epoch: 1874 loss_train: 0.0627 loss_val: 0.0548\n",
      "Epoch: 1875 loss_train: 0.0630 loss_val: 0.0552\n",
      "Epoch: 1876 loss_train: 0.0628 loss_val: 0.0544\n",
      "Epoch: 1877 loss_train: 0.0638 loss_val: 0.0530\n",
      "Epoch: 1878 loss_train: 0.0644 loss_val: 0.0542\n",
      "Epoch: 1879 loss_train: 0.0631 loss_val: 0.0540\n",
      "Epoch: 1880 loss_train: 0.0636 loss_val: 0.0564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1881 loss_train: 0.0627 loss_val: 0.0548\n",
      "Epoch: 1882 loss_train: 0.0630 loss_val: 0.0535\n",
      "Epoch: 1883 loss_train: 0.0637 loss_val: 0.0564\n",
      "Epoch: 1884 loss_train: 0.0641 loss_val: 0.0535\n",
      "Epoch: 1885 loss_train: 0.0641 loss_val: 0.0535\n",
      "Epoch: 1886 loss_train: 0.0635 loss_val: 0.0561\n",
      "Epoch: 1887 loss_train: 0.0645 loss_val: 0.0557\n",
      "Epoch: 1888 loss_train: 0.0647 loss_val: 0.0553\n",
      "Epoch: 1889 loss_train: 0.0636 loss_val: 0.0556\n",
      "Epoch: 1890 loss_train: 0.0633 loss_val: 0.0532\n",
      "Epoch: 1891 loss_train: 0.0646 loss_val: 0.0569\n",
      "Epoch: 1892 loss_train: 0.0647 loss_val: 0.0557\n",
      "Epoch: 1893 loss_train: 0.0635 loss_val: 0.0548\n",
      "Epoch: 1894 loss_train: 0.0637 loss_val: 0.0539\n",
      "Epoch: 1895 loss_train: 0.0622 loss_val: 0.0560\n",
      "Epoch: 1896 loss_train: 0.0640 loss_val: 0.0560\n",
      "Epoch: 1897 loss_train: 0.0635 loss_val: 0.0550\n",
      "Epoch: 1898 loss_train: 0.0626 loss_val: 0.0553\n",
      "Epoch: 1899 loss_train: 0.0616 loss_val: 0.0558\n",
      "Epoch: 1900 loss_train: 0.0643 loss_val: 0.0537\n",
      "Epoch: 1901 loss_train: 0.0638 loss_val: 0.0535\n",
      "Epoch: 1902 loss_train: 0.0638 loss_val: 0.0558\n",
      "Epoch: 1903 loss_train: 0.0634 loss_val: 0.0541\n",
      "Epoch: 1904 loss_train: 0.0624 loss_val: 0.0545\n",
      "Epoch: 1905 loss_train: 0.0633 loss_val: 0.0547\n",
      "Epoch: 1906 loss_train: 0.0631 loss_val: 0.0553\n",
      "Epoch: 1907 loss_train: 0.0627 loss_val: 0.0559\n",
      "Epoch: 1908 loss_train: 0.0647 loss_val: 0.0541\n",
      "Epoch: 1909 loss_train: 0.0631 loss_val: 0.0531\n",
      "Epoch: 1910 loss_train: 0.0633 loss_val: 0.0560\n",
      "Epoch: 1911 loss_train: 0.0642 loss_val: 0.0551\n",
      "Epoch: 1912 loss_train: 0.0642 loss_val: 0.0541\n",
      "Epoch: 1913 loss_train: 0.0649 loss_val: 0.0537\n",
      "Epoch: 1914 loss_train: 0.0630 loss_val: 0.0546\n",
      "Epoch: 1915 loss_train: 0.0633 loss_val: 0.0547\n",
      "Epoch: 1916 loss_train: 0.0630 loss_val: 0.0565\n",
      "Epoch: 1917 loss_train: 0.0633 loss_val: 0.0536\n",
      "Epoch: 1918 loss_train: 0.0626 loss_val: 0.0539\n",
      "Epoch: 1919 loss_train: 0.0632 loss_val: 0.0551\n",
      "Epoch: 1920 loss_train: 0.0641 loss_val: 0.0547\n",
      "Epoch: 1921 loss_train: 0.0632 loss_val: 0.0557\n",
      "Epoch: 1922 loss_train: 0.0635 loss_val: 0.0542\n",
      "Epoch: 1923 loss_train: 0.0618 loss_val: 0.0544\n",
      "Epoch: 1924 loss_train: 0.0641 loss_val: 0.0541\n",
      "Epoch: 1925 loss_train: 0.0623 loss_val: 0.0535\n",
      "Epoch: 1926 loss_train: 0.0626 loss_val: 0.0542\n",
      "Epoch: 1927 loss_train: 0.0630 loss_val: 0.0547\n",
      "Epoch: 1928 loss_train: 0.0629 loss_val: 0.0546\n",
      "Epoch: 1929 loss_train: 0.0644 loss_val: 0.0546\n",
      "Epoch: 1930 loss_train: 0.0639 loss_val: 0.0551\n",
      "Epoch: 1931 loss_train: 0.0630 loss_val: 0.0537\n",
      "Epoch: 1932 loss_train: 0.0639 loss_val: 0.0546\n",
      "Epoch: 1933 loss_train: 0.0625 loss_val: 0.0553\n",
      "Epoch: 1934 loss_train: 0.0640 loss_val: 0.0537\n",
      "Epoch: 1935 loss_train: 0.0636 loss_val: 0.0526\n",
      "Epoch: 1936 loss_train: 0.0631 loss_val: 0.0541\n",
      "Epoch: 1937 loss_train: 0.0639 loss_val: 0.0539\n",
      "Epoch: 1938 loss_train: 0.0623 loss_val: 0.0542\n",
      "Epoch: 1939 loss_train: 0.0608 loss_val: 0.0541\n",
      "Epoch: 1940 loss_train: 0.0618 loss_val: 0.0535\n",
      "Epoch: 1941 loss_train: 0.0631 loss_val: 0.0541\n",
      "Epoch: 1942 loss_train: 0.0627 loss_val: 0.0561\n",
      "Epoch: 1943 loss_train: 0.0642 loss_val: 0.0545\n",
      "Epoch: 1944 loss_train: 0.0627 loss_val: 0.0530\n",
      "Epoch: 1945 loss_train: 0.0619 loss_val: 0.0562\n",
      "Epoch: 1946 loss_train: 0.0619 loss_val: 0.0549\n",
      "Epoch: 1947 loss_train: 0.0647 loss_val: 0.0539\n",
      "Epoch: 1948 loss_train: 0.0635 loss_val: 0.0548\n",
      "Epoch: 1949 loss_train: 0.0635 loss_val: 0.0543\n",
      "Epoch: 1950 loss_train: 0.0631 loss_val: 0.0533\n",
      "Epoch: 1951 loss_train: 0.0643 loss_val: 0.0544\n",
      "Epoch: 1952 loss_train: 0.0638 loss_val: 0.0529\n",
      "Epoch: 1953 loss_train: 0.0639 loss_val: 0.0544\n",
      "Epoch: 1954 loss_train: 0.0637 loss_val: 0.0529\n",
      "Epoch: 1955 loss_train: 0.0644 loss_val: 0.0524\n",
      "Epoch: 1956 loss_train: 0.0627 loss_val: 0.0534\n",
      "Epoch: 1957 loss_train: 0.0633 loss_val: 0.0546\n",
      "Epoch: 1958 loss_train: 0.0638 loss_val: 0.0564\n",
      "Epoch: 1959 loss_train: 0.0648 loss_val: 0.0549\n",
      "Epoch: 1960 loss_train: 0.0640 loss_val: 0.0551\n",
      "Epoch: 1961 loss_train: 0.0644 loss_val: 0.0540\n",
      "Epoch: 1962 loss_train: 0.0639 loss_val: 0.0538\n",
      "Epoch: 1963 loss_train: 0.0638 loss_val: 0.0523\n",
      "Epoch: 1964 loss_train: 0.0622 loss_val: 0.0531\n",
      "Epoch: 1965 loss_train: 0.0620 loss_val: 0.0542\n",
      "Epoch: 1966 loss_train: 0.0637 loss_val: 0.0556\n",
      "Epoch: 1967 loss_train: 0.0633 loss_val: 0.0517\n",
      "Epoch: 1968 loss_train: 0.0637 loss_val: 0.0544\n",
      "Epoch: 1969 loss_train: 0.0624 loss_val: 0.0540\n",
      "Epoch: 1970 loss_train: 0.0625 loss_val: 0.0558\n",
      "Epoch: 1971 loss_train: 0.0642 loss_val: 0.0545\n",
      "Epoch: 1972 loss_train: 0.0645 loss_val: 0.0529\n",
      "Epoch: 1973 loss_train: 0.0650 loss_val: 0.0548\n",
      "Epoch: 1974 loss_train: 0.0634 loss_val: 0.0530\n",
      "Epoch: 1975 loss_train: 0.0633 loss_val: 0.0555\n",
      "Epoch: 1976 loss_train: 0.0635 loss_val: 0.0544\n",
      "Epoch: 1977 loss_train: 0.0633 loss_val: 0.0540\n",
      "Epoch: 1978 loss_train: 0.0630 loss_val: 0.0558\n",
      "Epoch: 1979 loss_train: 0.0619 loss_val: 0.0547\n",
      "Epoch: 1980 loss_train: 0.0615 loss_val: 0.0550\n",
      "Epoch: 1981 loss_train: 0.0624 loss_val: 0.0552\n",
      "Epoch: 1982 loss_train: 0.0620 loss_val: 0.0549\n",
      "Epoch: 1983 loss_train: 0.0625 loss_val: 0.0553\n",
      "Epoch: 1984 loss_train: 0.0623 loss_val: 0.0544\n",
      "Epoch: 1985 loss_train: 0.0612 loss_val: 0.0544\n",
      "Epoch: 1986 loss_train: 0.0621 loss_val: 0.0541\n",
      "Epoch: 1987 loss_train: 0.0628 loss_val: 0.0528\n",
      "Epoch: 1988 loss_train: 0.0639 loss_val: 0.0546\n",
      "Epoch: 1989 loss_train: 0.0627 loss_val: 0.0534\n",
      "Epoch: 1990 loss_train: 0.0620 loss_val: 0.0544\n",
      "Epoch: 1991 loss_train: 0.0634 loss_val: 0.0552\n",
      "Epoch: 1992 loss_train: 0.0643 loss_val: 0.0544\n",
      "Epoch: 1993 loss_train: 0.0629 loss_val: 0.0548\n",
      "Epoch: 1994 loss_train: 0.0629 loss_val: 0.0539\n",
      "Epoch: 1995 loss_train: 0.0625 loss_val: 0.0538\n",
      "Epoch: 1996 loss_train: 0.0640 loss_val: 0.0538\n",
      "Epoch: 1997 loss_train: 0.0639 loss_val: 0.0534\n",
      "Epoch: 1998 loss_train: 0.0625 loss_val: 0.0544\n",
      "Epoch: 1999 loss_train: 0.0636 loss_val: 0.0526\n",
      "Epoch: 2000 loss_train: 0.0632 loss_val: 0.0542\n",
      "Epoch: 2001 loss_train: 0.0624 loss_val: 0.0545\n",
      "Epoch: 2002 loss_train: 0.0627 loss_val: 0.0541\n",
      "Epoch: 2003 loss_train: 0.0637 loss_val: 0.0528\n",
      "Epoch: 2004 loss_train: 0.0640 loss_val: 0.0535\n",
      "Epoch: 2005 loss_train: 0.0629 loss_val: 0.0526\n",
      "Epoch: 2006 loss_train: 0.0631 loss_val: 0.0546\n",
      "Epoch: 2007 loss_train: 0.0626 loss_val: 0.0537\n",
      "Epoch: 2008 loss_train: 0.0621 loss_val: 0.0554\n",
      "Epoch: 2009 loss_train: 0.0636 loss_val: 0.0542\n",
      "Epoch: 2010 loss_train: 0.0625 loss_val: 0.0538\n",
      "Epoch: 2011 loss_train: 0.0622 loss_val: 0.0543\n",
      "Epoch: 2012 loss_train: 0.0639 loss_val: 0.0554\n",
      "Epoch: 2013 loss_train: 0.0624 loss_val: 0.0548\n",
      "Epoch: 2014 loss_train: 0.0639 loss_val: 0.0546\n",
      "Epoch: 2015 loss_train: 0.0617 loss_val: 0.0550\n",
      "Epoch: 2016 loss_train: 0.0620 loss_val: 0.0560\n",
      "Epoch: 2017 loss_train: 0.0635 loss_val: 0.0551\n",
      "Epoch: 2018 loss_train: 0.0634 loss_val: 0.0562\n",
      "Epoch: 2019 loss_train: 0.0626 loss_val: 0.0561\n",
      "Epoch: 2020 loss_train: 0.0644 loss_val: 0.0561\n",
      "Epoch: 2021 loss_train: 0.0632 loss_val: 0.0558\n",
      "Epoch: 2022 loss_train: 0.0619 loss_val: 0.0550\n",
      "Epoch: 2023 loss_train: 0.0629 loss_val: 0.0556\n",
      "Epoch: 2024 loss_train: 0.0623 loss_val: 0.0556\n",
      "Epoch: 2025 loss_train: 0.0628 loss_val: 0.0559\n",
      "Epoch: 2026 loss_train: 0.0620 loss_val: 0.0565\n",
      "Epoch: 2027 loss_train: 0.0639 loss_val: 0.0579\n",
      "Epoch: 2028 loss_train: 0.0640 loss_val: 0.0561\n",
      "Epoch: 2029 loss_train: 0.0637 loss_val: 0.0563\n",
      "Epoch: 2030 loss_train: 0.0632 loss_val: 0.0557\n",
      "Epoch: 2031 loss_train: 0.0636 loss_val: 0.0568\n",
      "Epoch: 2032 loss_train: 0.0629 loss_val: 0.0551\n",
      "Epoch: 2033 loss_train: 0.0625 loss_val: 0.0558\n",
      "Epoch: 2034 loss_train: 0.0635 loss_val: 0.0555\n",
      "Epoch: 2035 loss_train: 0.0623 loss_val: 0.0557\n",
      "Epoch: 2036 loss_train: 0.0626 loss_val: 0.0545\n",
      "Epoch: 2037 loss_train: 0.0629 loss_val: 0.0553\n",
      "Epoch: 2038 loss_train: 0.0633 loss_val: 0.0549\n",
      "Epoch: 2039 loss_train: 0.0628 loss_val: 0.0557\n",
      "Epoch: 2040 loss_train: 0.0616 loss_val: 0.0560\n",
      "Epoch: 2041 loss_train: 0.0627 loss_val: 0.0567\n",
      "Epoch: 2042 loss_train: 0.0623 loss_val: 0.0591\n",
      "Epoch: 2043 loss_train: 0.0625 loss_val: 0.0561\n",
      "Epoch: 2044 loss_train: 0.0625 loss_val: 0.0554\n",
      "Epoch: 2045 loss_train: 0.0631 loss_val: 0.0561\n",
      "Epoch: 2046 loss_train: 0.0629 loss_val: 0.0550\n",
      "Epoch: 2047 loss_train: 0.0630 loss_val: 0.0551\n",
      "Epoch: 2048 loss_train: 0.0616 loss_val: 0.0563\n",
      "Epoch: 2049 loss_train: 0.0617 loss_val: 0.0565\n",
      "Epoch: 2050 loss_train: 0.0620 loss_val: 0.0567\n",
      "Epoch: 2051 loss_train: 0.0632 loss_val: 0.0571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2052 loss_train: 0.0631 loss_val: 0.0567\n",
      "Epoch: 2053 loss_train: 0.0635 loss_val: 0.0572\n",
      "Epoch: 2054 loss_train: 0.0616 loss_val: 0.0560\n",
      "Epoch: 2055 loss_train: 0.0632 loss_val: 0.0563\n",
      "Epoch: 2056 loss_train: 0.0630 loss_val: 0.0565\n",
      "Epoch: 2057 loss_train: 0.0646 loss_val: 0.0574\n",
      "Epoch: 2058 loss_train: 0.0633 loss_val: 0.0562\n",
      "Epoch: 2059 loss_train: 0.0634 loss_val: 0.0577\n",
      "Epoch: 2060 loss_train: 0.0634 loss_val: 0.0574\n",
      "Epoch: 2061 loss_train: 0.0623 loss_val: 0.0547\n",
      "Epoch: 2062 loss_train: 0.0634 loss_val: 0.0570\n",
      "Epoch: 2063 loss_train: 0.0639 loss_val: 0.0560\n",
      "Epoch: 2064 loss_train: 0.0633 loss_val: 0.0551\n",
      "Epoch: 2065 loss_train: 0.0631 loss_val: 0.0559\n",
      "Epoch: 2066 loss_train: 0.0637 loss_val: 0.0563\n",
      "Epoch: 2067 loss_train: 0.0637 loss_val: 0.0581\n",
      "Epoch: 2068 loss_train: 0.0632 loss_val: 0.0576\n",
      "Epoch: 2069 loss_train: 0.0627 loss_val: 0.0547\n",
      "Epoch: 2070 loss_train: 0.0618 loss_val: 0.0581\n",
      "Epoch: 2071 loss_train: 0.0610 loss_val: 0.0557\n",
      "Epoch: 2072 loss_train: 0.0615 loss_val: 0.0569\n",
      "Epoch: 2073 loss_train: 0.0629 loss_val: 0.0563\n",
      "Epoch: 2074 loss_train: 0.0623 loss_val: 0.0551\n",
      "Epoch: 2075 loss_train: 0.0627 loss_val: 0.0558\n",
      "Epoch: 2076 loss_train: 0.0617 loss_val: 0.0556\n",
      "Epoch: 2077 loss_train: 0.0624 loss_val: 0.0544\n",
      "Epoch: 2078 loss_train: 0.0626 loss_val: 0.0554\n",
      "Epoch: 2079 loss_train: 0.0627 loss_val: 0.0554\n",
      "Epoch: 2080 loss_train: 0.0633 loss_val: 0.0570\n",
      "Epoch: 2081 loss_train: 0.0645 loss_val: 0.0567\n",
      "Epoch: 2082 loss_train: 0.0623 loss_val: 0.0562\n",
      "Epoch: 2083 loss_train: 0.0624 loss_val: 0.0554\n",
      "Epoch: 2084 loss_train: 0.0623 loss_val: 0.0564\n",
      "Epoch: 2085 loss_train: 0.0636 loss_val: 0.0572\n",
      "Epoch: 2086 loss_train: 0.0628 loss_val: 0.0561\n",
      "Epoch: 2087 loss_train: 0.0630 loss_val: 0.0563\n",
      "Epoch: 2088 loss_train: 0.0631 loss_val: 0.0565\n",
      "Epoch: 2089 loss_train: 0.0645 loss_val: 0.0577\n",
      "Epoch: 2090 loss_train: 0.0651 loss_val: 0.0565\n",
      "Epoch: 2091 loss_train: 0.0624 loss_val: 0.0569\n",
      "Epoch: 2092 loss_train: 0.0624 loss_val: 0.0566\n",
      "Epoch: 2093 loss_train: 0.0632 loss_val: 0.0574\n",
      "Epoch: 2094 loss_train: 0.0644 loss_val: 0.0573\n",
      "Epoch: 2095 loss_train: 0.0640 loss_val: 0.0553\n",
      "Epoch: 2096 loss_train: 0.0625 loss_val: 0.0567\n",
      "Epoch: 2097 loss_train: 0.0622 loss_val: 0.0568\n",
      "Epoch: 2098 loss_train: 0.0618 loss_val: 0.0568\n",
      "Epoch: 2099 loss_train: 0.0627 loss_val: 0.0560\n",
      "Epoch: 2100 loss_train: 0.0620 loss_val: 0.0556\n",
      "Epoch: 2101 loss_train: 0.0627 loss_val: 0.0554\n",
      "Epoch: 2102 loss_train: 0.0621 loss_val: 0.0547\n",
      "Epoch: 2103 loss_train: 0.0621 loss_val: 0.0564\n",
      "Epoch: 2104 loss_train: 0.0622 loss_val: 0.0580\n",
      "Epoch: 2105 loss_train: 0.0626 loss_val: 0.0577\n",
      "Epoch: 2106 loss_train: 0.0622 loss_val: 0.0547\n",
      "Epoch: 2107 loss_train: 0.0623 loss_val: 0.0556\n",
      "Epoch: 2108 loss_train: 0.0632 loss_val: 0.0555\n",
      "Epoch: 2109 loss_train: 0.0615 loss_val: 0.0545\n",
      "Epoch: 2110 loss_train: 0.0625 loss_val: 0.0551\n",
      "Epoch: 2111 loss_train: 0.0618 loss_val: 0.0548\n",
      "Epoch: 2112 loss_train: 0.0629 loss_val: 0.0555\n",
      "Epoch: 2113 loss_train: 0.0624 loss_val: 0.0564\n",
      "Epoch: 2114 loss_train: 0.0627 loss_val: 0.0566\n",
      "Epoch: 2115 loss_train: 0.0637 loss_val: 0.0576\n",
      "Epoch: 2116 loss_train: 0.0628 loss_val: 0.0555\n",
      "Epoch: 2117 loss_train: 0.0627 loss_val: 0.0550\n",
      "Epoch: 2118 loss_train: 0.0624 loss_val: 0.0558\n",
      "Epoch: 2119 loss_train: 0.0625 loss_val: 0.0552\n",
      "Epoch: 2120 loss_train: 0.0622 loss_val: 0.0549\n",
      "Epoch: 2121 loss_train: 0.0647 loss_val: 0.0584\n",
      "Epoch: 2122 loss_train: 0.0637 loss_val: 0.0570\n",
      "Epoch: 2123 loss_train: 0.0632 loss_val: 0.0551\n",
      "Epoch: 2124 loss_train: 0.0616 loss_val: 0.0556\n",
      "Epoch: 2125 loss_train: 0.0619 loss_val: 0.0542\n",
      "Epoch: 2126 loss_train: 0.0630 loss_val: 0.0563\n",
      "Epoch: 2127 loss_train: 0.0613 loss_val: 0.0552\n",
      "Epoch: 2128 loss_train: 0.0636 loss_val: 0.0544\n",
      "Epoch: 2129 loss_train: 0.0611 loss_val: 0.0549\n",
      "Epoch: 2130 loss_train: 0.0620 loss_val: 0.0545\n",
      "Epoch: 2131 loss_train: 0.0632 loss_val: 0.0544\n",
      "Epoch: 2132 loss_train: 0.0610 loss_val: 0.0561\n",
      "Epoch: 2133 loss_train: 0.0637 loss_val: 0.0551\n",
      "Epoch: 2134 loss_train: 0.0634 loss_val: 0.0557\n",
      "Epoch: 2135 loss_train: 0.0628 loss_val: 0.0591\n",
      "Epoch: 2136 loss_train: 0.0615 loss_val: 0.0577\n",
      "Epoch: 2137 loss_train: 0.0622 loss_val: 0.0533\n",
      "Epoch: 2138 loss_train: 0.0615 loss_val: 0.0534\n",
      "Epoch: 2139 loss_train: 0.0618 loss_val: 0.0551\n",
      "Epoch: 2140 loss_train: 0.0617 loss_val: 0.0551\n",
      "Epoch: 2141 loss_train: 0.0614 loss_val: 0.0554\n",
      "Epoch: 2142 loss_train: 0.0612 loss_val: 0.0559\n",
      "Epoch: 2143 loss_train: 0.0611 loss_val: 0.0557\n",
      "Epoch: 2144 loss_train: 0.0621 loss_val: 0.0565\n",
      "Epoch: 2145 loss_train: 0.0614 loss_val: 0.0553\n",
      "Epoch: 2146 loss_train: 0.0617 loss_val: 0.0534\n",
      "Epoch: 2147 loss_train: 0.0613 loss_val: 0.0539\n",
      "Epoch: 2148 loss_train: 0.0622 loss_val: 0.0546\n",
      "Epoch: 2149 loss_train: 0.0617 loss_val: 0.0536\n",
      "Epoch: 2150 loss_train: 0.0619 loss_val: 0.0550\n",
      "Epoch: 2151 loss_train: 0.0638 loss_val: 0.0546\n",
      "Epoch: 2152 loss_train: 0.0618 loss_val: 0.0558\n",
      "Epoch: 2153 loss_train: 0.0620 loss_val: 0.0529\n",
      "Epoch: 2154 loss_train: 0.0612 loss_val: 0.0538\n",
      "Epoch: 2155 loss_train: 0.0622 loss_val: 0.0555\n",
      "Epoch: 2156 loss_train: 0.0625 loss_val: 0.0527\n",
      "Epoch: 2157 loss_train: 0.0617 loss_val: 0.0541\n",
      "Epoch: 2158 loss_train: 0.0633 loss_val: 0.0544\n",
      "Epoch: 2159 loss_train: 0.0627 loss_val: 0.0539\n",
      "Epoch: 2160 loss_train: 0.0622 loss_val: 0.0554\n",
      "Epoch: 2161 loss_train: 0.0626 loss_val: 0.0548\n",
      "Epoch: 2162 loss_train: 0.0618 loss_val: 0.0545\n",
      "Epoch: 2163 loss_train: 0.0620 loss_val: 0.0543\n",
      "Epoch: 2164 loss_train: 0.0620 loss_val: 0.0523\n",
      "Epoch: 2165 loss_train: 0.0612 loss_val: 0.0547\n",
      "Epoch: 2166 loss_train: 0.0619 loss_val: 0.0556\n",
      "Epoch: 2167 loss_train: 0.0621 loss_val: 0.0548\n",
      "Epoch: 2168 loss_train: 0.0632 loss_val: 0.0541\n",
      "Epoch: 2169 loss_train: 0.0620 loss_val: 0.0532\n",
      "Epoch: 2170 loss_train: 0.0621 loss_val: 0.0536\n",
      "Epoch: 2171 loss_train: 0.0622 loss_val: 0.0546\n",
      "Epoch: 2172 loss_train: 0.0609 loss_val: 0.0547\n",
      "Epoch: 2173 loss_train: 0.0619 loss_val: 0.0559\n",
      "Epoch: 2174 loss_train: 0.0639 loss_val: 0.0559\n",
      "Epoch: 2175 loss_train: 0.0639 loss_val: 0.0533\n",
      "Epoch: 2176 loss_train: 0.0618 loss_val: 0.0525\n",
      "Epoch: 2177 loss_train: 0.0610 loss_val: 0.0541\n",
      "Epoch: 2178 loss_train: 0.0624 loss_val: 0.0545\n",
      "Epoch: 2179 loss_train: 0.0626 loss_val: 0.0537\n",
      "Epoch: 2180 loss_train: 0.0636 loss_val: 0.0564\n",
      "Epoch: 2181 loss_train: 0.0631 loss_val: 0.0552\n",
      "Epoch: 2182 loss_train: 0.0628 loss_val: 0.0554\n",
      "Epoch: 2183 loss_train: 0.0618 loss_val: 0.0570\n",
      "Epoch: 2184 loss_train: 0.0630 loss_val: 0.0548\n",
      "Epoch: 2185 loss_train: 0.0640 loss_val: 0.0561\n",
      "Epoch: 2186 loss_train: 0.0629 loss_val: 0.0546\n",
      "Epoch: 2187 loss_train: 0.0610 loss_val: 0.0540\n",
      "Epoch: 2188 loss_train: 0.0621 loss_val: 0.0553\n",
      "Epoch: 2189 loss_train: 0.0629 loss_val: 0.0550\n",
      "Epoch: 2190 loss_train: 0.0629 loss_val: 0.0556\n",
      "Epoch: 2191 loss_train: 0.0616 loss_val: 0.0563\n",
      "Epoch: 2192 loss_train: 0.0630 loss_val: 0.0552\n",
      "Epoch: 2193 loss_train: 0.0612 loss_val: 0.0553\n",
      "Epoch: 2194 loss_train: 0.0607 loss_val: 0.0558\n",
      "Epoch: 2195 loss_train: 0.0625 loss_val: 0.0558\n",
      "Epoch: 2196 loss_train: 0.0612 loss_val: 0.0557\n",
      "Epoch: 2197 loss_train: 0.0620 loss_val: 0.0550\n",
      "Epoch: 2198 loss_train: 0.0611 loss_val: 0.0569\n",
      "Epoch: 2199 loss_train: 0.0620 loss_val: 0.0556\n",
      "Epoch: 2200 loss_train: 0.0622 loss_val: 0.0561\n",
      "Epoch: 2201 loss_train: 0.0620 loss_val: 0.0553\n",
      "Epoch: 2202 loss_train: 0.0624 loss_val: 0.0570\n",
      "Epoch: 2203 loss_train: 0.0633 loss_val: 0.0583\n",
      "Epoch: 2204 loss_train: 0.0617 loss_val: 0.0580\n",
      "Epoch: 2205 loss_train: 0.0616 loss_val: 0.0569\n",
      "Epoch: 2206 loss_train: 0.0609 loss_val: 0.0586\n",
      "Epoch: 2207 loss_train: 0.0623 loss_val: 0.0560\n",
      "Epoch: 2208 loss_train: 0.0636 loss_val: 0.0573\n",
      "Epoch: 2209 loss_train: 0.0629 loss_val: 0.0569\n",
      "Epoch: 2210 loss_train: 0.0629 loss_val: 0.0569\n",
      "Epoch: 2211 loss_train: 0.0624 loss_val: 0.0573\n",
      "Epoch: 2212 loss_train: 0.0623 loss_val: 0.0565\n",
      "Epoch: 2213 loss_train: 0.0621 loss_val: 0.0579\n",
      "Epoch: 2214 loss_train: 0.0620 loss_val: 0.0584\n",
      "Epoch: 2215 loss_train: 0.0621 loss_val: 0.0562\n",
      "Epoch: 2216 loss_train: 0.0619 loss_val: 0.0575\n",
      "Epoch: 2217 loss_train: 0.0621 loss_val: 0.0582\n",
      "Epoch: 2218 loss_train: 0.0628 loss_val: 0.0563\n",
      "Epoch: 2219 loss_train: 0.0619 loss_val: 0.0592\n",
      "Epoch: 2220 loss_train: 0.0620 loss_val: 0.0569\n",
      "Epoch: 2221 loss_train: 0.0609 loss_val: 0.0552\n",
      "Epoch: 2222 loss_train: 0.0613 loss_val: 0.0559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2223 loss_train: 0.0616 loss_val: 0.0563\n",
      "Epoch: 2224 loss_train: 0.0619 loss_val: 0.0572\n",
      "Epoch: 2225 loss_train: 0.0622 loss_val: 0.0575\n",
      "Epoch: 2226 loss_train: 0.0634 loss_val: 0.0551\n",
      "Epoch: 2227 loss_train: 0.0614 loss_val: 0.0535\n",
      "Epoch: 2228 loss_train: 0.0617 loss_val: 0.0563\n",
      "Epoch: 2229 loss_train: 0.0612 loss_val: 0.0565\n",
      "Epoch: 2230 loss_train: 0.0610 loss_val: 0.0547\n",
      "Epoch: 2231 loss_train: 0.0614 loss_val: 0.0550\n",
      "Epoch: 2232 loss_train: 0.0608 loss_val: 0.0551\n",
      "Epoch: 2233 loss_train: 0.0606 loss_val: 0.0580\n",
      "Epoch: 2234 loss_train: 0.0630 loss_val: 0.0544\n",
      "Epoch: 2235 loss_train: 0.0615 loss_val: 0.0560\n",
      "Epoch: 2236 loss_train: 0.0618 loss_val: 0.0561\n",
      "Epoch: 2237 loss_train: 0.0625 loss_val: 0.0563\n",
      "Epoch: 2238 loss_train: 0.0611 loss_val: 0.0553\n",
      "Epoch: 2239 loss_train: 0.0614 loss_val: 0.0559\n",
      "Epoch: 2240 loss_train: 0.0611 loss_val: 0.0551\n",
      "Epoch: 2241 loss_train: 0.0611 loss_val: 0.0533\n",
      "Epoch: 2242 loss_train: 0.0602 loss_val: 0.0546\n",
      "Epoch: 2243 loss_train: 0.0612 loss_val: 0.0575\n",
      "Epoch: 2244 loss_train: 0.0632 loss_val: 0.0550\n",
      "Epoch: 2245 loss_train: 0.0624 loss_val: 0.0547\n",
      "Epoch: 2246 loss_train: 0.0632 loss_val: 0.0569\n",
      "Epoch: 2247 loss_train: 0.0639 loss_val: 0.0562\n",
      "Epoch: 2248 loss_train: 0.0635 loss_val: 0.0541\n",
      "Epoch: 2249 loss_train: 0.0628 loss_val: 0.0551\n",
      "Epoch: 2250 loss_train: 0.0623 loss_val: 0.0559\n",
      "Epoch: 2251 loss_train: 0.0612 loss_val: 0.0552\n",
      "Epoch: 2252 loss_train: 0.0615 loss_val: 0.0559\n",
      "Epoch: 2253 loss_train: 0.0616 loss_val: 0.0558\n",
      "Epoch: 2254 loss_train: 0.0616 loss_val: 0.0549\n",
      "Epoch: 2255 loss_train: 0.0615 loss_val: 0.0564\n",
      "Epoch: 2256 loss_train: 0.0613 loss_val: 0.0570\n",
      "Epoch: 2257 loss_train: 0.0620 loss_val: 0.0559\n",
      "Epoch: 2258 loss_train: 0.0629 loss_val: 0.0556\n",
      "Epoch: 2259 loss_train: 0.0622 loss_val: 0.0576\n",
      "Epoch: 2260 loss_train: 0.0618 loss_val: 0.0567\n",
      "Epoch: 2261 loss_train: 0.0616 loss_val: 0.0565\n",
      "Epoch: 2262 loss_train: 0.0627 loss_val: 0.0576\n",
      "Epoch: 2263 loss_train: 0.0613 loss_val: 0.0570\n",
      "Epoch: 2264 loss_train: 0.0620 loss_val: 0.0566\n",
      "Epoch: 2265 loss_train: 0.0640 loss_val: 0.0556\n",
      "Epoch: 2266 loss_train: 0.0617 loss_val: 0.0544\n",
      "Epoch: 2267 loss_train: 0.0630 loss_val: 0.0558\n",
      "Epoch: 2268 loss_train: 0.0630 loss_val: 0.0559\n",
      "Epoch: 2269 loss_train: 0.0619 loss_val: 0.0557\n",
      "Epoch: 2270 loss_train: 0.0622 loss_val: 0.0557\n",
      "Epoch: 2271 loss_train: 0.0625 loss_val: 0.0557\n",
      "Epoch: 2272 loss_train: 0.0614 loss_val: 0.0573\n",
      "Epoch: 2273 loss_train: 0.0616 loss_val: 0.0556\n",
      "Epoch: 2274 loss_train: 0.0624 loss_val: 0.0537\n",
      "Epoch: 2275 loss_train: 0.0627 loss_val: 0.0567\n",
      "Epoch: 2276 loss_train: 0.0628 loss_val: 0.0557\n",
      "Epoch: 2277 loss_train: 0.0633 loss_val: 0.0551\n",
      "Epoch: 2278 loss_train: 0.0625 loss_val: 0.0556\n",
      "Epoch: 2279 loss_train: 0.0622 loss_val: 0.0565\n",
      "Epoch: 2280 loss_train: 0.0636 loss_val: 0.0565\n",
      "Epoch: 2281 loss_train: 0.0614 loss_val: 0.0538\n",
      "Epoch: 2282 loss_train: 0.0615 loss_val: 0.0568\n",
      "Epoch: 2283 loss_train: 0.0619 loss_val: 0.0544\n",
      "Epoch: 2284 loss_train: 0.0608 loss_val: 0.0563\n",
      "Epoch: 2285 loss_train: 0.0617 loss_val: 0.0557\n",
      "Epoch: 2286 loss_train: 0.0627 loss_val: 0.0561\n",
      "Epoch: 2287 loss_train: 0.0618 loss_val: 0.0558\n",
      "Epoch: 2288 loss_train: 0.0623 loss_val: 0.0548\n",
      "Epoch: 2289 loss_train: 0.0605 loss_val: 0.0547\n",
      "Epoch: 2290 loss_train: 0.0617 loss_val: 0.0545\n",
      "Epoch: 2291 loss_train: 0.0601 loss_val: 0.0559\n",
      "Epoch: 2292 loss_train: 0.0610 loss_val: 0.0532\n",
      "Epoch: 2293 loss_train: 0.0619 loss_val: 0.0552\n",
      "Epoch: 2294 loss_train: 0.0624 loss_val: 0.0550\n",
      "Epoch: 2295 loss_train: 0.0649 loss_val: 0.0550\n",
      "Epoch: 2296 loss_train: 0.0626 loss_val: 0.0558\n",
      "Epoch: 2297 loss_train: 0.0631 loss_val: 0.0569\n",
      "Epoch: 2298 loss_train: 0.0632 loss_val: 0.0558\n",
      "Epoch: 2299 loss_train: 0.0621 loss_val: 0.0554\n",
      "Epoch: 2300 loss_train: 0.0609 loss_val: 0.0555\n",
      "Epoch: 2301 loss_train: 0.0625 loss_val: 0.0563\n",
      "Epoch: 2302 loss_train: 0.0627 loss_val: 0.0558\n",
      "Epoch: 2303 loss_train: 0.0619 loss_val: 0.0563\n",
      "Epoch: 2304 loss_train: 0.0624 loss_val: 0.0561\n",
      "Epoch: 2305 loss_train: 0.0620 loss_val: 0.0546\n",
      "Epoch: 2306 loss_train: 0.0609 loss_val: 0.0541\n",
      "Epoch: 2307 loss_train: 0.0616 loss_val: 0.0538\n",
      "Epoch: 2308 loss_train: 0.0621 loss_val: 0.0545\n",
      "Epoch: 2309 loss_train: 0.0632 loss_val: 0.0560\n",
      "Epoch: 2310 loss_train: 0.0615 loss_val: 0.0524\n",
      "Epoch: 2311 loss_train: 0.0603 loss_val: 0.0544\n",
      "Epoch: 2312 loss_train: 0.0610 loss_val: 0.0543\n",
      "Epoch: 2313 loss_train: 0.0610 loss_val: 0.0552\n",
      "Epoch: 2314 loss_train: 0.0619 loss_val: 0.0533\n",
      "Epoch: 2315 loss_train: 0.0603 loss_val: 0.0534\n",
      "Epoch: 2316 loss_train: 0.0603 loss_val: 0.0528\n",
      "Epoch: 2317 loss_train: 0.0610 loss_val: 0.0533\n",
      "Epoch: 2318 loss_train: 0.0608 loss_val: 0.0548\n",
      "Epoch: 2319 loss_train: 0.0615 loss_val: 0.0539\n",
      "Epoch: 2320 loss_train: 0.0606 loss_val: 0.0535\n",
      "Epoch: 2321 loss_train: 0.0627 loss_val: 0.0552\n",
      "Epoch: 2322 loss_train: 0.0617 loss_val: 0.0557\n",
      "Epoch: 2323 loss_train: 0.0617 loss_val: 0.0552\n",
      "Epoch: 2324 loss_train: 0.0630 loss_val: 0.0547\n",
      "Epoch: 2325 loss_train: 0.0615 loss_val: 0.0542\n",
      "Epoch: 2326 loss_train: 0.0613 loss_val: 0.0555\n",
      "Epoch: 2327 loss_train: 0.0613 loss_val: 0.0559\n",
      "Epoch: 2328 loss_train: 0.0620 loss_val: 0.0531\n",
      "Epoch: 2329 loss_train: 0.0625 loss_val: 0.0566\n",
      "Epoch: 2330 loss_train: 0.0615 loss_val: 0.0547\n",
      "Epoch: 2331 loss_train: 0.0613 loss_val: 0.0563\n",
      "Epoch: 2332 loss_train: 0.0614 loss_val: 0.0575\n",
      "Epoch: 2333 loss_train: 0.0606 loss_val: 0.0553\n",
      "Epoch: 2334 loss_train: 0.0608 loss_val: 0.0565\n",
      "Epoch: 2335 loss_train: 0.0613 loss_val: 0.0555\n",
      "Epoch: 2336 loss_train: 0.0599 loss_val: 0.0551\n",
      "Epoch: 2337 loss_train: 0.0629 loss_val: 0.0551\n",
      "Epoch: 2338 loss_train: 0.0612 loss_val: 0.0548\n",
      "Epoch: 2339 loss_train: 0.0615 loss_val: 0.0555\n",
      "Epoch: 2340 loss_train: 0.0618 loss_val: 0.0564\n",
      "Epoch: 2341 loss_train: 0.0614 loss_val: 0.0567\n",
      "Epoch: 2342 loss_train: 0.0609 loss_val: 0.0554\n",
      "Epoch: 2343 loss_train: 0.0618 loss_val: 0.0575\n",
      "Epoch: 2344 loss_train: 0.0635 loss_val: 0.0568\n",
      "Epoch: 2345 loss_train: 0.0624 loss_val: 0.0550\n",
      "Epoch: 2346 loss_train: 0.0612 loss_val: 0.0554\n",
      "Epoch: 2347 loss_train: 0.0616 loss_val: 0.0548\n",
      "Epoch: 2348 loss_train: 0.0612 loss_val: 0.0547\n",
      "Epoch: 2349 loss_train: 0.0628 loss_val: 0.0551\n",
      "Epoch: 2350 loss_train: 0.0629 loss_val: 0.0560\n",
      "Epoch: 2351 loss_train: 0.0619 loss_val: 0.0575\n",
      "Epoch: 2352 loss_train: 0.0633 loss_val: 0.0557\n",
      "Epoch: 2353 loss_train: 0.0614 loss_val: 0.0578\n",
      "Epoch: 2354 loss_train: 0.0617 loss_val: 0.0548\n",
      "Epoch: 2355 loss_train: 0.0618 loss_val: 0.0547\n",
      "Epoch: 2356 loss_train: 0.0608 loss_val: 0.0547\n",
      "Epoch: 2357 loss_train: 0.0619 loss_val: 0.0563\n",
      "Epoch: 2358 loss_train: 0.0611 loss_val: 0.0555\n",
      "Epoch: 2359 loss_train: 0.0601 loss_val: 0.0554\n",
      "Epoch: 2360 loss_train: 0.0606 loss_val: 0.0557\n",
      "Epoch: 2361 loss_train: 0.0606 loss_val: 0.0562\n",
      "Epoch: 2362 loss_train: 0.0623 loss_val: 0.0542\n",
      "Epoch: 2363 loss_train: 0.0612 loss_val: 0.0533\n",
      "Epoch: 2364 loss_train: 0.0631 loss_val: 0.0536\n",
      "Epoch: 2365 loss_train: 0.0616 loss_val: 0.0554\n",
      "Epoch: 2366 loss_train: 0.0616 loss_val: 0.0542\n",
      "Epoch: 2367 loss_train: 0.0619 loss_val: 0.0583\n",
      "Epoch: 2368 loss_train: 0.0634 loss_val: 0.0563\n",
      "Epoch: 2369 loss_train: 0.0609 loss_val: 0.0565\n",
      "Epoch: 2370 loss_train: 0.0620 loss_val: 0.0588\n",
      "Epoch: 2371 loss_train: 0.0641 loss_val: 0.0570\n",
      "Epoch: 2372 loss_train: 0.0635 loss_val: 0.0554\n",
      "Epoch: 2373 loss_train: 0.0611 loss_val: 0.0546\n",
      "Epoch: 2374 loss_train: 0.0619 loss_val: 0.0552\n",
      "Epoch: 2375 loss_train: 0.0614 loss_val: 0.0542\n",
      "Epoch: 2376 loss_train: 0.0615 loss_val: 0.0553\n",
      "Epoch: 2377 loss_train: 0.0617 loss_val: 0.0541\n",
      "Epoch: 2378 loss_train: 0.0609 loss_val: 0.0544\n",
      "Epoch: 2379 loss_train: 0.0611 loss_val: 0.0547\n",
      "Epoch: 2380 loss_train: 0.0616 loss_val: 0.0546\n",
      "Epoch: 2381 loss_train: 0.0606 loss_val: 0.0543\n",
      "Epoch: 2382 loss_train: 0.0613 loss_val: 0.0562\n",
      "Epoch: 2383 loss_train: 0.0624 loss_val: 0.0541\n",
      "Epoch: 2384 loss_train: 0.0603 loss_val: 0.0560\n",
      "Epoch: 2385 loss_train: 0.0622 loss_val: 0.0555\n",
      "Epoch: 2386 loss_train: 0.0624 loss_val: 0.0551\n",
      "Epoch: 2387 loss_train: 0.0603 loss_val: 0.0551\n",
      "Epoch: 2388 loss_train: 0.0615 loss_val: 0.0560\n",
      "Epoch: 2389 loss_train: 0.0608 loss_val: 0.0546\n",
      "Epoch: 2390 loss_train: 0.0614 loss_val: 0.0556\n",
      "Epoch: 2391 loss_train: 0.0617 loss_val: 0.0560\n",
      "Epoch: 2392 loss_train: 0.0611 loss_val: 0.0553\n",
      "Epoch: 2393 loss_train: 0.0612 loss_val: 0.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2394 loss_train: 0.0608 loss_val: 0.0550\n",
      "Epoch: 2395 loss_train: 0.0617 loss_val: 0.0559\n",
      "Epoch: 2396 loss_train: 0.0624 loss_val: 0.0561\n",
      "Epoch: 2397 loss_train: 0.0618 loss_val: 0.0544\n",
      "Epoch: 2398 loss_train: 0.0615 loss_val: 0.0568\n",
      "Epoch: 2399 loss_train: 0.0629 loss_val: 0.0543\n",
      "Epoch: 2400 loss_train: 0.0616 loss_val: 0.0559\n",
      "Epoch: 2401 loss_train: 0.0622 loss_val: 0.0549\n",
      "Epoch: 2402 loss_train: 0.0624 loss_val: 0.0557\n",
      "Epoch: 2403 loss_train: 0.0609 loss_val: 0.0552\n",
      "Epoch: 2404 loss_train: 0.0617 loss_val: 0.0563\n",
      "Epoch: 2405 loss_train: 0.0630 loss_val: 0.0569\n",
      "Epoch: 2406 loss_train: 0.0627 loss_val: 0.0548\n",
      "Epoch: 2407 loss_train: 0.0618 loss_val: 0.0569\n",
      "Epoch: 2408 loss_train: 0.0609 loss_val: 0.0556\n",
      "Epoch: 2409 loss_train: 0.0607 loss_val: 0.0542\n",
      "Epoch: 2410 loss_train: 0.0624 loss_val: 0.0539\n",
      "Epoch: 2411 loss_train: 0.0623 loss_val: 0.0539\n",
      "Epoch: 2412 loss_train: 0.0620 loss_val: 0.0539\n",
      "Epoch: 2413 loss_train: 0.0616 loss_val: 0.0577\n",
      "Epoch: 2414 loss_train: 0.0613 loss_val: 0.0553\n",
      "Epoch: 2415 loss_train: 0.0615 loss_val: 0.0567\n",
      "Epoch: 2416 loss_train: 0.0613 loss_val: 0.0561\n",
      "Epoch: 2417 loss_train: 0.0607 loss_val: 0.0556\n",
      "Epoch: 2418 loss_train: 0.0610 loss_val: 0.0550\n",
      "Epoch: 2419 loss_train: 0.0609 loss_val: 0.0557\n",
      "Epoch: 2420 loss_train: 0.0609 loss_val: 0.0565\n",
      "Epoch: 2421 loss_train: 0.0606 loss_val: 0.0552\n",
      "Epoch: 2422 loss_train: 0.0609 loss_val: 0.0565\n",
      "Epoch: 2423 loss_train: 0.0609 loss_val: 0.0572\n",
      "Epoch: 2424 loss_train: 0.0602 loss_val: 0.0559\n",
      "Epoch: 2425 loss_train: 0.0622 loss_val: 0.0569\n",
      "Epoch: 2426 loss_train: 0.0619 loss_val: 0.0573\n",
      "Epoch: 2427 loss_train: 0.0624 loss_val: 0.0559\n",
      "Epoch: 2428 loss_train: 0.0610 loss_val: 0.0583\n",
      "Epoch: 2429 loss_train: 0.0607 loss_val: 0.0544\n",
      "Epoch: 2430 loss_train: 0.0602 loss_val: 0.0545\n",
      "Epoch: 2431 loss_train: 0.0601 loss_val: 0.0555\n",
      "Epoch: 2432 loss_train: 0.0600 loss_val: 0.0558\n",
      "Epoch: 2433 loss_train: 0.0605 loss_val: 0.0543\n",
      "Epoch: 2434 loss_train: 0.0603 loss_val: 0.0549\n",
      "Epoch: 2435 loss_train: 0.0613 loss_val: 0.0545\n",
      "Epoch: 2436 loss_train: 0.0615 loss_val: 0.0543\n",
      "Epoch: 2437 loss_train: 0.0609 loss_val: 0.0545\n",
      "Epoch: 2438 loss_train: 0.0612 loss_val: 0.0552\n",
      "Epoch: 2439 loss_train: 0.0612 loss_val: 0.0536\n",
      "Epoch: 2440 loss_train: 0.0611 loss_val: 0.0546\n",
      "Epoch: 2441 loss_train: 0.0616 loss_val: 0.0561\n",
      "Epoch: 2442 loss_train: 0.0611 loss_val: 0.0550\n",
      "Epoch: 2443 loss_train: 0.0625 loss_val: 0.0553\n",
      "Epoch: 2444 loss_train: 0.0606 loss_val: 0.0531\n",
      "Epoch: 2445 loss_train: 0.0618 loss_val: 0.0564\n",
      "Epoch: 2446 loss_train: 0.0610 loss_val: 0.0565\n",
      "Epoch: 2447 loss_train: 0.0618 loss_val: 0.0567\n",
      "Epoch: 2448 loss_train: 0.0616 loss_val: 0.0551\n",
      "Epoch: 2449 loss_train: 0.0598 loss_val: 0.0546\n",
      "Epoch: 2450 loss_train: 0.0610 loss_val: 0.0560\n",
      "Epoch: 2451 loss_train: 0.0604 loss_val: 0.0563\n",
      "Epoch: 2452 loss_train: 0.0624 loss_val: 0.0558\n",
      "Epoch: 2453 loss_train: 0.0625 loss_val: 0.0588\n",
      "Epoch: 2454 loss_train: 0.0609 loss_val: 0.0539\n",
      "Epoch: 2455 loss_train: 0.0609 loss_val: 0.0562\n",
      "Epoch: 2456 loss_train: 0.0610 loss_val: 0.0540\n",
      "Epoch: 2457 loss_train: 0.0599 loss_val: 0.0548\n",
      "Epoch: 2458 loss_train: 0.0615 loss_val: 0.0558\n",
      "Epoch: 2459 loss_train: 0.0607 loss_val: 0.0563\n",
      "Epoch: 2460 loss_train: 0.0616 loss_val: 0.0558\n",
      "Epoch: 2461 loss_train: 0.0617 loss_val: 0.0554\n",
      "Epoch: 2462 loss_train: 0.0621 loss_val: 0.0572\n",
      "Epoch: 2463 loss_train: 0.0603 loss_val: 0.0552\n",
      "Epoch: 2464 loss_train: 0.0610 loss_val: 0.0571\n",
      "Epoch: 2465 loss_train: 0.0597 loss_val: 0.0548\n",
      "Epoch: 2466 loss_train: 0.0611 loss_val: 0.0558\n",
      "Epoch: 2467 loss_train: 0.0616 loss_val: 0.0555\n",
      "Epoch: 2468 loss_train: 0.0612 loss_val: 0.0555\n",
      "Epoch: 2469 loss_train: 0.0620 loss_val: 0.0563\n",
      "Epoch: 2470 loss_train: 0.0629 loss_val: 0.0568\n",
      "Epoch: 2471 loss_train: 0.0622 loss_val: 0.0547\n",
      "Epoch: 2472 loss_train: 0.0615 loss_val: 0.0553\n",
      "Epoch: 2473 loss_train: 0.0609 loss_val: 0.0553\n",
      "Epoch: 2474 loss_train: 0.0633 loss_val: 0.0555\n",
      "Epoch: 2475 loss_train: 0.0612 loss_val: 0.0540\n",
      "Epoch: 2476 loss_train: 0.0610 loss_val: 0.0551\n",
      "Epoch: 2477 loss_train: 0.0611 loss_val: 0.0543\n",
      "Epoch: 2478 loss_train: 0.0611 loss_val: 0.0545\n",
      "Epoch: 2479 loss_train: 0.0617 loss_val: 0.0553\n",
      "Epoch: 2480 loss_train: 0.0621 loss_val: 0.0558\n",
      "Epoch: 2481 loss_train: 0.0608 loss_val: 0.0566\n",
      "Epoch: 2482 loss_train: 0.0619 loss_val: 0.0564\n",
      "Epoch: 2483 loss_train: 0.0616 loss_val: 0.0547\n",
      "Epoch: 2484 loss_train: 0.0604 loss_val: 0.0559\n",
      "Epoch: 2485 loss_train: 0.0602 loss_val: 0.0553\n",
      "Epoch: 2486 loss_train: 0.0600 loss_val: 0.0558\n",
      "Epoch: 2487 loss_train: 0.0612 loss_val: 0.0564\n",
      "Epoch: 2488 loss_train: 0.0597 loss_val: 0.0558\n",
      "Epoch: 2489 loss_train: 0.0602 loss_val: 0.0542\n",
      "Epoch: 2490 loss_train: 0.0610 loss_val: 0.0562\n",
      "Epoch: 2491 loss_train: 0.0612 loss_val: 0.0565\n",
      "Epoch: 2492 loss_train: 0.0620 loss_val: 0.0552\n",
      "Epoch: 2493 loss_train: 0.0603 loss_val: 0.0571\n",
      "Epoch: 2494 loss_train: 0.0609 loss_val: 0.0568\n",
      "Epoch: 2495 loss_train: 0.0614 loss_val: 0.0554\n",
      "Epoch: 2496 loss_train: 0.0612 loss_val: 0.0544\n",
      "Epoch: 2497 loss_train: 0.0612 loss_val: 0.0573\n",
      "Epoch: 2498 loss_train: 0.0618 loss_val: 0.0559\n",
      "Epoch: 2499 loss_train: 0.0604 loss_val: 0.0557\n",
      "Epoch: 2500 loss_train: 0.0612 loss_val: 0.0552\n",
      "Epoch: 2501 loss_train: 0.0602 loss_val: 0.0565\n",
      "Epoch: 2502 loss_train: 0.0605 loss_val: 0.0556\n",
      "Epoch: 2503 loss_train: 0.0605 loss_val: 0.0564\n",
      "Epoch: 2504 loss_train: 0.0611 loss_val: 0.0555\n",
      "Epoch: 2505 loss_train: 0.0604 loss_val: 0.0564\n",
      "Epoch: 2506 loss_train: 0.0635 loss_val: 0.0550\n",
      "Epoch: 2507 loss_train: 0.0636 loss_val: 0.0547\n",
      "Epoch: 2508 loss_train: 0.0623 loss_val: 0.0562\n",
      "Epoch: 2509 loss_train: 0.0618 loss_val: 0.0547\n",
      "Epoch: 2510 loss_train: 0.0618 loss_val: 0.0569\n",
      "Epoch: 2511 loss_train: 0.0623 loss_val: 0.0537\n",
      "Epoch: 2512 loss_train: 0.0612 loss_val: 0.0553\n",
      "Epoch: 2513 loss_train: 0.0602 loss_val: 0.0553\n",
      "Epoch: 2514 loss_train: 0.0613 loss_val: 0.0546\n",
      "Epoch: 2515 loss_train: 0.0619 loss_val: 0.0568\n",
      "Epoch: 2516 loss_train: 0.0615 loss_val: 0.0565\n",
      "Epoch: 2517 loss_train: 0.0611 loss_val: 0.0552\n",
      "Epoch: 2518 loss_train: 0.0601 loss_val: 0.0555\n",
      "Epoch: 2519 loss_train: 0.0614 loss_val: 0.0546\n",
      "Epoch: 2520 loss_train: 0.0606 loss_val: 0.0561\n",
      "Epoch: 2521 loss_train: 0.0612 loss_val: 0.0561\n",
      "Epoch: 2522 loss_train: 0.0622 loss_val: 0.0564\n",
      "Epoch: 2523 loss_train: 0.0600 loss_val: 0.0560\n",
      "Epoch: 2524 loss_train: 0.0613 loss_val: 0.0557\n",
      "Epoch: 2525 loss_train: 0.0619 loss_val: 0.0551\n",
      "Epoch: 2526 loss_train: 0.0632 loss_val: 0.0544\n",
      "Epoch: 2527 loss_train: 0.0616 loss_val: 0.0554\n",
      "Epoch: 2528 loss_train: 0.0613 loss_val: 0.0559\n",
      "Epoch: 2529 loss_train: 0.0610 loss_val: 0.0547\n",
      "Epoch: 2530 loss_train: 0.0616 loss_val: 0.0573\n",
      "Epoch: 2531 loss_train: 0.0611 loss_val: 0.0549\n",
      "Epoch: 2532 loss_train: 0.0609 loss_val: 0.0547\n",
      "Epoch: 2533 loss_train: 0.0616 loss_val: 0.0556\n",
      "Epoch: 2534 loss_train: 0.0617 loss_val: 0.0564\n",
      "Epoch: 2535 loss_train: 0.0602 loss_val: 0.0568\n",
      "Epoch: 2536 loss_train: 0.0610 loss_val: 0.0556\n",
      "Epoch: 2537 loss_train: 0.0619 loss_val: 0.0560\n",
      "Epoch: 2538 loss_train: 0.0622 loss_val: 0.0553\n",
      "Epoch: 2539 loss_train: 0.0603 loss_val: 0.0559\n",
      "Epoch: 2540 loss_train: 0.0606 loss_val: 0.0554\n",
      "Epoch: 2541 loss_train: 0.0615 loss_val: 0.0529\n",
      "Epoch: 2542 loss_train: 0.0615 loss_val: 0.0539\n",
      "Epoch: 2543 loss_train: 0.0611 loss_val: 0.0547\n",
      "Epoch: 2544 loss_train: 0.0600 loss_val: 0.0543\n",
      "Epoch: 2545 loss_train: 0.0609 loss_val: 0.0544\n",
      "Epoch: 2546 loss_train: 0.0607 loss_val: 0.0546\n",
      "Epoch: 2547 loss_train: 0.0593 loss_val: 0.0538\n",
      "Epoch: 2548 loss_train: 0.0620 loss_val: 0.0543\n",
      "Epoch: 2549 loss_train: 0.0607 loss_val: 0.0550\n",
      "Epoch: 2550 loss_train: 0.0610 loss_val: 0.0541\n",
      "Epoch: 2551 loss_train: 0.0598 loss_val: 0.0544\n",
      "Epoch: 2552 loss_train: 0.0590 loss_val: 0.0548\n",
      "Epoch: 2553 loss_train: 0.0615 loss_val: 0.0547\n",
      "Epoch: 2554 loss_train: 0.0606 loss_val: 0.0562\n",
      "Epoch: 2555 loss_train: 0.0613 loss_val: 0.0552\n",
      "Epoch: 2556 loss_train: 0.0617 loss_val: 0.0547\n",
      "Epoch: 2557 loss_train: 0.0607 loss_val: 0.0547\n",
      "Epoch: 2558 loss_train: 0.0599 loss_val: 0.0555\n",
      "Epoch: 2559 loss_train: 0.0620 loss_val: 0.0567\n",
      "Epoch: 2560 loss_train: 0.0613 loss_val: 0.0550\n",
      "Epoch: 2561 loss_train: 0.0600 loss_val: 0.0553\n",
      "Epoch: 2562 loss_train: 0.0604 loss_val: 0.0562\n",
      "Epoch: 2563 loss_train: 0.0601 loss_val: 0.0564\n",
      "Epoch: 2564 loss_train: 0.0596 loss_val: 0.0592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2565 loss_train: 0.0607 loss_val: 0.0563\n",
      "Epoch: 2566 loss_train: 0.0600 loss_val: 0.0557\n",
      "Epoch: 2567 loss_train: 0.0608 loss_val: 0.0553\n",
      "Epoch: 2568 loss_train: 0.0612 loss_val: 0.0552\n",
      "Epoch: 2569 loss_train: 0.0614 loss_val: 0.0544\n",
      "Epoch: 2570 loss_train: 0.0591 loss_val: 0.0548\n",
      "Epoch: 2571 loss_train: 0.0597 loss_val: 0.0560\n",
      "Epoch: 2572 loss_train: 0.0613 loss_val: 0.0545\n",
      "Epoch: 2573 loss_train: 0.0598 loss_val: 0.0543\n",
      "Epoch: 2574 loss_train: 0.0610 loss_val: 0.0559\n",
      "Epoch: 2575 loss_train: 0.0614 loss_val: 0.0547\n",
      "Epoch: 2576 loss_train: 0.0603 loss_val: 0.0545\n",
      "Epoch: 2577 loss_train: 0.0600 loss_val: 0.0546\n",
      "Epoch: 2578 loss_train: 0.0612 loss_val: 0.0570\n",
      "Epoch: 2579 loss_train: 0.0607 loss_val: 0.0547\n",
      "Epoch: 2580 loss_train: 0.0616 loss_val: 0.0532\n",
      "Epoch: 2581 loss_train: 0.0613 loss_val: 0.0559\n",
      "Epoch: 2582 loss_train: 0.0603 loss_val: 0.0547\n",
      "Epoch: 2583 loss_train: 0.0613 loss_val: 0.0538\n",
      "Epoch: 2584 loss_train: 0.0616 loss_val: 0.0541\n",
      "Epoch: 2585 loss_train: 0.0613 loss_val: 0.0541\n",
      "Epoch: 2586 loss_train: 0.0619 loss_val: 0.0539\n",
      "Epoch: 2587 loss_train: 0.0596 loss_val: 0.0539\n",
      "Epoch: 2588 loss_train: 0.0607 loss_val: 0.0537\n",
      "Epoch: 2589 loss_train: 0.0598 loss_val: 0.0536\n",
      "Epoch: 2590 loss_train: 0.0606 loss_val: 0.0550\n",
      "Epoch: 2591 loss_train: 0.0603 loss_val: 0.0548\n",
      "Epoch: 2592 loss_train: 0.0593 loss_val: 0.0534\n",
      "Epoch: 2593 loss_train: 0.0602 loss_val: 0.0531\n",
      "Epoch: 2594 loss_train: 0.0613 loss_val: 0.0548\n",
      "Epoch: 2595 loss_train: 0.0611 loss_val: 0.0540\n",
      "Epoch: 2596 loss_train: 0.0603 loss_val: 0.0542\n",
      "Epoch: 2597 loss_train: 0.0605 loss_val: 0.0543\n",
      "Epoch: 2598 loss_train: 0.0597 loss_val: 0.0544\n",
      "Epoch: 2599 loss_train: 0.0602 loss_val: 0.0565\n",
      "Epoch: 2600 loss_train: 0.0606 loss_val: 0.0535\n",
      "Epoch: 2601 loss_train: 0.0609 loss_val: 0.0554\n",
      "Epoch: 2602 loss_train: 0.0594 loss_val: 0.0544\n",
      "Epoch: 2603 loss_train: 0.0604 loss_val: 0.0535\n",
      "Epoch: 2604 loss_train: 0.0601 loss_val: 0.0558\n",
      "Epoch: 2605 loss_train: 0.0607 loss_val: 0.0570\n",
      "Epoch: 2606 loss_train: 0.0597 loss_val: 0.0569\n",
      "Epoch: 2607 loss_train: 0.0599 loss_val: 0.0566\n",
      "Epoch: 2608 loss_train: 0.0618 loss_val: 0.0563\n",
      "Epoch: 2609 loss_train: 0.0599 loss_val: 0.0561\n",
      "Epoch: 2610 loss_train: 0.0595 loss_val: 0.0559\n",
      "Epoch: 2611 loss_train: 0.0583 loss_val: 0.0570\n",
      "Epoch: 2612 loss_train: 0.0608 loss_val: 0.0561\n",
      "Epoch: 2613 loss_train: 0.0610 loss_val: 0.0549\n",
      "Epoch: 2614 loss_train: 0.0605 loss_val: 0.0558\n",
      "Epoch: 2615 loss_train: 0.0614 loss_val: 0.0548\n",
      "Epoch: 2616 loss_train: 0.0600 loss_val: 0.0533\n",
      "Epoch: 2617 loss_train: 0.0594 loss_val: 0.0552\n",
      "Epoch: 2618 loss_train: 0.0611 loss_val: 0.0560\n",
      "Epoch: 2619 loss_train: 0.0595 loss_val: 0.0557\n",
      "Epoch: 2620 loss_train: 0.0604 loss_val: 0.0550\n",
      "Epoch: 2621 loss_train: 0.0598 loss_val: 0.0553\n",
      "Epoch: 2622 loss_train: 0.0603 loss_val: 0.0552\n",
      "Epoch: 2623 loss_train: 0.0606 loss_val: 0.0547\n",
      "Epoch: 2624 loss_train: 0.0605 loss_val: 0.0541\n",
      "Epoch: 2625 loss_train: 0.0614 loss_val: 0.0553\n",
      "Epoch: 2626 loss_train: 0.0601 loss_val: 0.0560\n",
      "Epoch: 2627 loss_train: 0.0597 loss_val: 0.0562\n",
      "Epoch: 2628 loss_train: 0.0594 loss_val: 0.0573\n",
      "Epoch: 2629 loss_train: 0.0602 loss_val: 0.0554\n",
      "Epoch: 2630 loss_train: 0.0594 loss_val: 0.0572\n",
      "Epoch: 2631 loss_train: 0.0618 loss_val: 0.0565\n",
      "Epoch: 2632 loss_train: 0.0615 loss_val: 0.0561\n",
      "Epoch: 2633 loss_train: 0.0611 loss_val: 0.0560\n",
      "Epoch: 2634 loss_train: 0.0604 loss_val: 0.0573\n",
      "Epoch: 2635 loss_train: 0.0602 loss_val: 0.0560\n",
      "Epoch: 2636 loss_train: 0.0610 loss_val: 0.0556\n",
      "Epoch: 2637 loss_train: 0.0611 loss_val: 0.0573\n",
      "Epoch: 2638 loss_train: 0.0604 loss_val: 0.0577\n",
      "Epoch: 2639 loss_train: 0.0597 loss_val: 0.0576\n",
      "Epoch: 2640 loss_train: 0.0610 loss_val: 0.0580\n",
      "Epoch: 2641 loss_train: 0.0601 loss_val: 0.0569\n",
      "Epoch: 2642 loss_train: 0.0597 loss_val: 0.0568\n",
      "Epoch: 2643 loss_train: 0.0597 loss_val: 0.0582\n",
      "Epoch: 2644 loss_train: 0.0608 loss_val: 0.0557\n",
      "Epoch: 2645 loss_train: 0.0591 loss_val: 0.0555\n",
      "Epoch: 2646 loss_train: 0.0612 loss_val: 0.0553\n",
      "Epoch: 2647 loss_train: 0.0607 loss_val: 0.0554\n",
      "Epoch: 2648 loss_train: 0.0608 loss_val: 0.0549\n",
      "Epoch: 2649 loss_train: 0.0598 loss_val: 0.0584\n",
      "Epoch: 2650 loss_train: 0.0614 loss_val: 0.0558\n",
      "Epoch: 2651 loss_train: 0.0606 loss_val: 0.0572\n",
      "Epoch: 2652 loss_train: 0.0608 loss_val: 0.0564\n",
      "Epoch: 2653 loss_train: 0.0610 loss_val: 0.0567\n",
      "Epoch: 2654 loss_train: 0.0607 loss_val: 0.0573\n",
      "Epoch: 2655 loss_train: 0.0607 loss_val: 0.0576\n",
      "Epoch: 2656 loss_train: 0.0600 loss_val: 0.0582\n",
      "Epoch: 2657 loss_train: 0.0606 loss_val: 0.0594\n",
      "Epoch: 2658 loss_train: 0.0616 loss_val: 0.0583\n",
      "Epoch: 2659 loss_train: 0.0601 loss_val: 0.0581\n",
      "Epoch: 2660 loss_train: 0.0592 loss_val: 0.0577\n",
      "Epoch: 2661 loss_train: 0.0587 loss_val: 0.0573\n",
      "Epoch: 2662 loss_train: 0.0609 loss_val: 0.0561\n",
      "Epoch: 2663 loss_train: 0.0613 loss_val: 0.0569\n",
      "Epoch: 2664 loss_train: 0.0602 loss_val: 0.0564\n",
      "Epoch: 2665 loss_train: 0.0613 loss_val: 0.0575\n",
      "Epoch: 2666 loss_train: 0.0593 loss_val: 0.0588\n",
      "Epoch: 2667 loss_train: 0.0606 loss_val: 0.0578\n",
      "Epoch: 2668 loss_train: 0.0607 loss_val: 0.0577\n",
      "Epoch: 2669 loss_train: 0.0608 loss_val: 0.0559\n",
      "Epoch: 2670 loss_train: 0.0610 loss_val: 0.0558\n",
      "Epoch: 2671 loss_train: 0.0616 loss_val: 0.0566\n",
      "Epoch: 2672 loss_train: 0.0608 loss_val: 0.0534\n",
      "Epoch: 2673 loss_train: 0.0603 loss_val: 0.0549\n",
      "Epoch: 2674 loss_train: 0.0622 loss_val: 0.0548\n",
      "Epoch: 2675 loss_train: 0.0606 loss_val: 0.0558\n",
      "Epoch: 2676 loss_train: 0.0601 loss_val: 0.0556\n",
      "Epoch: 2677 loss_train: 0.0602 loss_val: 0.0550\n",
      "Epoch: 2678 loss_train: 0.0617 loss_val: 0.0582\n",
      "Epoch: 2679 loss_train: 0.0591 loss_val: 0.0562\n",
      "Epoch: 2680 loss_train: 0.0601 loss_val: 0.0579\n",
      "Epoch: 2681 loss_train: 0.0599 loss_val: 0.0580\n",
      "Epoch: 2682 loss_train: 0.0589 loss_val: 0.0552\n",
      "Epoch: 2683 loss_train: 0.0599 loss_val: 0.0563\n",
      "Epoch: 2684 loss_train: 0.0594 loss_val: 0.0554\n",
      "Epoch: 2685 loss_train: 0.0598 loss_val: 0.0570\n",
      "Epoch: 2686 loss_train: 0.0603 loss_val: 0.0561\n",
      "Epoch: 2687 loss_train: 0.0611 loss_val: 0.0546\n",
      "Epoch: 2688 loss_train: 0.0601 loss_val: 0.0548\n",
      "Epoch: 2689 loss_train: 0.0609 loss_val: 0.0560\n",
      "Epoch: 2690 loss_train: 0.0604 loss_val: 0.0578\n",
      "Epoch: 2691 loss_train: 0.0591 loss_val: 0.0575\n",
      "Epoch: 2692 loss_train: 0.0598 loss_val: 0.0545\n",
      "Epoch: 2693 loss_train: 0.0604 loss_val: 0.0564\n",
      "Epoch: 2694 loss_train: 0.0601 loss_val: 0.0576\n",
      "Epoch: 2695 loss_train: 0.0600 loss_val: 0.0584\n",
      "Epoch: 2696 loss_train: 0.0616 loss_val: 0.0558\n",
      "Epoch: 2697 loss_train: 0.0617 loss_val: 0.0579\n",
      "Epoch: 2698 loss_train: 0.0625 loss_val: 0.0569\n",
      "Epoch: 2699 loss_train: 0.0609 loss_val: 0.0560\n",
      "Epoch: 2700 loss_train: 0.0614 loss_val: 0.0574\n",
      "Epoch: 2701 loss_train: 0.0597 loss_val: 0.0562\n",
      "Epoch: 2702 loss_train: 0.0599 loss_val: 0.0561\n",
      "Epoch: 2703 loss_train: 0.0598 loss_val: 0.0554\n",
      "Epoch: 2704 loss_train: 0.0607 loss_val: 0.0566\n",
      "Epoch: 2705 loss_train: 0.0611 loss_val: 0.0563\n",
      "Epoch: 2706 loss_train: 0.0603 loss_val: 0.0582\n",
      "Epoch: 2707 loss_train: 0.0604 loss_val: 0.0585\n",
      "Epoch: 2708 loss_train: 0.0594 loss_val: 0.0562\n",
      "Epoch: 2709 loss_train: 0.0612 loss_val: 0.0572\n",
      "Epoch: 2710 loss_train: 0.0610 loss_val: 0.0589\n",
      "Epoch: 2711 loss_train: 0.0609 loss_val: 0.0558\n",
      "Epoch: 2712 loss_train: 0.0622 loss_val: 0.0563\n",
      "Epoch: 2713 loss_train: 0.0609 loss_val: 0.0567\n",
      "Epoch: 2714 loss_train: 0.0602 loss_val: 0.0571\n",
      "Epoch: 2715 loss_train: 0.0613 loss_val: 0.0567\n",
      "Epoch: 2716 loss_train: 0.0621 loss_val: 0.0552\n",
      "Epoch: 2717 loss_train: 0.0597 loss_val: 0.0555\n",
      "Epoch: 2718 loss_train: 0.0597 loss_val: 0.0549\n",
      "Epoch: 2719 loss_train: 0.0602 loss_val: 0.0552\n",
      "Epoch: 2720 loss_train: 0.0605 loss_val: 0.0556\n",
      "Epoch: 2721 loss_train: 0.0613 loss_val: 0.0549\n",
      "Epoch: 2722 loss_train: 0.0605 loss_val: 0.0546\n",
      "Epoch: 2723 loss_train: 0.0601 loss_val: 0.0556\n",
      "Epoch: 2724 loss_train: 0.0587 loss_val: 0.0574\n",
      "Epoch: 2725 loss_train: 0.0584 loss_val: 0.0556\n",
      "Epoch: 2726 loss_train: 0.0606 loss_val: 0.0551\n",
      "Epoch: 2727 loss_train: 0.0611 loss_val: 0.0568\n",
      "Epoch: 2728 loss_train: 0.0607 loss_val: 0.0548\n",
      "Epoch: 2729 loss_train: 0.0611 loss_val: 0.0556\n",
      "Epoch: 2730 loss_train: 0.0600 loss_val: 0.0566\n",
      "Epoch: 2731 loss_train: 0.0587 loss_val: 0.0551\n",
      "Epoch: 2732 loss_train: 0.0607 loss_val: 0.0561\n",
      "Epoch: 2733 loss_train: 0.0603 loss_val: 0.0550\n",
      "Epoch: 2734 loss_train: 0.0590 loss_val: 0.0549\n",
      "Epoch: 2735 loss_train: 0.0595 loss_val: 0.0563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2736 loss_train: 0.0603 loss_val: 0.0544\n",
      "Epoch: 2737 loss_train: 0.0596 loss_val: 0.0552\n",
      "Epoch: 2738 loss_train: 0.0595 loss_val: 0.0560\n",
      "Epoch: 2739 loss_train: 0.0597 loss_val: 0.0546\n",
      "Epoch: 2740 loss_train: 0.0602 loss_val: 0.0559\n",
      "Epoch: 2741 loss_train: 0.0601 loss_val: 0.0561\n",
      "Epoch: 2742 loss_train: 0.0586 loss_val: 0.0546\n",
      "Epoch: 2743 loss_train: 0.0597 loss_val: 0.0553\n",
      "Epoch: 2744 loss_train: 0.0593 loss_val: 0.0554\n",
      "Epoch: 2745 loss_train: 0.0592 loss_val: 0.0562\n",
      "Epoch: 2746 loss_train: 0.0604 loss_val: 0.0536\n",
      "Epoch: 2747 loss_train: 0.0600 loss_val: 0.0544\n",
      "Epoch: 2748 loss_train: 0.0603 loss_val: 0.0540\n",
      "Epoch: 2749 loss_train: 0.0601 loss_val: 0.0558\n",
      "Epoch: 2750 loss_train: 0.0603 loss_val: 0.0564\n",
      "Epoch: 2751 loss_train: 0.0600 loss_val: 0.0550\n",
      "Epoch: 2752 loss_train: 0.0605 loss_val: 0.0551\n",
      "Epoch: 2753 loss_train: 0.0591 loss_val: 0.0546\n",
      "Epoch: 2754 loss_train: 0.0591 loss_val: 0.0553\n",
      "Epoch: 2755 loss_train: 0.0597 loss_val: 0.0532\n",
      "Epoch: 2756 loss_train: 0.0599 loss_val: 0.0540\n",
      "Epoch: 2757 loss_train: 0.0602 loss_val: 0.0551\n",
      "Epoch: 2758 loss_train: 0.0612 loss_val: 0.0565\n",
      "Epoch: 2759 loss_train: 0.0606 loss_val: 0.0555\n",
      "Epoch: 2760 loss_train: 0.0611 loss_val: 0.0569\n",
      "Epoch: 2761 loss_train: 0.0610 loss_val: 0.0561\n",
      "Epoch: 2762 loss_train: 0.0601 loss_val: 0.0546\n",
      "Epoch: 2763 loss_train: 0.0602 loss_val: 0.0549\n",
      "Epoch: 2764 loss_train: 0.0593 loss_val: 0.0556\n",
      "Epoch: 2765 loss_train: 0.0605 loss_val: 0.0567\n",
      "Epoch: 2766 loss_train: 0.0598 loss_val: 0.0553\n",
      "Epoch: 2767 loss_train: 0.0598 loss_val: 0.0544\n",
      "Epoch: 2768 loss_train: 0.0603 loss_val: 0.0563\n",
      "Epoch: 2769 loss_train: 0.0621 loss_val: 0.0569\n",
      "Epoch: 2770 loss_train: 0.0616 loss_val: 0.0545\n",
      "Epoch: 2771 loss_train: 0.0620 loss_val: 0.0542\n",
      "Epoch: 2772 loss_train: 0.0602 loss_val: 0.0548\n",
      "Epoch: 2773 loss_train: 0.0607 loss_val: 0.0535\n",
      "Epoch: 2774 loss_train: 0.0592 loss_val: 0.0545\n",
      "Epoch: 2775 loss_train: 0.0593 loss_val: 0.0566\n",
      "Epoch: 2776 loss_train: 0.0617 loss_val: 0.0560\n",
      "Epoch: 2777 loss_train: 0.0606 loss_val: 0.0567\n",
      "Epoch: 2778 loss_train: 0.0601 loss_val: 0.0560\n",
      "Epoch: 2779 loss_train: 0.0601 loss_val: 0.0564\n",
      "Epoch: 2780 loss_train: 0.0601 loss_val: 0.0542\n",
      "Epoch: 2781 loss_train: 0.0596 loss_val: 0.0569\n",
      "Epoch: 2782 loss_train: 0.0607 loss_val: 0.0539\n",
      "Epoch: 2783 loss_train: 0.0595 loss_val: 0.0566\n",
      "Epoch: 2784 loss_train: 0.0600 loss_val: 0.0554\n",
      "Epoch: 2785 loss_train: 0.0601 loss_val: 0.0554\n",
      "Epoch: 2786 loss_train: 0.0628 loss_val: 0.0570\n",
      "Epoch: 2787 loss_train: 0.0610 loss_val: 0.0572\n",
      "Epoch: 2788 loss_train: 0.0607 loss_val: 0.0558\n",
      "Epoch: 2789 loss_train: 0.0614 loss_val: 0.0565\n",
      "Epoch: 2790 loss_train: 0.0604 loss_val: 0.0559\n",
      "Epoch: 2791 loss_train: 0.0602 loss_val: 0.0550\n",
      "Epoch: 2792 loss_train: 0.0602 loss_val: 0.0535\n",
      "Epoch: 2793 loss_train: 0.0596 loss_val: 0.0540\n",
      "Epoch: 2794 loss_train: 0.0618 loss_val: 0.0546\n",
      "Epoch: 2795 loss_train: 0.0609 loss_val: 0.0555\n",
      "Epoch: 2796 loss_train: 0.0598 loss_val: 0.0559\n",
      "Epoch: 2797 loss_train: 0.0593 loss_val: 0.0551\n",
      "Epoch: 2798 loss_train: 0.0600 loss_val: 0.0573\n",
      "Epoch: 2799 loss_train: 0.0603 loss_val: 0.0563\n",
      "Epoch: 2800 loss_train: 0.0604 loss_val: 0.0564\n",
      "Epoch: 2801 loss_train: 0.0598 loss_val: 0.0550\n",
      "Epoch: 2802 loss_train: 0.0599 loss_val: 0.0560\n",
      "Epoch: 2803 loss_train: 0.0600 loss_val: 0.0547\n",
      "Epoch: 2804 loss_train: 0.0600 loss_val: 0.0535\n",
      "Epoch: 2805 loss_train: 0.0596 loss_val: 0.0567\n",
      "Epoch: 2806 loss_train: 0.0600 loss_val: 0.0567\n",
      "Epoch: 2807 loss_train: 0.0594 loss_val: 0.0551\n",
      "Epoch: 2808 loss_train: 0.0599 loss_val: 0.0550\n",
      "Epoch: 2809 loss_train: 0.0587 loss_val: 0.0551\n",
      "Epoch: 2810 loss_train: 0.0597 loss_val: 0.0562\n",
      "Epoch: 2811 loss_train: 0.0595 loss_val: 0.0556\n",
      "Epoch: 2812 loss_train: 0.0599 loss_val: 0.0549\n",
      "Epoch: 2813 loss_train: 0.0599 loss_val: 0.0545\n",
      "Epoch: 2814 loss_train: 0.0598 loss_val: 0.0539\n",
      "Epoch: 2815 loss_train: 0.0600 loss_val: 0.0549\n",
      "Epoch: 2816 loss_train: 0.0591 loss_val: 0.0564\n",
      "Epoch: 2817 loss_train: 0.0588 loss_val: 0.0550\n",
      "Epoch: 2818 loss_train: 0.0594 loss_val: 0.0551\n",
      "Epoch: 2819 loss_train: 0.0592 loss_val: 0.0557\n",
      "Epoch: 2820 loss_train: 0.0591 loss_val: 0.0559\n",
      "Epoch: 2821 loss_train: 0.0604 loss_val: 0.0552\n",
      "Epoch: 2822 loss_train: 0.0608 loss_val: 0.0558\n",
      "Epoch: 2823 loss_train: 0.0603 loss_val: 0.0583\n",
      "Epoch: 2824 loss_train: 0.0600 loss_val: 0.0565\n",
      "Epoch: 2825 loss_train: 0.0594 loss_val: 0.0567\n",
      "Epoch: 2826 loss_train: 0.0602 loss_val: 0.0577\n",
      "Epoch: 2827 loss_train: 0.0625 loss_val: 0.0550\n",
      "Epoch: 2828 loss_train: 0.0608 loss_val: 0.0522\n",
      "Epoch: 2829 loss_train: 0.0607 loss_val: 0.0531\n",
      "Epoch: 2830 loss_train: 0.0600 loss_val: 0.0534\n",
      "Epoch: 2831 loss_train: 0.0600 loss_val: 0.0547\n",
      "Epoch: 2832 loss_train: 0.0586 loss_val: 0.0552\n",
      "Epoch: 2833 loss_train: 0.0603 loss_val: 0.0551\n",
      "Epoch: 2834 loss_train: 0.0611 loss_val: 0.0567\n",
      "Epoch: 2835 loss_train: 0.0612 loss_val: 0.0559\n",
      "Epoch: 2836 loss_train: 0.0606 loss_val: 0.0539\n",
      "Epoch: 2837 loss_train: 0.0593 loss_val: 0.0556\n",
      "Epoch: 2838 loss_train: 0.0595 loss_val: 0.0545\n",
      "Epoch: 2839 loss_train: 0.0576 loss_val: 0.0542\n",
      "Epoch: 2840 loss_train: 0.0602 loss_val: 0.0554\n",
      "Epoch: 2841 loss_train: 0.0603 loss_val: 0.0545\n",
      "Epoch: 2842 loss_train: 0.0610 loss_val: 0.0570\n",
      "Epoch: 2843 loss_train: 0.0627 loss_val: 0.0564\n",
      "Epoch: 2844 loss_train: 0.0622 loss_val: 0.0566\n",
      "Epoch: 2845 loss_train: 0.0600 loss_val: 0.0565\n",
      "Epoch: 2846 loss_train: 0.0595 loss_val: 0.0574\n",
      "Epoch: 2847 loss_train: 0.0612 loss_val: 0.0550\n",
      "Epoch: 2848 loss_train: 0.0605 loss_val: 0.0565\n",
      "Epoch: 2849 loss_train: 0.0594 loss_val: 0.0572\n",
      "Epoch: 2850 loss_train: 0.0596 loss_val: 0.0556\n",
      "Epoch: 2851 loss_train: 0.0605 loss_val: 0.0573\n",
      "Epoch: 2852 loss_train: 0.0587 loss_val: 0.0556\n",
      "Epoch: 2853 loss_train: 0.0592 loss_val: 0.0580\n",
      "Epoch: 2854 loss_train: 0.0604 loss_val: 0.0583\n",
      "Epoch: 2855 loss_train: 0.0607 loss_val: 0.0573\n",
      "Epoch: 2856 loss_train: 0.0581 loss_val: 0.0570\n",
      "Epoch: 2857 loss_train: 0.0596 loss_val: 0.0564\n",
      "Epoch: 2858 loss_train: 0.0603 loss_val: 0.0549\n",
      "Epoch: 2859 loss_train: 0.0614 loss_val: 0.0569\n",
      "Epoch: 2860 loss_train: 0.0604 loss_val: 0.0583\n",
      "Epoch: 2861 loss_train: 0.0595 loss_val: 0.0554\n",
      "Epoch: 2862 loss_train: 0.0605 loss_val: 0.0552\n",
      "Epoch: 2863 loss_train: 0.0599 loss_val: 0.0537\n",
      "Epoch: 2864 loss_train: 0.0602 loss_val: 0.0573\n",
      "Epoch: 2865 loss_train: 0.0589 loss_val: 0.0558\n",
      "Epoch: 2866 loss_train: 0.0591 loss_val: 0.0573\n",
      "Epoch: 2867 loss_train: 0.0600 loss_val: 0.0579\n",
      "Epoch: 2868 loss_train: 0.0588 loss_val: 0.0570\n",
      "Epoch: 2869 loss_train: 0.0600 loss_val: 0.0573\n",
      "Epoch: 2870 loss_train: 0.0593 loss_val: 0.0560\n",
      "Epoch: 2871 loss_train: 0.0607 loss_val: 0.0554\n",
      "Epoch: 2872 loss_train: 0.0597 loss_val: 0.0566\n",
      "Epoch: 2873 loss_train: 0.0576 loss_val: 0.0536\n",
      "Epoch: 2874 loss_train: 0.0591 loss_val: 0.0554\n",
      "Epoch: 2875 loss_train: 0.0615 loss_val: 0.0564\n",
      "Epoch: 2876 loss_train: 0.0597 loss_val: 0.0571\n",
      "Epoch: 2877 loss_train: 0.0598 loss_val: 0.0571\n",
      "Epoch: 2878 loss_train: 0.0599 loss_val: 0.0570\n",
      "Epoch: 2879 loss_train: 0.0610 loss_val: 0.0556\n",
      "Epoch: 2880 loss_train: 0.0608 loss_val: 0.0545\n",
      "Epoch: 2881 loss_train: 0.0603 loss_val: 0.0563\n",
      "Epoch: 2882 loss_train: 0.0594 loss_val: 0.0572\n",
      "Epoch: 2883 loss_train: 0.0597 loss_val: 0.0577\n",
      "Epoch: 2884 loss_train: 0.0610 loss_val: 0.0583\n",
      "Epoch: 2885 loss_train: 0.0588 loss_val: 0.0564\n",
      "Epoch: 2886 loss_train: 0.0604 loss_val: 0.0572\n",
      "Epoch: 2887 loss_train: 0.0604 loss_val: 0.0562\n",
      "Epoch: 2888 loss_train: 0.0589 loss_val: 0.0550\n",
      "Epoch: 2889 loss_train: 0.0592 loss_val: 0.0555\n",
      "Epoch: 2890 loss_train: 0.0599 loss_val: 0.0576\n",
      "Epoch: 2891 loss_train: 0.0599 loss_val: 0.0580\n",
      "Epoch: 2892 loss_train: 0.0599 loss_val: 0.0546\n",
      "Epoch: 2893 loss_train: 0.0596 loss_val: 0.0584\n",
      "Epoch: 2894 loss_train: 0.0589 loss_val: 0.0574\n",
      "Epoch: 2895 loss_train: 0.0590 loss_val: 0.0576\n",
      "Epoch: 2896 loss_train: 0.0596 loss_val: 0.0572\n",
      "Epoch: 2897 loss_train: 0.0599 loss_val: 0.0553\n",
      "Epoch: 2898 loss_train: 0.0621 loss_val: 0.0549\n",
      "Epoch: 2899 loss_train: 0.0604 loss_val: 0.0555\n",
      "Epoch: 2900 loss_train: 0.0606 loss_val: 0.0554\n",
      "Epoch: 2901 loss_train: 0.0604 loss_val: 0.0577\n",
      "Epoch: 2902 loss_train: 0.0599 loss_val: 0.0562\n",
      "Epoch: 2903 loss_train: 0.0606 loss_val: 0.0561\n",
      "Epoch: 2904 loss_train: 0.0596 loss_val: 0.0562\n",
      "Epoch: 2905 loss_train: 0.0599 loss_val: 0.0564\n",
      "Epoch: 2906 loss_train: 0.0590 loss_val: 0.0553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2907 loss_train: 0.0599 loss_val: 0.0590\n",
      "Epoch: 2908 loss_train: 0.0602 loss_val: 0.0586\n",
      "Epoch: 2909 loss_train: 0.0598 loss_val: 0.0560\n",
      "Epoch: 2910 loss_train: 0.0603 loss_val: 0.0559\n",
      "Epoch: 2911 loss_train: 0.0602 loss_val: 0.0574\n",
      "Epoch: 2912 loss_train: 0.0602 loss_val: 0.0583\n",
      "Epoch: 2913 loss_train: 0.0594 loss_val: 0.0578\n",
      "Epoch: 2914 loss_train: 0.0602 loss_val: 0.0570\n",
      "Epoch: 2915 loss_train: 0.0594 loss_val: 0.0580\n",
      "Epoch: 2916 loss_train: 0.0592 loss_val: 0.0558\n",
      "Epoch: 2917 loss_train: 0.0596 loss_val: 0.0578\n",
      "Epoch: 2918 loss_train: 0.0599 loss_val: 0.0584\n",
      "Epoch: 2919 loss_train: 0.0599 loss_val: 0.0581\n",
      "Epoch: 2920 loss_train: 0.0617 loss_val: 0.0555\n",
      "Epoch: 2921 loss_train: 0.0613 loss_val: 0.0554\n",
      "Epoch: 2922 loss_train: 0.0621 loss_val: 0.0589\n",
      "Epoch: 2923 loss_train: 0.0602 loss_val: 0.0560\n",
      "Epoch: 2924 loss_train: 0.0598 loss_val: 0.0559\n",
      "Epoch: 2925 loss_train: 0.0603 loss_val: 0.0559\n",
      "Epoch: 2926 loss_train: 0.0598 loss_val: 0.0568\n",
      "Epoch: 2927 loss_train: 0.0591 loss_val: 0.0548\n",
      "Epoch: 2928 loss_train: 0.0593 loss_val: 0.0576\n",
      "Epoch: 2929 loss_train: 0.0601 loss_val: 0.0549\n",
      "Epoch: 2930 loss_train: 0.0597 loss_val: 0.0537\n",
      "Epoch: 2931 loss_train: 0.0595 loss_val: 0.0543\n",
      "Epoch: 2932 loss_train: 0.0595 loss_val: 0.0555\n",
      "Epoch: 2933 loss_train: 0.0600 loss_val: 0.0545\n",
      "Epoch: 2934 loss_train: 0.0596 loss_val: 0.0557\n",
      "Epoch: 2935 loss_train: 0.0595 loss_val: 0.0557\n",
      "Epoch: 2936 loss_train: 0.0588 loss_val: 0.0555\n",
      "Epoch: 2937 loss_train: 0.0591 loss_val: 0.0557\n",
      "Epoch: 2938 loss_train: 0.0595 loss_val: 0.0552\n",
      "Epoch: 2939 loss_train: 0.0596 loss_val: 0.0556\n",
      "Epoch: 2940 loss_train: 0.0592 loss_val: 0.0568\n",
      "Epoch: 2941 loss_train: 0.0589 loss_val: 0.0534\n",
      "Epoch: 2942 loss_train: 0.0592 loss_val: 0.0570\n",
      "Epoch: 2943 loss_train: 0.0592 loss_val: 0.0543\n",
      "Epoch: 2944 loss_train: 0.0579 loss_val: 0.0579\n",
      "Epoch: 2945 loss_train: 0.0582 loss_val: 0.0563\n",
      "Epoch: 2946 loss_train: 0.0578 loss_val: 0.0568\n",
      "Epoch: 2947 loss_train: 0.0591 loss_val: 0.0545\n",
      "Epoch: 2948 loss_train: 0.0581 loss_val: 0.0562\n",
      "Epoch: 2949 loss_train: 0.0600 loss_val: 0.0575\n",
      "Epoch: 2950 loss_train: 0.0591 loss_val: 0.0561\n",
      "Epoch: 2951 loss_train: 0.0604 loss_val: 0.0571\n",
      "Epoch: 2952 loss_train: 0.0587 loss_val: 0.0569\n",
      "Epoch: 2953 loss_train: 0.0592 loss_val: 0.0568\n",
      "Epoch: 2954 loss_train: 0.0603 loss_val: 0.0556\n",
      "Epoch: 2955 loss_train: 0.0588 loss_val: 0.0544\n",
      "Epoch: 2956 loss_train: 0.0598 loss_val: 0.0566\n",
      "Epoch: 2957 loss_train: 0.0595 loss_val: 0.0569\n",
      "Epoch: 2958 loss_train: 0.0590 loss_val: 0.0561\n",
      "Epoch: 2959 loss_train: 0.0588 loss_val: 0.0553\n",
      "Epoch: 2960 loss_train: 0.0589 loss_val: 0.0591\n",
      "Epoch: 2961 loss_train: 0.0598 loss_val: 0.0558\n",
      "Epoch: 2962 loss_train: 0.0596 loss_val: 0.0561\n",
      "Epoch: 2963 loss_train: 0.0591 loss_val: 0.0549\n",
      "Epoch: 2964 loss_train: 0.0601 loss_val: 0.0567\n",
      "Epoch: 2965 loss_train: 0.0613 loss_val: 0.0562\n",
      "Epoch: 2966 loss_train: 0.0589 loss_val: 0.0570\n",
      "Epoch: 2967 loss_train: 0.0597 loss_val: 0.0585\n",
      "Epoch: 2968 loss_train: 0.0590 loss_val: 0.0562\n",
      "Epoch: 2969 loss_train: 0.0606 loss_val: 0.0553\n",
      "Epoch: 2970 loss_train: 0.0600 loss_val: 0.0570\n",
      "Epoch: 2971 loss_train: 0.0597 loss_val: 0.0568\n",
      "Epoch: 2972 loss_train: 0.0590 loss_val: 0.0570\n",
      "Epoch: 2973 loss_train: 0.0591 loss_val: 0.0554\n",
      "Epoch: 2974 loss_train: 0.0596 loss_val: 0.0571\n",
      "Epoch: 2975 loss_train: 0.0593 loss_val: 0.0582\n",
      "Epoch: 2976 loss_train: 0.0590 loss_val: 0.0573\n",
      "Epoch: 2977 loss_train: 0.0600 loss_val: 0.0563\n",
      "Epoch: 2978 loss_train: 0.0589 loss_val: 0.0561\n",
      "Epoch: 2979 loss_train: 0.0601 loss_val: 0.0574\n",
      "Epoch: 2980 loss_train: 0.0591 loss_val: 0.0561\n",
      "Epoch: 2981 loss_train: 0.0600 loss_val: 0.0561\n",
      "Epoch: 2982 loss_train: 0.0599 loss_val: 0.0573\n",
      "Epoch: 2983 loss_train: 0.0593 loss_val: 0.0552\n",
      "Epoch: 2984 loss_train: 0.0594 loss_val: 0.0567\n",
      "Epoch: 2985 loss_train: 0.0599 loss_val: 0.0565\n",
      "Epoch: 2986 loss_train: 0.0601 loss_val: 0.0566\n",
      "Epoch: 2987 loss_train: 0.0595 loss_val: 0.0563\n",
      "Epoch: 2988 loss_train: 0.0587 loss_val: 0.0584\n",
      "Epoch: 2989 loss_train: 0.0587 loss_val: 0.0580\n",
      "Epoch: 2990 loss_train: 0.0594 loss_val: 0.0559\n",
      "Epoch: 2991 loss_train: 0.0594 loss_val: 0.0579\n",
      "Epoch: 2992 loss_train: 0.0604 loss_val: 0.0546\n",
      "Epoch: 2993 loss_train: 0.0587 loss_val: 0.0550\n",
      "Epoch: 2994 loss_train: 0.0598 loss_val: 0.0553\n",
      "Epoch: 2995 loss_train: 0.0592 loss_val: 0.0573\n",
      "Epoch: 2996 loss_train: 0.0606 loss_val: 0.0590\n",
      "Epoch: 2997 loss_train: 0.0597 loss_val: 0.0578\n",
      "Epoch: 2998 loss_train: 0.0579 loss_val: 0.0562\n",
      "Epoch: 2999 loss_train: 0.0588 loss_val: 0.0563\n",
      "Epoch: 3000 loss_train: 0.0594 loss_val: 0.0539\n",
      "Epoch: 3001 loss_train: 0.0596 loss_val: 0.0539\n",
      "Epoch: 3002 loss_train: 0.0581 loss_val: 0.0549\n",
      "Epoch: 3003 loss_train: 0.0585 loss_val: 0.0562\n",
      "Epoch: 3004 loss_train: 0.0595 loss_val: 0.0553\n",
      "Epoch: 3005 loss_train: 0.0588 loss_val: 0.0554\n",
      "Epoch: 3006 loss_train: 0.0596 loss_val: 0.0552\n",
      "Epoch: 3007 loss_train: 0.0599 loss_val: 0.0570\n",
      "Epoch: 3008 loss_train: 0.0611 loss_val: 0.0560\n",
      "Epoch: 3009 loss_train: 0.0593 loss_val: 0.0559\n",
      "Epoch: 3010 loss_train: 0.0587 loss_val: 0.0566\n",
      "Epoch: 3011 loss_train: 0.0595 loss_val: 0.0583\n",
      "Epoch: 3012 loss_train: 0.0597 loss_val: 0.0564\n",
      "Epoch: 3013 loss_train: 0.0605 loss_val: 0.0568\n",
      "Epoch: 3014 loss_train: 0.0605 loss_val: 0.0572\n",
      "Epoch: 3015 loss_train: 0.0598 loss_val: 0.0565\n",
      "Epoch: 3016 loss_train: 0.0580 loss_val: 0.0535\n",
      "Epoch: 3017 loss_train: 0.0595 loss_val: 0.0553\n",
      "Epoch: 3018 loss_train: 0.0614 loss_val: 0.0560\n",
      "Epoch: 3019 loss_train: 0.0602 loss_val: 0.0544\n",
      "Epoch: 3020 loss_train: 0.0589 loss_val: 0.0545\n",
      "Epoch: 3021 loss_train: 0.0597 loss_val: 0.0568\n",
      "Epoch: 3022 loss_train: 0.0605 loss_val: 0.0553\n",
      "Epoch: 3023 loss_train: 0.0597 loss_val: 0.0562\n",
      "Epoch: 3024 loss_train: 0.0589 loss_val: 0.0560\n",
      "Epoch: 3025 loss_train: 0.0601 loss_val: 0.0565\n",
      "Epoch: 3026 loss_train: 0.0596 loss_val: 0.0567\n",
      "Epoch: 3027 loss_train: 0.0594 loss_val: 0.0562\n",
      "Epoch: 3028 loss_train: 0.0587 loss_val: 0.0567\n",
      "Epoch: 3029 loss_train: 0.0587 loss_val: 0.0556\n",
      "Epoch: 3030 loss_train: 0.0583 loss_val: 0.0565\n",
      "Epoch: 3031 loss_train: 0.0590 loss_val: 0.0557\n",
      "Epoch: 3032 loss_train: 0.0598 loss_val: 0.0527\n",
      "Epoch: 3033 loss_train: 0.0597 loss_val: 0.0555\n",
      "Epoch: 3034 loss_train: 0.0580 loss_val: 0.0551\n",
      "Epoch: 3035 loss_train: 0.0594 loss_val: 0.0543\n",
      "Epoch: 3036 loss_train: 0.0612 loss_val: 0.0559\n",
      "Epoch: 3037 loss_train: 0.0602 loss_val: 0.0572\n",
      "Epoch: 3038 loss_train: 0.0598 loss_val: 0.0565\n",
      "Epoch: 3039 loss_train: 0.0596 loss_val: 0.0567\n",
      "Epoch: 3040 loss_train: 0.0604 loss_val: 0.0557\n",
      "Epoch: 3041 loss_train: 0.0592 loss_val: 0.0557\n",
      "Epoch: 3042 loss_train: 0.0606 loss_val: 0.0575\n",
      "Epoch: 3043 loss_train: 0.0585 loss_val: 0.0565\n",
      "Epoch: 3044 loss_train: 0.0602 loss_val: 0.0572\n",
      "Epoch: 3045 loss_train: 0.0595 loss_val: 0.0575\n",
      "Epoch: 3046 loss_train: 0.0593 loss_val: 0.0562\n",
      "Epoch: 3047 loss_train: 0.0604 loss_val: 0.0568\n",
      "Epoch: 3048 loss_train: 0.0598 loss_val: 0.0578\n",
      "Epoch: 3049 loss_train: 0.0593 loss_val: 0.0570\n",
      "Epoch: 3050 loss_train: 0.0605 loss_val: 0.0565\n",
      "Epoch: 3051 loss_train: 0.0608 loss_val: 0.0558\n",
      "Epoch: 3052 loss_train: 0.0601 loss_val: 0.0576\n",
      "Epoch: 3053 loss_train: 0.0605 loss_val: 0.0554\n",
      "Epoch: 3054 loss_train: 0.0593 loss_val: 0.0554\n",
      "Epoch: 3055 loss_train: 0.0589 loss_val: 0.0570\n",
      "Epoch: 3056 loss_train: 0.0600 loss_val: 0.0562\n",
      "Epoch: 3057 loss_train: 0.0585 loss_val: 0.0552\n",
      "Epoch: 3058 loss_train: 0.0597 loss_val: 0.0567\n",
      "Epoch: 3059 loss_train: 0.0595 loss_val: 0.0564\n",
      "Epoch: 3060 loss_train: 0.0587 loss_val: 0.0558\n",
      "Epoch: 3061 loss_train: 0.0588 loss_val: 0.0578\n",
      "Epoch: 3062 loss_train: 0.0601 loss_val: 0.0565\n",
      "Epoch: 3063 loss_train: 0.0592 loss_val: 0.0582\n",
      "Epoch: 3064 loss_train: 0.0590 loss_val: 0.0590\n",
      "Epoch: 3065 loss_train: 0.0590 loss_val: 0.0545\n",
      "Epoch: 3066 loss_train: 0.0607 loss_val: 0.0593\n",
      "Epoch: 3067 loss_train: 0.0604 loss_val: 0.0592\n",
      "Epoch: 3068 loss_train: 0.0593 loss_val: 0.0568\n",
      "Epoch: 3069 loss_train: 0.0598 loss_val: 0.0574\n",
      "Epoch: 3070 loss_train: 0.0591 loss_val: 0.0595\n",
      "Epoch: 3071 loss_train: 0.0579 loss_val: 0.0594\n",
      "Epoch: 3072 loss_train: 0.0599 loss_val: 0.0578\n",
      "Epoch: 3073 loss_train: 0.0599 loss_val: 0.0577\n",
      "Epoch: 3074 loss_train: 0.0594 loss_val: 0.0594\n",
      "Epoch: 3075 loss_train: 0.0595 loss_val: 0.0582\n",
      "Epoch: 3076 loss_train: 0.0584 loss_val: 0.0572\n",
      "Epoch: 3077 loss_train: 0.0588 loss_val: 0.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3078 loss_train: 0.0587 loss_val: 0.0571\n",
      "Epoch: 3079 loss_train: 0.0580 loss_val: 0.0588\n",
      "Epoch: 3080 loss_train: 0.0604 loss_val: 0.0570\n",
      "Epoch: 3081 loss_train: 0.0609 loss_val: 0.0594\n",
      "Epoch: 3082 loss_train: 0.0603 loss_val: 0.0598\n",
      "Epoch: 3083 loss_train: 0.0599 loss_val: 0.0590\n",
      "Epoch: 3084 loss_train: 0.0604 loss_val: 0.0580\n",
      "Epoch: 3085 loss_train: 0.0578 loss_val: 0.0567\n",
      "Epoch: 3086 loss_train: 0.0579 loss_val: 0.0562\n",
      "Epoch: 3087 loss_train: 0.0576 loss_val: 0.0576\n",
      "Epoch: 3088 loss_train: 0.0587 loss_val: 0.0573\n",
      "Epoch: 3089 loss_train: 0.0579 loss_val: 0.0577\n",
      "Epoch: 3090 loss_train: 0.0583 loss_val: 0.0578\n",
      "Epoch: 3091 loss_train: 0.0574 loss_val: 0.0565\n",
      "Epoch: 3092 loss_train: 0.0578 loss_val: 0.0579\n",
      "Epoch: 3093 loss_train: 0.0581 loss_val: 0.0554\n",
      "Epoch: 3094 loss_train: 0.0590 loss_val: 0.0574\n",
      "Epoch: 3095 loss_train: 0.0593 loss_val: 0.0562\n",
      "Epoch: 3096 loss_train: 0.0604 loss_val: 0.0576\n",
      "Epoch: 3097 loss_train: 0.0598 loss_val: 0.0559\n",
      "Epoch: 3098 loss_train: 0.0598 loss_val: 0.0559\n",
      "Epoch: 3099 loss_train: 0.0588 loss_val: 0.0534\n",
      "Epoch: 3100 loss_train: 0.0584 loss_val: 0.0545\n",
      "Epoch: 3101 loss_train: 0.0599 loss_val: 0.0566\n",
      "Epoch: 3102 loss_train: 0.0576 loss_val: 0.0555\n",
      "Epoch: 3103 loss_train: 0.0603 loss_val: 0.0589\n",
      "Epoch: 3104 loss_train: 0.0603 loss_val: 0.0563\n",
      "Epoch: 3105 loss_train: 0.0601 loss_val: 0.0576\n",
      "Epoch: 3106 loss_train: 0.0615 loss_val: 0.0564\n",
      "Epoch: 3107 loss_train: 0.0599 loss_val: 0.0566\n",
      "Epoch: 3108 loss_train: 0.0595 loss_val: 0.0547\n",
      "Epoch: 3109 loss_train: 0.0599 loss_val: 0.0545\n",
      "Epoch: 3110 loss_train: 0.0593 loss_val: 0.0565\n",
      "Epoch: 3111 loss_train: 0.0582 loss_val: 0.0544\n",
      "Epoch: 3112 loss_train: 0.0589 loss_val: 0.0530\n",
      "Epoch: 3113 loss_train: 0.0590 loss_val: 0.0535\n",
      "Epoch: 3114 loss_train: 0.0594 loss_val: 0.0552\n",
      "Epoch: 3115 loss_train: 0.0606 loss_val: 0.0571\n",
      "Epoch: 3116 loss_train: 0.0599 loss_val: 0.0553\n",
      "Epoch: 3117 loss_train: 0.0589 loss_val: 0.0560\n",
      "Epoch: 3118 loss_train: 0.0587 loss_val: 0.0567\n",
      "Epoch: 3119 loss_train: 0.0585 loss_val: 0.0573\n",
      "Epoch: 3120 loss_train: 0.0600 loss_val: 0.0571\n",
      "Epoch: 3121 loss_train: 0.0579 loss_val: 0.0564\n",
      "Epoch: 3122 loss_train: 0.0583 loss_val: 0.0567\n",
      "Epoch: 3123 loss_train: 0.0592 loss_val: 0.0556\n",
      "Epoch: 3124 loss_train: 0.0601 loss_val: 0.0543\n",
      "Epoch: 3125 loss_train: 0.0605 loss_val: 0.0569\n",
      "Epoch: 3126 loss_train: 0.0587 loss_val: 0.0558\n",
      "Epoch: 3127 loss_train: 0.0582 loss_val: 0.0568\n",
      "Epoch: 3128 loss_train: 0.0584 loss_val: 0.0559\n",
      "Epoch: 3129 loss_train: 0.0601 loss_val: 0.0567\n",
      "Epoch: 3130 loss_train: 0.0602 loss_val: 0.0579\n",
      "Epoch: 3131 loss_train: 0.0585 loss_val: 0.0565\n",
      "Epoch: 3132 loss_train: 0.0584 loss_val: 0.0585\n",
      "Epoch: 3133 loss_train: 0.0585 loss_val: 0.0569\n",
      "Epoch: 3134 loss_train: 0.0594 loss_val: 0.0552\n",
      "Epoch: 3135 loss_train: 0.0596 loss_val: 0.0563\n",
      "Epoch: 3136 loss_train: 0.0584 loss_val: 0.0555\n",
      "Epoch: 3137 loss_train: 0.0582 loss_val: 0.0541\n",
      "Epoch: 3138 loss_train: 0.0583 loss_val: 0.0555\n",
      "Epoch: 3139 loss_train: 0.0591 loss_val: 0.0553\n",
      "Epoch: 3140 loss_train: 0.0590 loss_val: 0.0552\n",
      "Epoch: 3141 loss_train: 0.0598 loss_val: 0.0569\n",
      "Epoch: 3142 loss_train: 0.0584 loss_val: 0.0560\n",
      "Epoch: 3143 loss_train: 0.0587 loss_val: 0.0550\n",
      "Epoch: 3144 loss_train: 0.0590 loss_val: 0.0530\n",
      "Epoch: 3145 loss_train: 0.0591 loss_val: 0.0555\n",
      "Epoch: 3146 loss_train: 0.0583 loss_val: 0.0562\n",
      "Epoch: 3147 loss_train: 0.0585 loss_val: 0.0554\n",
      "Epoch: 3148 loss_train: 0.0586 loss_val: 0.0565\n",
      "Epoch: 3149 loss_train: 0.0588 loss_val: 0.0559\n",
      "Epoch: 3150 loss_train: 0.0604 loss_val: 0.0561\n",
      "Epoch: 3151 loss_train: 0.0593 loss_val: 0.0566\n",
      "Epoch: 3152 loss_train: 0.0585 loss_val: 0.0563\n",
      "Epoch: 3153 loss_train: 0.0589 loss_val: 0.0566\n",
      "Epoch: 3154 loss_train: 0.0585 loss_val: 0.0551\n",
      "Epoch: 3155 loss_train: 0.0598 loss_val: 0.0539\n",
      "Epoch: 3156 loss_train: 0.0582 loss_val: 0.0571\n",
      "Epoch: 3157 loss_train: 0.0584 loss_val: 0.0547\n",
      "Epoch: 3463 loss_train: 0.0592 loss_val: 0.0554\n",
      "Epoch: 3464 loss_train: 0.0596 loss_val: 0.0552\n",
      "Epoch: 3465 loss_train: 0.0591 loss_val: 0.0526\n",
      "Epoch: 3466 loss_train: 0.0585 loss_val: 0.0568\n",
      "Epoch: 3467 loss_train: 0.0589 loss_val: 0.0575\n",
      "Epoch: 3468 loss_train: 0.0589 loss_val: 0.0559\n",
      "Epoch: 3469 loss_train: 0.0587 loss_val: 0.0551\n",
      "Epoch: 3470 loss_train: 0.0590 loss_val: 0.0574\n",
      "Epoch: 3471 loss_train: 0.0589 loss_val: 0.0582\n",
      "Epoch: 3472 loss_train: 0.0585 loss_val: 0.0545\n",
      "Epoch: 3473 loss_train: 0.0596 loss_val: 0.0548\n",
      "Epoch: 3474 loss_train: 0.0581 loss_val: 0.0572\n",
      "Epoch: 3475 loss_train: 0.0592 loss_val: 0.0537\n",
      "Epoch: 3476 loss_train: 0.0583 loss_val: 0.0556\n",
      "Epoch: 3477 loss_train: 0.0571 loss_val: 0.0544\n",
      "Epoch: 3478 loss_train: 0.0573 loss_val: 0.0552\n",
      "Epoch: 3479 loss_train: 0.0580 loss_val: 0.0535\n",
      "Epoch: 3480 loss_train: 0.0589 loss_val: 0.0560\n",
      "Epoch: 3481 loss_train: 0.0595 loss_val: 0.0541\n",
      "Epoch: 3482 loss_train: 0.0591 loss_val: 0.0558\n",
      "Epoch: 3483 loss_train: 0.0588 loss_val: 0.0549\n",
      "Epoch: 3484 loss_train: 0.0584 loss_val: 0.0582\n",
      "Epoch: 3485 loss_train: 0.0583 loss_val: 0.0547\n",
      "Epoch: 3486 loss_train: 0.0574 loss_val: 0.0536\n",
      "Epoch: 3487 loss_train: 0.0577 loss_val: 0.0542\n",
      "Epoch: 3488 loss_train: 0.0578 loss_val: 0.0526\n",
      "Epoch: 3489 loss_train: 0.0594 loss_val: 0.0551\n",
      "Epoch: 3490 loss_train: 0.0583 loss_val: 0.0564\n",
      "Epoch: 3491 loss_train: 0.0579 loss_val: 0.0580\n",
      "Epoch: 3492 loss_train: 0.0587 loss_val: 0.0549\n",
      "Epoch: 3493 loss_train: 0.0583 loss_val: 0.0562\n",
      "Epoch: 3494 loss_train: 0.0590 loss_val: 0.0548\n",
      "Epoch: 3495 loss_train: 0.0603 loss_val: 0.0568\n",
      "Epoch: 3496 loss_train: 0.0582 loss_val: 0.0572\n",
      "Epoch: 3497 loss_train: 0.0584 loss_val: 0.0548\n",
      "Epoch: 3498 loss_train: 0.0589 loss_val: 0.0549\n",
      "Epoch: 3499 loss_train: 0.0586 loss_val: 0.0567\n",
      "Epoch: 3500 loss_train: 0.0579 loss_val: 0.0570\n",
      "Epoch: 3501 loss_train: 0.0575 loss_val: 0.0555\n",
      "Epoch: 3502 loss_train: 0.0568 loss_val: 0.0563\n",
      "Epoch: 3503 loss_train: 0.0569 loss_val: 0.0583\n",
      "Epoch: 3504 loss_train: 0.0577 loss_val: 0.0557\n",
      "Epoch: 3505 loss_train: 0.0570 loss_val: 0.0553\n",
      "Epoch: 3506 loss_train: 0.0566 loss_val: 0.0560\n",
      "Epoch: 3507 loss_train: 0.0578 loss_val: 0.0542\n",
      "Epoch: 3508 loss_train: 0.0571 loss_val: 0.0563\n",
      "Epoch: 3509 loss_train: 0.0594 loss_val: 0.0590\n",
      "Epoch: 3510 loss_train: 0.0592 loss_val: 0.0545\n",
      "Epoch: 3511 loss_train: 0.0597 loss_val: 0.0551\n",
      "Epoch: 3512 loss_train: 0.0600 loss_val: 0.0586\n",
      "Epoch: 3513 loss_train: 0.0595 loss_val: 0.0571\n",
      "Epoch: 3514 loss_train: 0.0576 loss_val: 0.0572\n",
      "Epoch: 3515 loss_train: 0.0581 loss_val: 0.0561\n",
      "Epoch: 3516 loss_train: 0.0570 loss_val: 0.0581\n",
      "Epoch: 3517 loss_train: 0.0579 loss_val: 0.0564\n",
      "Epoch: 3518 loss_train: 0.0585 loss_val: 0.0582\n",
      "Epoch: 3519 loss_train: 0.0592 loss_val: 0.0562\n",
      "Epoch: 3520 loss_train: 0.0571 loss_val: 0.0574\n",
      "Epoch: 3521 loss_train: 0.0574 loss_val: 0.0569\n",
      "Epoch: 3522 loss_train: 0.0569 loss_val: 0.0566\n",
      "Epoch: 3523 loss_train: 0.0577 loss_val: 0.0568\n",
      "Epoch: 3524 loss_train: 0.0588 loss_val: 0.0583\n",
      "Epoch: 3525 loss_train: 0.0598 loss_val: 0.0583\n",
      "Epoch: 3526 loss_train: 0.0589 loss_val: 0.0552\n",
      "Epoch: 3527 loss_train: 0.0587 loss_val: 0.0561\n",
      "Epoch: 3528 loss_train: 0.0599 loss_val: 0.0559\n",
      "Epoch: 3529 loss_train: 0.0588 loss_val: 0.0548\n",
      "Epoch: 3530 loss_train: 0.0577 loss_val: 0.0552\n",
      "Epoch: 3531 loss_train: 0.0586 loss_val: 0.0542\n",
      "Epoch: 3532 loss_train: 0.0568 loss_val: 0.0547\n",
      "Epoch: 3533 loss_train: 0.0583 loss_val: 0.0540\n",
      "Epoch: 3534 loss_train: 0.0574 loss_val: 0.0537\n",
      "Epoch: 3535 loss_train: 0.0576 loss_val: 0.0551\n",
      "Epoch: 3536 loss_train: 0.0578 loss_val: 0.0581\n",
      "Epoch: 3537 loss_train: 0.0584 loss_val: 0.0558\n",
      "Epoch: 3538 loss_train: 0.0592 loss_val: 0.0593\n",
      "Epoch: 3539 loss_train: 0.0579 loss_val: 0.0560\n",
      "Epoch: 3540 loss_train: 0.0579 loss_val: 0.0559\n",
      "Epoch: 3541 loss_train: 0.0580 loss_val: 0.0562\n",
      "Epoch: 3542 loss_train: 0.0587 loss_val: 0.0579\n",
      "Epoch: 3543 loss_train: 0.0589 loss_val: 0.0569\n",
      "Epoch: 3544 loss_train: 0.0581 loss_val: 0.0569\n",
      "Epoch: 3545 loss_train: 0.0583 loss_val: 0.0548\n",
      "Epoch: 3546 loss_train: 0.0568 loss_val: 0.0542\n",
      "Epoch: 3547 loss_train: 0.0583 loss_val: 0.0568\n",
      "Epoch: 3548 loss_train: 0.0578 loss_val: 0.0575\n",
      "Epoch: 3549 loss_train: 0.0597 loss_val: 0.0559\n",
      "Epoch: 3550 loss_train: 0.0588 loss_val: 0.0561\n",
      "Epoch: 3551 loss_train: 0.0569 loss_val: 0.0544\n",
      "Epoch: 3552 loss_train: 0.0575 loss_val: 0.0561\n",
      "Epoch: 3553 loss_train: 0.0580 loss_val: 0.0548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3554 loss_train: 0.0569 loss_val: 0.0569\n",
      "Epoch: 3555 loss_train: 0.0588 loss_val: 0.0572\n",
      "Epoch: 3556 loss_train: 0.0585 loss_val: 0.0570\n",
      "Epoch: 3557 loss_train: 0.0584 loss_val: 0.0576\n",
      "Epoch: 3558 loss_train: 0.0592 loss_val: 0.0570\n",
      "Epoch: 3559 loss_train: 0.0575 loss_val: 0.0574\n",
      "Epoch: 3560 loss_train: 0.0579 loss_val: 0.0594\n",
      "Epoch: 3561 loss_train: 0.0579 loss_val: 0.0566\n",
      "Epoch: 3562 loss_train: 0.0579 loss_val: 0.0556\n",
      "Epoch: 3563 loss_train: 0.0583 loss_val: 0.0536\n",
      "Epoch: 3564 loss_train: 0.0590 loss_val: 0.0557\n",
      "Epoch: 3565 loss_train: 0.0584 loss_val: 0.0587\n",
      "Epoch: 3566 loss_train: 0.0593 loss_val: 0.0555\n",
      "Epoch: 3567 loss_train: 0.0582 loss_val: 0.0555\n",
      "Epoch: 3568 loss_train: 0.0570 loss_val: 0.0543\n",
      "Epoch: 3569 loss_train: 0.0560 loss_val: 0.0553\n",
      "Epoch: 3570 loss_train: 0.0569 loss_val: 0.0579\n",
      "Epoch: 3571 loss_train: 0.0587 loss_val: 0.0563\n",
      "Epoch: 3572 loss_train: 0.0583 loss_val: 0.0558\n",
      "Epoch: 3573 loss_train: 0.0584 loss_val: 0.0555\n",
      "Epoch: 3574 loss_train: 0.0577 loss_val: 0.0544\n",
      "Epoch: 3575 loss_train: 0.0577 loss_val: 0.0548\n",
      "Epoch: 3576 loss_train: 0.0579 loss_val: 0.0559\n",
      "Epoch: 3577 loss_train: 0.0567 loss_val: 0.0547\n",
      "Epoch: 3578 loss_train: 0.0569 loss_val: 0.0559\n",
      "Epoch: 3579 loss_train: 0.0571 loss_val: 0.0564\n",
      "Epoch: 3580 loss_train: 0.0571 loss_val: 0.0559\n",
      "Epoch: 3581 loss_train: 0.0559 loss_val: 0.0542\n",
      "Epoch: 3582 loss_train: 0.0573 loss_val: 0.0532\n",
      "Epoch: 3583 loss_train: 0.0573 loss_val: 0.0566\n",
      "Epoch: 3584 loss_train: 0.0573 loss_val: 0.0552\n",
      "Epoch: 3585 loss_train: 0.0577 loss_val: 0.0524\n",
      "Epoch: 3586 loss_train: 0.0580 loss_val: 0.0544\n",
      "Epoch: 3587 loss_train: 0.0572 loss_val: 0.0544\n",
      "Epoch: 3588 loss_train: 0.0571 loss_val: 0.0540\n",
      "Epoch: 3589 loss_train: 0.0588 loss_val: 0.0530\n",
      "Epoch: 3590 loss_train: 0.0576 loss_val: 0.0547\n",
      "Epoch: 3591 loss_train: 0.0578 loss_val: 0.0534\n",
      "Epoch: 3592 loss_train: 0.0579 loss_val: 0.0560\n",
      "Epoch: 3593 loss_train: 0.0593 loss_val: 0.0553\n",
      "Epoch: 3594 loss_train: 0.0584 loss_val: 0.0528\n",
      "Epoch: 3595 loss_train: 0.0573 loss_val: 0.0549\n",
      "Epoch: 3596 loss_train: 0.0581 loss_val: 0.0540\n",
      "Epoch: 3597 loss_train: 0.0576 loss_val: 0.0546\n",
      "Epoch: 3598 loss_train: 0.0575 loss_val: 0.0547\n",
      "Epoch: 3599 loss_train: 0.0589 loss_val: 0.0537\n",
      "Epoch: 3600 loss_train: 0.0580 loss_val: 0.0560\n",
      "Epoch: 3601 loss_train: 0.0578 loss_val: 0.0523\n",
      "Epoch: 3602 loss_train: 0.0578 loss_val: 0.0554\n",
      "Epoch: 3603 loss_train: 0.0574 loss_val: 0.0545\n",
      "Epoch: 3604 loss_train: 0.0585 loss_val: 0.0554\n",
      "Epoch: 3605 loss_train: 0.0571 loss_val: 0.0562\n",
      "Epoch: 3606 loss_train: 0.0577 loss_val: 0.0555\n",
      "Epoch: 3607 loss_train: 0.0583 loss_val: 0.0535\n",
      "Epoch: 3608 loss_train: 0.0573 loss_val: 0.0535\n",
      "Epoch: 3609 loss_train: 0.0579 loss_val: 0.0559\n",
      "Epoch: 3610 loss_train: 0.0579 loss_val: 0.0555\n",
      "Epoch: 3611 loss_train: 0.0581 loss_val: 0.0563\n",
      "Epoch: 3612 loss_train: 0.0590 loss_val: 0.0552\n",
      "Epoch: 3613 loss_train: 0.0587 loss_val: 0.0555\n",
      "Epoch: 3614 loss_train: 0.0579 loss_val: 0.0536\n",
      "Epoch: 3615 loss_train: 0.0597 loss_val: 0.0539\n",
      "Epoch: 3616 loss_train: 0.0595 loss_val: 0.0550\n",
      "Epoch: 3617 loss_train: 0.0572 loss_val: 0.0536\n",
      "Epoch: 3618 loss_train: 0.0570 loss_val: 0.0544\n",
      "Epoch: 3619 loss_train: 0.0574 loss_val: 0.0541\n",
      "Epoch: 3620 loss_train: 0.0574 loss_val: 0.0535\n",
      "Epoch: 3621 loss_train: 0.0581 loss_val: 0.0553\n",
      "Epoch: 3622 loss_train: 0.0593 loss_val: 0.0553\n",
      "Epoch: 3623 loss_train: 0.0583 loss_val: 0.0534\n",
      "Epoch: 3624 loss_train: 0.0581 loss_val: 0.0552\n",
      "Epoch: 3625 loss_train: 0.0564 loss_val: 0.0534\n",
      "Epoch: 3626 loss_train: 0.0581 loss_val: 0.0545\n",
      "Epoch: 3627 loss_train: 0.0602 loss_val: 0.0536\n",
      "Epoch: 3628 loss_train: 0.0605 loss_val: 0.0532\n",
      "Epoch: 3629 loss_train: 0.0591 loss_val: 0.0543\n",
      "Epoch: 3630 loss_train: 0.0603 loss_val: 0.0548\n",
      "Epoch: 3631 loss_train: 0.0608 loss_val: 0.0536\n",
      "Epoch: 3632 loss_train: 0.0594 loss_val: 0.0556\n",
      "Epoch: 3633 loss_train: 0.0584 loss_val: 0.0538\n",
      "Epoch: 3634 loss_train: 0.0575 loss_val: 0.0534\n",
      "Epoch: 3635 loss_train: 0.0571 loss_val: 0.0541\n",
      "Epoch: 3636 loss_train: 0.0584 loss_val: 0.0547\n",
      "Epoch: 3637 loss_train: 0.0592 loss_val: 0.0556\n",
      "Epoch: 3638 loss_train: 0.0579 loss_val: 0.0552\n",
      "Epoch: 3639 loss_train: 0.0594 loss_val: 0.0547\n",
      "Epoch: 3640 loss_train: 0.0579 loss_val: 0.0527\n",
      "Epoch: 3641 loss_train: 0.0585 loss_val: 0.0560\n",
      "Epoch: 3642 loss_train: 0.0578 loss_val: 0.0529\n",
      "Epoch: 3643 loss_train: 0.0583 loss_val: 0.0564\n",
      "Epoch: 3644 loss_train: 0.0565 loss_val: 0.0538\n",
      "Epoch: 3645 loss_train: 0.0567 loss_val: 0.0537\n",
      "Epoch: 3646 loss_train: 0.0578 loss_val: 0.0548\n",
      "Epoch: 3647 loss_train: 0.0575 loss_val: 0.0531\n",
      "Epoch: 3648 loss_train: 0.0575 loss_val: 0.0541\n",
      "Epoch: 3649 loss_train: 0.0582 loss_val: 0.0545\n",
      "Epoch: 3650 loss_train: 0.0578 loss_val: 0.0571\n",
      "Epoch: 3651 loss_train: 0.0569 loss_val: 0.0556\n",
      "Epoch: 3652 loss_train: 0.0580 loss_val: 0.0550\n",
      "Epoch: 3653 loss_train: 0.0586 loss_val: 0.0533\n",
      "Epoch: 3654 loss_train: 0.0579 loss_val: 0.0544\n",
      "Epoch: 3655 loss_train: 0.0586 loss_val: 0.0550\n",
      "Epoch: 3656 loss_train: 0.0577 loss_val: 0.0543\n",
      "Epoch: 3657 loss_train: 0.0580 loss_val: 0.0540\n",
      "Epoch: 3658 loss_train: 0.0585 loss_val: 0.0527\n",
      "Epoch: 3659 loss_train: 0.0576 loss_val: 0.0538\n",
      "Epoch: 3660 loss_train: 0.0573 loss_val: 0.0546\n",
      "Epoch: 3661 loss_train: 0.0582 loss_val: 0.0534\n",
      "Epoch: 3662 loss_train: 0.0571 loss_val: 0.0536\n",
      "Epoch: 3663 loss_train: 0.0584 loss_val: 0.0528\n",
      "Epoch: 3664 loss_train: 0.0579 loss_val: 0.0554\n",
      "Epoch: 3665 loss_train: 0.0568 loss_val: 0.0545\n",
      "Epoch: 3666 loss_train: 0.0579 loss_val: 0.0543\n",
      "Epoch: 3667 loss_train: 0.0571 loss_val: 0.0555\n",
      "Epoch: 3668 loss_train: 0.0593 loss_val: 0.0545\n",
      "Epoch: 3669 loss_train: 0.0570 loss_val: 0.0561\n",
      "Epoch: 3670 loss_train: 0.0596 loss_val: 0.0560\n",
      "Epoch: 3671 loss_train: 0.0586 loss_val: 0.0565\n",
      "Epoch: 3672 loss_train: 0.0580 loss_val: 0.0546\n",
      "Epoch: 3673 loss_train: 0.0573 loss_val: 0.0549\n",
      "Epoch: 3674 loss_train: 0.0594 loss_val: 0.0575\n",
      "Epoch: 3675 loss_train: 0.0593 loss_val: 0.0560\n",
      "Epoch: 3676 loss_train: 0.0574 loss_val: 0.0564\n",
      "Epoch: 3677 loss_train: 0.0580 loss_val: 0.0556\n",
      "Epoch: 3678 loss_train: 0.0580 loss_val: 0.0560\n",
      "Epoch: 3679 loss_train: 0.0567 loss_val: 0.0556\n",
      "Epoch: 3680 loss_train: 0.0575 loss_val: 0.0550\n",
      "Epoch: 3681 loss_train: 0.0583 loss_val: 0.0558\n",
      "Epoch: 3682 loss_train: 0.0586 loss_val: 0.0585\n",
      "Epoch: 3683 loss_train: 0.0591 loss_val: 0.0558\n",
      "Epoch: 3684 loss_train: 0.0574 loss_val: 0.0536\n",
      "Epoch: 3685 loss_train: 0.0581 loss_val: 0.0548\n",
      "Epoch: 3686 loss_train: 0.0581 loss_val: 0.0578\n",
      "Epoch: 3687 loss_train: 0.0583 loss_val: 0.0540\n",
      "Epoch: 3688 loss_train: 0.0602 loss_val: 0.0549\n",
      "Epoch: 3689 loss_train: 0.0574 loss_val: 0.0554\n",
      "Epoch: 3690 loss_train: 0.0586 loss_val: 0.0539\n",
      "Epoch: 3691 loss_train: 0.0573 loss_val: 0.0554\n",
      "Epoch: 3692 loss_train: 0.0591 loss_val: 0.0560\n",
      "Epoch: 3693 loss_train: 0.0588 loss_val: 0.0542\n",
      "Epoch: 3694 loss_train: 0.0599 loss_val: 0.0554\n",
      "Epoch: 3695 loss_train: 0.0575 loss_val: 0.0574\n",
      "Epoch: 3696 loss_train: 0.0579 loss_val: 0.0538\n",
      "Epoch: 3697 loss_train: 0.0581 loss_val: 0.0554\n",
      "Epoch: 3698 loss_train: 0.0564 loss_val: 0.0540\n",
      "Epoch: 3699 loss_train: 0.0572 loss_val: 0.0560\n",
      "Epoch: 3700 loss_train: 0.0570 loss_val: 0.0548\n",
      "Epoch: 3701 loss_train: 0.0590 loss_val: 0.0556\n",
      "Epoch: 3702 loss_train: 0.0587 loss_val: 0.0560\n",
      "Epoch: 3703 loss_train: 0.0574 loss_val: 0.0568\n",
      "Epoch: 3704 loss_train: 0.0574 loss_val: 0.0554\n",
      "Epoch: 3705 loss_train: 0.0583 loss_val: 0.0551\n",
      "Epoch: 3706 loss_train: 0.0580 loss_val: 0.0557\n",
      "Epoch: 3707 loss_train: 0.0575 loss_val: 0.0546\n",
      "Epoch: 3708 loss_train: 0.0574 loss_val: 0.0533\n",
      "Epoch: 3709 loss_train: 0.0576 loss_val: 0.0532\n",
      "Epoch: 3710 loss_train: 0.0574 loss_val: 0.0541\n",
      "Epoch: 3711 loss_train: 0.0575 loss_val: 0.0550\n",
      "Epoch: 3712 loss_train: 0.0590 loss_val: 0.0536\n",
      "Epoch: 3713 loss_train: 0.0570 loss_val: 0.0544\n",
      "Epoch: 3714 loss_train: 0.0576 loss_val: 0.0561\n",
      "Epoch: 3715 loss_train: 0.0572 loss_val: 0.0566\n",
      "Epoch: 3716 loss_train: 0.0582 loss_val: 0.0558\n",
      "Epoch: 3717 loss_train: 0.0576 loss_val: 0.0554\n",
      "Epoch: 3718 loss_train: 0.0568 loss_val: 0.0561\n",
      "Epoch: 3719 loss_train: 0.0575 loss_val: 0.0536\n",
      "Epoch: 3720 loss_train: 0.0580 loss_val: 0.0546\n",
      "Epoch: 3721 loss_train: 0.0577 loss_val: 0.0551\n",
      "Epoch: 3722 loss_train: 0.0571 loss_val: 0.0554\n",
      "Epoch: 3723 loss_train: 0.0585 loss_val: 0.0548\n",
      "Epoch: 3724 loss_train: 0.0574 loss_val: 0.0524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3725 loss_train: 0.0589 loss_val: 0.0537\n",
      "Epoch: 3726 loss_train: 0.0581 loss_val: 0.0549\n",
      "Epoch: 3727 loss_train: 0.0585 loss_val: 0.0542\n",
      "Epoch: 3728 loss_train: 0.0576 loss_val: 0.0556\n",
      "Epoch: 3729 loss_train: 0.0573 loss_val: 0.0549\n",
      "Epoch: 3730 loss_train: 0.0588 loss_val: 0.0526\n",
      "Epoch: 3731 loss_train: 0.0581 loss_val: 0.0555\n",
      "Epoch: 3732 loss_train: 0.0570 loss_val: 0.0531\n",
      "Epoch: 3733 loss_train: 0.0578 loss_val: 0.0562\n",
      "Epoch: 3734 loss_train: 0.0579 loss_val: 0.0531\n",
      "Epoch: 3735 loss_train: 0.0555 loss_val: 0.0546\n",
      "Epoch: 3736 loss_train: 0.0562 loss_val: 0.0536\n",
      "Epoch: 3737 loss_train: 0.0578 loss_val: 0.0555\n",
      "Epoch: 3738 loss_train: 0.0576 loss_val: 0.0555\n",
      "Epoch: 3739 loss_train: 0.0577 loss_val: 0.0547\n",
      "Epoch: 3740 loss_train: 0.0571 loss_val: 0.0555\n",
      "Epoch: 3741 loss_train: 0.0594 loss_val: 0.0563\n",
      "Epoch: 3742 loss_train: 0.0578 loss_val: 0.0570\n",
      "Epoch: 3743 loss_train: 0.0580 loss_val: 0.0558\n",
      "Epoch: 3744 loss_train: 0.0577 loss_val: 0.0546\n",
      "Epoch: 3745 loss_train: 0.0565 loss_val: 0.0566\n",
      "Epoch: 3746 loss_train: 0.0570 loss_val: 0.0569\n",
      "Epoch: 3747 loss_train: 0.0574 loss_val: 0.0562\n",
      "Epoch: 3748 loss_train: 0.0563 loss_val: 0.0555\n",
      "Epoch: 3749 loss_train: 0.0574 loss_val: 0.0559\n",
      "Epoch: 3750 loss_train: 0.0571 loss_val: 0.0568\n",
      "Epoch: 3751 loss_train: 0.0587 loss_val: 0.0553\n",
      "Epoch: 3752 loss_train: 0.0572 loss_val: 0.0561\n",
      "Epoch: 3753 loss_train: 0.0575 loss_val: 0.0557\n",
      "Epoch: 3754 loss_train: 0.0584 loss_val: 0.0550\n",
      "Epoch: 3755 loss_train: 0.0576 loss_val: 0.0538\n",
      "Epoch: 3756 loss_train: 0.0589 loss_val: 0.0561\n",
      "Epoch: 3757 loss_train: 0.0580 loss_val: 0.0548\n",
      "Epoch: 3758 loss_train: 0.0563 loss_val: 0.0574\n",
      "Epoch: 3759 loss_train: 0.0585 loss_val: 0.0560\n",
      "Epoch: 3760 loss_train: 0.0577 loss_val: 0.0562\n",
      "Epoch: 3761 loss_train: 0.0582 loss_val: 0.0538\n",
      "Epoch: 3762 loss_train: 0.0576 loss_val: 0.0550\n",
      "Epoch: 3763 loss_train: 0.0579 loss_val: 0.0575\n",
      "Epoch: 3764 loss_train: 0.0573 loss_val: 0.0580\n",
      "Epoch: 3765 loss_train: 0.0573 loss_val: 0.0549\n",
      "Epoch: 3766 loss_train: 0.0579 loss_val: 0.0575\n",
      "Epoch: 3767 loss_train: 0.0579 loss_val: 0.0574\n",
      "Epoch: 3768 loss_train: 0.0578 loss_val: 0.0572\n",
      "Epoch: 3769 loss_train: 0.0581 loss_val: 0.0566\n",
      "Epoch: 3770 loss_train: 0.0579 loss_val: 0.0557\n",
      "Epoch: 3771 loss_train: 0.0581 loss_val: 0.0561\n",
      "Epoch: 3772 loss_train: 0.0589 loss_val: 0.0563\n",
      "Epoch: 3773 loss_train: 0.0574 loss_val: 0.0556\n",
      "Epoch: 3774 loss_train: 0.0563 loss_val: 0.0573\n",
      "Epoch: 3775 loss_train: 0.0580 loss_val: 0.0551\n",
      "Epoch: 3776 loss_train: 0.0580 loss_val: 0.0552\n",
      "Epoch: 3777 loss_train: 0.0576 loss_val: 0.0546\n",
      "Epoch: 3778 loss_train: 0.0589 loss_val: 0.0550\n",
      "Epoch: 3779 loss_train: 0.0566 loss_val: 0.0525\n",
      "Epoch: 3780 loss_train: 0.0565 loss_val: 0.0550\n",
      "Epoch: 3781 loss_train: 0.0578 loss_val: 0.0562\n",
      "Epoch: 3782 loss_train: 0.0589 loss_val: 0.0550\n",
      "Epoch: 3783 loss_train: 0.0585 loss_val: 0.0546\n",
      "Epoch: 3784 loss_train: 0.0571 loss_val: 0.0566\n",
      "Epoch: 3785 loss_train: 0.0579 loss_val: 0.0551\n",
      "Epoch: 3786 loss_train: 0.0568 loss_val: 0.0544\n",
      "Epoch: 3787 loss_train: 0.0575 loss_val: 0.0547\n",
      "Epoch: 3788 loss_train: 0.0570 loss_val: 0.0569\n",
      "Epoch: 3789 loss_train: 0.0578 loss_val: 0.0569\n",
      "Epoch: 3790 loss_train: 0.0578 loss_val: 0.0548\n",
      "Epoch: 3791 loss_train: 0.0573 loss_val: 0.0542\n",
      "Epoch: 3792 loss_train: 0.0571 loss_val: 0.0551\n",
      "Epoch: 3793 loss_train: 0.0570 loss_val: 0.0551\n",
      "Epoch: 3794 loss_train: 0.0569 loss_val: 0.0572\n",
      "Epoch: 3795 loss_train: 0.0579 loss_val: 0.0560\n",
      "Epoch: 3796 loss_train: 0.0564 loss_val: 0.0539\n",
      "Epoch: 3797 loss_train: 0.0576 loss_val: 0.0558\n",
      "Epoch: 3798 loss_train: 0.0571 loss_val: 0.0528\n",
      "Epoch: 3799 loss_train: 0.0579 loss_val: 0.0555\n",
      "Epoch: 3800 loss_train: 0.0586 loss_val: 0.0554\n",
      "Epoch: 3801 loss_train: 0.0579 loss_val: 0.0558\n",
      "Epoch: 3802 loss_train: 0.0582 loss_val: 0.0540\n",
      "Epoch: 3803 loss_train: 0.0576 loss_val: 0.0543\n",
      "Epoch: 3804 loss_train: 0.0586 loss_val: 0.0532\n",
      "Epoch: 3805 loss_train: 0.0570 loss_val: 0.0539\n",
      "Epoch: 3806 loss_train: 0.0573 loss_val: 0.0555\n",
      "Epoch: 3807 loss_train: 0.0574 loss_val: 0.0558\n",
      "Epoch: 3808 loss_train: 0.0590 loss_val: 0.0564\n",
      "Epoch: 3809 loss_train: 0.0581 loss_val: 0.0548\n",
      "Epoch: 3810 loss_train: 0.0581 loss_val: 0.0565\n",
      "Epoch: 3811 loss_train: 0.0577 loss_val: 0.0555\n",
      "Epoch: 3812 loss_train: 0.0577 loss_val: 0.0550\n",
      "Epoch: 3813 loss_train: 0.0585 loss_val: 0.0560\n",
      "Epoch: 3814 loss_train: 0.0568 loss_val: 0.0544\n",
      "Epoch: 3815 loss_train: 0.0575 loss_val: 0.0583\n",
      "Epoch: 3816 loss_train: 0.0585 loss_val: 0.0568\n",
      "Epoch: 3817 loss_train: 0.0588 loss_val: 0.0549\n",
      "Epoch: 3818 loss_train: 0.0592 loss_val: 0.0543\n",
      "Epoch: 3819 loss_train: 0.0578 loss_val: 0.0557\n",
      "Epoch: 3820 loss_train: 0.0577 loss_val: 0.0556\n",
      "Epoch: 3821 loss_train: 0.0570 loss_val: 0.0546\n",
      "Epoch: 3822 loss_train: 0.0575 loss_val: 0.0545\n",
      "Epoch: 3823 loss_train: 0.0576 loss_val: 0.0533\n",
      "Epoch: 3824 loss_train: 0.0554 loss_val: 0.0560\n",
      "Epoch: 3825 loss_train: 0.0562 loss_val: 0.0554\n",
      "Epoch: 3826 loss_train: 0.0590 loss_val: 0.0553\n",
      "Epoch: 3827 loss_train: 0.0566 loss_val: 0.0571\n",
      "Epoch: 3828 loss_train: 0.0566 loss_val: 0.0552\n",
      "Epoch: 3829 loss_train: 0.0580 loss_val: 0.0542\n",
      "Epoch: 3830 loss_train: 0.0595 loss_val: 0.0557\n",
      "Epoch: 3831 loss_train: 0.0581 loss_val: 0.0548\n",
      "Epoch: 3832 loss_train: 0.0583 loss_val: 0.0546\n",
      "Epoch: 3833 loss_train: 0.0582 loss_val: 0.0547\n",
      "Epoch: 3834 loss_train: 0.0577 loss_val: 0.0559\n",
      "Epoch: 3835 loss_train: 0.0576 loss_val: 0.0544\n",
      "Epoch: 3836 loss_train: 0.0583 loss_val: 0.0554\n",
      "Epoch: 3837 loss_train: 0.0565 loss_val: 0.0529\n",
      "Epoch: 3838 loss_train: 0.0587 loss_val: 0.0544\n",
      "Epoch: 3839 loss_train: 0.0584 loss_val: 0.0536\n",
      "Epoch: 3840 loss_train: 0.0578 loss_val: 0.0526\n",
      "Epoch: 3841 loss_train: 0.0566 loss_val: 0.0538\n",
      "Epoch: 3842 loss_train: 0.0595 loss_val: 0.0543\n",
      "Epoch: 3843 loss_train: 0.0584 loss_val: 0.0550\n",
      "Epoch: 3844 loss_train: 0.0572 loss_val: 0.0521\n",
      "Epoch: 3845 loss_train: 0.0575 loss_val: 0.0553\n",
      "Epoch: 3846 loss_train: 0.0586 loss_val: 0.0527\n",
      "Epoch: 3847 loss_train: 0.0572 loss_val: 0.0546\n",
      "Epoch: 3848 loss_train: 0.0582 loss_val: 0.0561\n",
      "Epoch: 3849 loss_train: 0.0585 loss_val: 0.0551\n",
      "Epoch: 3850 loss_train: 0.0591 loss_val: 0.0524\n",
      "Epoch: 3851 loss_train: 0.0580 loss_val: 0.0549\n",
      "Epoch: 3852 loss_train: 0.0584 loss_val: 0.0549\n",
      "Epoch: 3853 loss_train: 0.0578 loss_val: 0.0534\n",
      "Epoch: 3854 loss_train: 0.0586 loss_val: 0.0511\n",
      "Epoch: 3855 loss_train: 0.0580 loss_val: 0.0532\n",
      "Epoch: 3856 loss_train: 0.0584 loss_val: 0.0538\n",
      "Epoch: 3857 loss_train: 0.0580 loss_val: 0.0545\n",
      "Epoch: 3858 loss_train: 0.0572 loss_val: 0.0549\n",
      "Epoch: 3859 loss_train: 0.0570 loss_val: 0.0557\n",
      "Epoch: 3860 loss_train: 0.0577 loss_val: 0.0525\n",
      "Epoch: 3861 loss_train: 0.0575 loss_val: 0.0514\n",
      "Epoch: 3862 loss_train: 0.0573 loss_val: 0.0516\n",
      "Epoch: 3863 loss_train: 0.0566 loss_val: 0.0522\n",
      "Epoch: 3864 loss_train: 0.0580 loss_val: 0.0544\n",
      "Epoch: 3865 loss_train: 0.0574 loss_val: 0.0553\n",
      "Epoch: 3866 loss_train: 0.0570 loss_val: 0.0552\n",
      "Epoch: 3867 loss_train: 0.0566 loss_val: 0.0550\n",
      "Epoch: 3868 loss_train: 0.0575 loss_val: 0.0545\n",
      "Epoch: 3869 loss_train: 0.0573 loss_val: 0.0543\n",
      "Epoch: 3870 loss_train: 0.0570 loss_val: 0.0549\n",
      "Epoch: 3871 loss_train: 0.0583 loss_val: 0.0552\n",
      "Epoch: 3872 loss_train: 0.0586 loss_val: 0.0555\n",
      "Epoch: 3873 loss_train: 0.0563 loss_val: 0.0535\n",
      "Epoch: 3874 loss_train: 0.0569 loss_val: 0.0540\n",
      "Epoch: 3875 loss_train: 0.0578 loss_val: 0.0541\n",
      "Epoch: 3876 loss_train: 0.0584 loss_val: 0.0550\n",
      "Epoch: 3877 loss_train: 0.0580 loss_val: 0.0544\n",
      "Epoch: 3878 loss_train: 0.0576 loss_val: 0.0542\n",
      "Epoch: 3879 loss_train: 0.0578 loss_val: 0.0557\n",
      "Epoch: 3880 loss_train: 0.0575 loss_val: 0.0550\n",
      "Epoch: 3881 loss_train: 0.0572 loss_val: 0.0533\n",
      "Epoch: 3882 loss_train: 0.0561 loss_val: 0.0528\n",
      "Epoch: 3883 loss_train: 0.0568 loss_val: 0.0534\n",
      "Epoch: 3884 loss_train: 0.0583 loss_val: 0.0519\n",
      "Epoch: 3885 loss_train: 0.0565 loss_val: 0.0526\n",
      "Epoch: 3886 loss_train: 0.0566 loss_val: 0.0543\n",
      "Epoch: 3887 loss_train: 0.0592 loss_val: 0.0567\n",
      "Epoch: 3888 loss_train: 0.0573 loss_val: 0.0547\n",
      "Epoch: 3889 loss_train: 0.0570 loss_val: 0.0549\n",
      "Epoch: 3890 loss_train: 0.0573 loss_val: 0.0531\n",
      "Epoch: 3891 loss_train: 0.0560 loss_val: 0.0550\n",
      "Epoch: 3892 loss_train: 0.0569 loss_val: 0.0548\n",
      "Epoch: 3893 loss_train: 0.0578 loss_val: 0.0552\n",
      "Epoch: 3894 loss_train: 0.0583 loss_val: 0.0545\n",
      "Epoch: 3895 loss_train: 0.0569 loss_val: 0.0541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3896 loss_train: 0.0569 loss_val: 0.0546\n",
      "Epoch: 3897 loss_train: 0.0574 loss_val: 0.0551\n",
      "Epoch: 3898 loss_train: 0.0583 loss_val: 0.0545\n",
      "Epoch: 3899 loss_train: 0.0589 loss_val: 0.0558\n",
      "Epoch: 3900 loss_train: 0.0564 loss_val: 0.0535\n",
      "Epoch: 3901 loss_train: 0.0568 loss_val: 0.0545\n",
      "Epoch: 3902 loss_train: 0.0567 loss_val: 0.0552\n",
      "Epoch: 3903 loss_train: 0.0560 loss_val: 0.0533\n",
      "Epoch: 3904 loss_train: 0.0570 loss_val: 0.0541\n",
      "Epoch: 3905 loss_train: 0.0565 loss_val: 0.0531\n",
      "Epoch: 3906 loss_train: 0.0571 loss_val: 0.0550\n",
      "Epoch: 3907 loss_train: 0.0583 loss_val: 0.0545\n",
      "Epoch: 3908 loss_train: 0.0578 loss_val: 0.0539\n",
      "Epoch: 3909 loss_train: 0.0563 loss_val: 0.0537\n",
      "Epoch: 3910 loss_train: 0.0567 loss_val: 0.0538\n",
      "Epoch: 3911 loss_train: 0.0574 loss_val: 0.0549\n",
      "Epoch: 3912 loss_train: 0.0579 loss_val: 0.0567\n",
      "Epoch: 3913 loss_train: 0.0565 loss_val: 0.0557\n",
      "Epoch: 3914 loss_train: 0.0578 loss_val: 0.0562\n",
      "Epoch: 3915 loss_train: 0.0578 loss_val: 0.0550\n",
      "Epoch: 3916 loss_train: 0.0563 loss_val: 0.0558\n",
      "Epoch: 3917 loss_train: 0.0582 loss_val: 0.0567\n",
      "Epoch: 3918 loss_train: 0.0572 loss_val: 0.0526\n",
      "Epoch: 3919 loss_train: 0.0569 loss_val: 0.0534\n",
      "Epoch: 3920 loss_train: 0.0577 loss_val: 0.0532\n",
      "Epoch: 3921 loss_train: 0.0573 loss_val: 0.0547\n",
      "Epoch: 3922 loss_train: 0.0565 loss_val: 0.0540\n",
      "Epoch: 3923 loss_train: 0.0572 loss_val: 0.0547\n",
      "Epoch: 3924 loss_train: 0.0567 loss_val: 0.0539\n",
      "Epoch: 3925 loss_train: 0.0589 loss_val: 0.0549\n",
      "Epoch: 3926 loss_train: 0.0577 loss_val: 0.0551\n",
      "Epoch: 3927 loss_train: 0.0579 loss_val: 0.0544\n",
      "Epoch: 3928 loss_train: 0.0568 loss_val: 0.0541\n",
      "Epoch: 3929 loss_train: 0.0573 loss_val: 0.0547\n",
      "Epoch: 3930 loss_train: 0.0579 loss_val: 0.0544\n",
      "Epoch: 3931 loss_train: 0.0578 loss_val: 0.0582\n",
      "Epoch: 3932 loss_train: 0.0574 loss_val: 0.0562\n",
      "Epoch: 3933 loss_train: 0.0572 loss_val: 0.0537\n",
      "Epoch: 3934 loss_train: 0.0553 loss_val: 0.0546\n",
      "Epoch: 3935 loss_train: 0.0581 loss_val: 0.0554\n",
      "Epoch: 3936 loss_train: 0.0574 loss_val: 0.0528\n",
      "Epoch: 3937 loss_train: 0.0565 loss_val: 0.0528\n",
      "Epoch: 3938 loss_train: 0.0578 loss_val: 0.0527\n",
      "Epoch: 3939 loss_train: 0.0574 loss_val: 0.0538\n",
      "Epoch: 3940 loss_train: 0.0566 loss_val: 0.0544\n",
      "Epoch: 3941 loss_train: 0.0568 loss_val: 0.0543\n",
      "Epoch: 3942 loss_train: 0.0572 loss_val: 0.0554\n",
      "Epoch: 3943 loss_train: 0.0579 loss_val: 0.0538\n",
      "Epoch: 3944 loss_train: 0.0568 loss_val: 0.0531\n",
      "Epoch: 3945 loss_train: 0.0570 loss_val: 0.0551\n",
      "Epoch: 3946 loss_train: 0.0567 loss_val: 0.0549\n",
      "Epoch: 3947 loss_train: 0.0564 loss_val: 0.0534\n",
      "Epoch: 3948 loss_train: 0.0570 loss_val: 0.0545\n",
      "Epoch: 3949 loss_train: 0.0570 loss_val: 0.0534\n",
      "Epoch: 3950 loss_train: 0.0576 loss_val: 0.0542\n",
      "Epoch: 3951 loss_train: 0.0569 loss_val: 0.0543\n",
      "Epoch: 3952 loss_train: 0.0551 loss_val: 0.0554\n",
      "Epoch: 3953 loss_train: 0.0562 loss_val: 0.0535\n",
      "Epoch: 3954 loss_train: 0.0564 loss_val: 0.0559\n",
      "Epoch: 3955 loss_train: 0.0567 loss_val: 0.0538\n",
      "Epoch: 3956 loss_train: 0.0579 loss_val: 0.0556\n",
      "Epoch: 3957 loss_train: 0.0577 loss_val: 0.0531\n",
      "Epoch: 3958 loss_train: 0.0570 loss_val: 0.0539\n",
      "Epoch: 3959 loss_train: 0.0569 loss_val: 0.0540\n",
      "Epoch: 3960 loss_train: 0.0571 loss_val: 0.0563\n",
      "Epoch: 3961 loss_train: 0.0573 loss_val: 0.0535\n",
      "Epoch: 3962 loss_train: 0.0579 loss_val: 0.0518\n",
      "Epoch: 3963 loss_train: 0.0573 loss_val: 0.0553\n",
      "Epoch: 3964 loss_train: 0.0585 loss_val: 0.0565\n",
      "Epoch: 3965 loss_train: 0.0585 loss_val: 0.0547\n",
      "Epoch: 3966 loss_train: 0.0586 loss_val: 0.0566\n",
      "Epoch: 3967 loss_train: 0.0572 loss_val: 0.0552\n",
      "Epoch: 3968 loss_train: 0.0565 loss_val: 0.0556\n",
      "Epoch: 3969 loss_train: 0.0574 loss_val: 0.0545\n",
      "Epoch: 3970 loss_train: 0.0582 loss_val: 0.0581\n",
      "Epoch: 3971 loss_train: 0.0572 loss_val: 0.0557\n",
      "Epoch: 3972 loss_train: 0.0573 loss_val: 0.0557\n",
      "Epoch: 3973 loss_train: 0.0575 loss_val: 0.0558\n",
      "Epoch: 3974 loss_train: 0.0580 loss_val: 0.0580\n",
      "Epoch: 3975 loss_train: 0.0578 loss_val: 0.0604\n",
      "Epoch: 3976 loss_train: 0.0576 loss_val: 0.0569\n",
      "Epoch: 3977 loss_train: 0.0569 loss_val: 0.0562\n",
      "Epoch: 3978 loss_train: 0.0568 loss_val: 0.0542\n",
      "Epoch: 3979 loss_train: 0.0576 loss_val: 0.0563\n",
      "Epoch: 3980 loss_train: 0.0574 loss_val: 0.0542\n",
      "Epoch: 3981 loss_train: 0.0570 loss_val: 0.0552\n",
      "Epoch: 3982 loss_train: 0.0565 loss_val: 0.0553\n",
      "Epoch: 3983 loss_train: 0.0565 loss_val: 0.0560\n",
      "Epoch: 3984 loss_train: 0.0562 loss_val: 0.0535\n",
      "Epoch: 3985 loss_train: 0.0571 loss_val: 0.0563\n",
      "Epoch: 3986 loss_train: 0.0572 loss_val: 0.0545\n",
      "Epoch: 3987 loss_train: 0.0567 loss_val: 0.0563\n",
      "Epoch: 3988 loss_train: 0.0586 loss_val: 0.0559\n",
      "Epoch: 3989 loss_train: 0.0592 loss_val: 0.0534\n",
      "Epoch: 3990 loss_train: 0.0569 loss_val: 0.0548\n",
      "Epoch: 3991 loss_train: 0.0566 loss_val: 0.0548\n",
      "Epoch: 3992 loss_train: 0.0574 loss_val: 0.0534\n",
      "Epoch: 3993 loss_train: 0.0571 loss_val: 0.0546\n",
      "Epoch: 3994 loss_train: 0.0570 loss_val: 0.0561\n",
      "Epoch: 3995 loss_train: 0.0575 loss_val: 0.0541\n",
      "Epoch: 3996 loss_train: 0.0574 loss_val: 0.0542\n",
      "Epoch: 3997 loss_train: 0.0581 loss_val: 0.0546\n",
      "Epoch: 3998 loss_train: 0.0576 loss_val: 0.0555\n",
      "Epoch: 3999 loss_train: 0.0573 loss_val: 0.0554\n",
      "Epoch: 4000 loss_train: 0.0567 loss_val: 0.0533\n",
      "Epoch: 4001 loss_train: 0.0565 loss_val: 0.0526\n",
      "Epoch: 4002 loss_train: 0.0584 loss_val: 0.0550\n",
      "Epoch: 4003 loss_train: 0.0569 loss_val: 0.0535\n",
      "Epoch: 4004 loss_train: 0.0568 loss_val: 0.0569\n",
      "Epoch: 4005 loss_train: 0.0573 loss_val: 0.0554\n",
      "Epoch: 4006 loss_train: 0.0586 loss_val: 0.0566\n",
      "Epoch: 4007 loss_train: 0.0561 loss_val: 0.0549\n",
      "Epoch: 4008 loss_train: 0.0567 loss_val: 0.0539\n",
      "Epoch: 4009 loss_train: 0.0572 loss_val: 0.0542\n",
      "Epoch: 4010 loss_train: 0.0563 loss_val: 0.0534\n",
      "Epoch: 4011 loss_train: 0.0566 loss_val: 0.0547\n",
      "Epoch: 4012 loss_train: 0.0568 loss_val: 0.0531\n",
      "Epoch: 4013 loss_train: 0.0564 loss_val: 0.0548\n",
      "Epoch: 4014 loss_train: 0.0566 loss_val: 0.0554\n",
      "Epoch: 4015 loss_train: 0.0577 loss_val: 0.0561\n",
      "Epoch: 4016 loss_train: 0.0570 loss_val: 0.0543\n",
      "Epoch: 4017 loss_train: 0.0567 loss_val: 0.0549\n",
      "Epoch: 4018 loss_train: 0.0565 loss_val: 0.0560\n",
      "Epoch: 4019 loss_train: 0.0567 loss_val: 0.0560\n",
      "Epoch: 4020 loss_train: 0.0568 loss_val: 0.0541\n",
      "Epoch: 4021 loss_train: 0.0583 loss_val: 0.0556\n",
      "Epoch: 4022 loss_train: 0.0565 loss_val: 0.0547\n",
      "Epoch: 4023 loss_train: 0.0588 loss_val: 0.0554\n",
      "Epoch: 4024 loss_train: 0.0572 loss_val: 0.0564\n",
      "Epoch: 4025 loss_train: 0.0578 loss_val: 0.0563\n",
      "Epoch: 4026 loss_train: 0.0564 loss_val: 0.0562\n",
      "Epoch: 4027 loss_train: 0.0587 loss_val: 0.0564\n",
      "Epoch: 4028 loss_train: 0.0571 loss_val: 0.0561\n",
      "Epoch: 4029 loss_train: 0.0572 loss_val: 0.0572\n",
      "Epoch: 4030 loss_train: 0.0566 loss_val: 0.0547\n",
      "Epoch: 4031 loss_train: 0.0569 loss_val: 0.0544\n",
      "Epoch: 4032 loss_train: 0.0575 loss_val: 0.0523\n",
      "Epoch: 4033 loss_train: 0.0563 loss_val: 0.0533\n",
      "Epoch: 4034 loss_train: 0.0571 loss_val: 0.0525\n",
      "Epoch: 4035 loss_train: 0.0569 loss_val: 0.0570\n",
      "Epoch: 4036 loss_train: 0.0554 loss_val: 0.0571\n",
      "Epoch: 4037 loss_train: 0.0570 loss_val: 0.0564\n",
      "Epoch: 4038 loss_train: 0.0583 loss_val: 0.0544\n",
      "Epoch: 4039 loss_train: 0.0578 loss_val: 0.0556\n",
      "Epoch: 4040 loss_train: 0.0563 loss_val: 0.0554\n",
      "Epoch: 4041 loss_train: 0.0577 loss_val: 0.0564\n",
      "Epoch: 4042 loss_train: 0.0577 loss_val: 0.0559\n",
      "Epoch: 4043 loss_train: 0.0572 loss_val: 0.0584\n",
      "Epoch: 4044 loss_train: 0.0567 loss_val: 0.0553\n",
      "Epoch: 4045 loss_train: 0.0574 loss_val: 0.0545\n",
      "Epoch: 4046 loss_train: 0.0572 loss_val: 0.0552\n",
      "Epoch: 4047 loss_train: 0.0568 loss_val: 0.0561\n",
      "Epoch: 4048 loss_train: 0.0571 loss_val: 0.0555\n",
      "Epoch: 4049 loss_train: 0.0560 loss_val: 0.0543\n",
      "Epoch: 4050 loss_train: 0.0574 loss_val: 0.0549\n",
      "Epoch: 4051 loss_train: 0.0576 loss_val: 0.0556\n",
      "Epoch: 4052 loss_train: 0.0574 loss_val: 0.0538\n",
      "Epoch: 4053 loss_train: 0.0559 loss_val: 0.0546\n",
      "Epoch: 4054 loss_train: 0.0571 loss_val: 0.0542\n",
      "Epoch: 4055 loss_train: 0.0577 loss_val: 0.0578\n",
      "Epoch: 4056 loss_train: 0.0584 loss_val: 0.0532\n",
      "Epoch: 4057 loss_train: 0.0569 loss_val: 0.0539\n",
      "Epoch: 4058 loss_train: 0.0565 loss_val: 0.0514\n",
      "Epoch: 4059 loss_train: 0.0579 loss_val: 0.0537\n",
      "Epoch: 4060 loss_train: 0.0582 loss_val: 0.0540\n",
      "Epoch: 4061 loss_train: 0.0568 loss_val: 0.0536\n",
      "Epoch: 4062 loss_train: 0.0575 loss_val: 0.0537\n",
      "Epoch: 4063 loss_train: 0.0571 loss_val: 0.0549\n",
      "Epoch: 4064 loss_train: 0.0571 loss_val: 0.0548\n",
      "Epoch: 4065 loss_train: 0.0587 loss_val: 0.0559\n",
      "Epoch: 4066 loss_train: 0.0567 loss_val: 0.0531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4067 loss_train: 0.0579 loss_val: 0.0562\n",
      "Epoch: 4068 loss_train: 0.0569 loss_val: 0.0579\n",
      "Epoch: 4069 loss_train: 0.0562 loss_val: 0.0539\n",
      "Epoch: 4070 loss_train: 0.0576 loss_val: 0.0564\n",
      "Epoch: 4071 loss_train: 0.0578 loss_val: 0.0550\n",
      "Epoch: 4072 loss_train: 0.0577 loss_val: 0.0548\n",
      "Epoch: 4073 loss_train: 0.0563 loss_val: 0.0535\n",
      "Epoch: 4074 loss_train: 0.0566 loss_val: 0.0546\n",
      "Epoch: 4075 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 4076 loss_train: 0.0565 loss_val: 0.0568\n",
      "Epoch: 4077 loss_train: 0.0569 loss_val: 0.0551\n",
      "Epoch: 4078 loss_train: 0.0569 loss_val: 0.0551\n",
      "Epoch: 4079 loss_train: 0.0569 loss_val: 0.0537\n",
      "Epoch: 4080 loss_train: 0.0558 loss_val: 0.0560\n",
      "Epoch: 4081 loss_train: 0.0568 loss_val: 0.0523\n",
      "Epoch: 4082 loss_train: 0.0586 loss_val: 0.0533\n",
      "Epoch: 4083 loss_train: 0.0569 loss_val: 0.0528\n",
      "Epoch: 4084 loss_train: 0.0571 loss_val: 0.0530\n",
      "Epoch: 4085 loss_train: 0.0567 loss_val: 0.0528\n",
      "Epoch: 4086 loss_train: 0.0574 loss_val: 0.0535\n",
      "Epoch: 4087 loss_train: 0.0566 loss_val: 0.0518\n",
      "Epoch: 4088 loss_train: 0.0572 loss_val: 0.0522\n",
      "Epoch: 4089 loss_train: 0.0574 loss_val: 0.0531\n",
      "Epoch: 4090 loss_train: 0.0565 loss_val: 0.0540\n",
      "Epoch: 4091 loss_train: 0.0574 loss_val: 0.0557\n",
      "Epoch: 4092 loss_train: 0.0573 loss_val: 0.0525\n",
      "Epoch: 4093 loss_train: 0.0563 loss_val: 0.0539\n",
      "Epoch: 4094 loss_train: 0.0569 loss_val: 0.0547\n",
      "Epoch: 4095 loss_train: 0.0570 loss_val: 0.0584\n",
      "Epoch: 4096 loss_train: 0.0586 loss_val: 0.0553\n",
      "Epoch: 4097 loss_train: 0.0565 loss_val: 0.0531\n",
      "Epoch: 4098 loss_train: 0.0562 loss_val: 0.0537\n",
      "Epoch: 4099 loss_train: 0.0551 loss_val: 0.0533\n",
      "Epoch: 4100 loss_train: 0.0562 loss_val: 0.0536\n",
      "Epoch: 4101 loss_train: 0.0588 loss_val: 0.0539\n",
      "Epoch: 4102 loss_train: 0.0566 loss_val: 0.0548\n",
      "Epoch: 4103 loss_train: 0.0566 loss_val: 0.0555\n",
      "Epoch: 4104 loss_train: 0.0568 loss_val: 0.0555\n",
      "Epoch: 4105 loss_train: 0.0561 loss_val: 0.0556\n",
      "Epoch: 4106 loss_train: 0.0566 loss_val: 0.0562\n",
      "Epoch: 4107 loss_train: 0.0572 loss_val: 0.0544\n",
      "Epoch: 4108 loss_train: 0.0568 loss_val: 0.0556\n",
      "Epoch: 4109 loss_train: 0.0584 loss_val: 0.0555\n",
      "Epoch: 4110 loss_train: 0.0577 loss_val: 0.0537\n",
      "Epoch: 4111 loss_train: 0.0567 loss_val: 0.0533\n",
      "Epoch: 4112 loss_train: 0.0571 loss_val: 0.0547\n",
      "Epoch: 4113 loss_train: 0.0564 loss_val: 0.0529\n",
      "Epoch: 4114 loss_train: 0.0576 loss_val: 0.0549\n",
      "Epoch: 4115 loss_train: 0.0581 loss_val: 0.0537\n",
      "Epoch: 4116 loss_train: 0.0560 loss_val: 0.0541\n",
      "Epoch: 4117 loss_train: 0.0567 loss_val: 0.0544\n",
      "Epoch: 4118 loss_train: 0.0570 loss_val: 0.0539\n",
      "Epoch: 4119 loss_train: 0.0560 loss_val: 0.0545\n",
      "Epoch: 4120 loss_train: 0.0577 loss_val: 0.0551\n",
      "Epoch: 4121 loss_train: 0.0559 loss_val: 0.0560\n",
      "Epoch: 4122 loss_train: 0.0583 loss_val: 0.0580\n",
      "Epoch: 4123 loss_train: 0.0575 loss_val: 0.0566\n",
      "Epoch: 4124 loss_train: 0.0561 loss_val: 0.0562\n",
      "Epoch: 4125 loss_train: 0.0571 loss_val: 0.0565\n",
      "Epoch: 4126 loss_train: 0.0569 loss_val: 0.0564\n",
      "Epoch: 4127 loss_train: 0.0568 loss_val: 0.0573\n",
      "Epoch: 4128 loss_train: 0.0592 loss_val: 0.0577\n",
      "Epoch: 4129 loss_train: 0.0577 loss_val: 0.0560\n",
      "Epoch: 4130 loss_train: 0.0568 loss_val: 0.0578\n",
      "Epoch: 4131 loss_train: 0.0570 loss_val: 0.0577\n",
      "Epoch: 4132 loss_train: 0.0567 loss_val: 0.0575\n",
      "Epoch: 4133 loss_train: 0.0576 loss_val: 0.0569\n",
      "Epoch: 4134 loss_train: 0.0572 loss_val: 0.0567\n",
      "Epoch: 4135 loss_train: 0.0575 loss_val: 0.0564\n",
      "Epoch: 4136 loss_train: 0.0571 loss_val: 0.0555\n",
      "Epoch: 4137 loss_train: 0.0567 loss_val: 0.0542\n",
      "Epoch: 4138 loss_train: 0.0563 loss_val: 0.0565\n",
      "Epoch: 4139 loss_train: 0.0565 loss_val: 0.0538\n",
      "Epoch: 4140 loss_train: 0.0567 loss_val: 0.0567\n",
      "Epoch: 4141 loss_train: 0.0575 loss_val: 0.0553\n",
      "Epoch: 4142 loss_train: 0.0573 loss_val: 0.0563\n",
      "Epoch: 4143 loss_train: 0.0561 loss_val: 0.0554\n",
      "Epoch: 4144 loss_train: 0.0561 loss_val: 0.0559\n",
      "Epoch: 4145 loss_train: 0.0558 loss_val: 0.0564\n",
      "Epoch: 4146 loss_train: 0.0564 loss_val: 0.0549\n",
      "Epoch: 4147 loss_train: 0.0554 loss_val: 0.0562\n",
      "Epoch: 4148 loss_train: 0.0568 loss_val: 0.0545\n",
      "Epoch: 4149 loss_train: 0.0571 loss_val: 0.0545\n",
      "Epoch: 4150 loss_train: 0.0566 loss_val: 0.0536\n",
      "Epoch: 4151 loss_train: 0.0578 loss_val: 0.0563\n",
      "Epoch: 4152 loss_train: 0.0564 loss_val: 0.0541\n",
      "Epoch: 4153 loss_train: 0.0561 loss_val: 0.0540\n",
      "Epoch: 4154 loss_train: 0.0579 loss_val: 0.0561\n",
      "Epoch: 4155 loss_train: 0.0573 loss_val: 0.0554\n",
      "Epoch: 4156 loss_train: 0.0577 loss_val: 0.0551\n",
      "Epoch: 4157 loss_train: 0.0574 loss_val: 0.0570\n",
      "Epoch: 4158 loss_train: 0.0570 loss_val: 0.0561\n",
      "Epoch: 4159 loss_train: 0.0574 loss_val: 0.0580\n",
      "Epoch: 4160 loss_train: 0.0565 loss_val: 0.0563\n",
      "Epoch: 4161 loss_train: 0.0575 loss_val: 0.0528\n",
      "Epoch: 4162 loss_train: 0.0566 loss_val: 0.0552\n",
      "Epoch: 4163 loss_train: 0.0581 loss_val: 0.0567\n",
      "Epoch: 4164 loss_train: 0.0581 loss_val: 0.0584\n",
      "Epoch: 4165 loss_train: 0.0582 loss_val: 0.0556\n",
      "Epoch: 4166 loss_train: 0.0566 loss_val: 0.0562\n",
      "Epoch: 4167 loss_train: 0.0575 loss_val: 0.0538\n",
      "Epoch: 4168 loss_train: 0.0566 loss_val: 0.0546\n",
      "Epoch: 4169 loss_train: 0.0574 loss_val: 0.0550\n",
      "Epoch: 4170 loss_train: 0.0570 loss_val: 0.0551\n",
      "Epoch: 4171 loss_train: 0.0565 loss_val: 0.0546\n",
      "Epoch: 4172 loss_train: 0.0574 loss_val: 0.0545\n",
      "Epoch: 4173 loss_train: 0.0570 loss_val: 0.0557\n",
      "Epoch: 4174 loss_train: 0.0570 loss_val: 0.0561\n",
      "Epoch: 4175 loss_train: 0.0586 loss_val: 0.0536\n",
      "Epoch: 4176 loss_train: 0.0566 loss_val: 0.0541\n",
      "Epoch: 4177 loss_train: 0.0567 loss_val: 0.0539\n",
      "Epoch: 4178 loss_train: 0.0562 loss_val: 0.0550\n",
      "Epoch: 4179 loss_train: 0.0572 loss_val: 0.0547\n",
      "Epoch: 4180 loss_train: 0.0566 loss_val: 0.0567\n",
      "Epoch: 4181 loss_train: 0.0562 loss_val: 0.0541\n",
      "Epoch: 4182 loss_train: 0.0554 loss_val: 0.0559\n",
      "Epoch: 4183 loss_train: 0.0556 loss_val: 0.0554\n",
      "Epoch: 4184 loss_train: 0.0561 loss_val: 0.0557\n",
      "Epoch: 4185 loss_train: 0.0575 loss_val: 0.0549\n",
      "Epoch: 4186 loss_train: 0.0570 loss_val: 0.0574\n",
      "Epoch: 4187 loss_train: 0.0568 loss_val: 0.0572\n",
      "Epoch: 4188 loss_train: 0.0580 loss_val: 0.0570\n",
      "Epoch: 4189 loss_train: 0.0566 loss_val: 0.0540\n",
      "Epoch: 4190 loss_train: 0.0558 loss_val: 0.0535\n",
      "Epoch: 4191 loss_train: 0.0583 loss_val: 0.0553\n",
      "Epoch: 4192 loss_train: 0.0572 loss_val: 0.0561\n",
      "Epoch: 4193 loss_train: 0.0573 loss_val: 0.0549\n",
      "Epoch: 4194 loss_train: 0.0567 loss_val: 0.0553\n",
      "Epoch: 4195 loss_train: 0.0577 loss_val: 0.0561\n",
      "Epoch: 4196 loss_train: 0.0572 loss_val: 0.0563\n",
      "Epoch: 4197 loss_train: 0.0563 loss_val: 0.0555\n",
      "Epoch: 4198 loss_train: 0.0581 loss_val: 0.0559\n",
      "Epoch: 4199 loss_train: 0.0564 loss_val: 0.0543\n",
      "Epoch: 4200 loss_train: 0.0586 loss_val: 0.0568\n",
      "Epoch: 4201 loss_train: 0.0605 loss_val: 0.0542\n",
      "Epoch: 4202 loss_train: 0.0602 loss_val: 0.0545\n",
      "Epoch: 4203 loss_train: 0.0572 loss_val: 0.0551\n",
      "Epoch: 4204 loss_train: 0.0568 loss_val: 0.0555\n",
      "Epoch: 4205 loss_train: 0.0554 loss_val: 0.0544\n",
      "Epoch: 4206 loss_train: 0.0572 loss_val: 0.0538\n",
      "Epoch: 4207 loss_train: 0.0577 loss_val: 0.0542\n",
      "Epoch: 4208 loss_train: 0.0575 loss_val: 0.0563\n",
      "Epoch: 4209 loss_train: 0.0567 loss_val: 0.0572\n",
      "Epoch: 4210 loss_train: 0.0580 loss_val: 0.0545\n",
      "Epoch: 4211 loss_train: 0.0567 loss_val: 0.0583\n",
      "Epoch: 4212 loss_train: 0.0578 loss_val: 0.0591\n",
      "Epoch: 4213 loss_train: 0.0578 loss_val: 0.0563\n",
      "Epoch: 4214 loss_train: 0.0575 loss_val: 0.0563\n",
      "Epoch: 4215 loss_train: 0.0586 loss_val: 0.0587\n",
      "Epoch: 4216 loss_train: 0.0560 loss_val: 0.0563\n",
      "Epoch: 4217 loss_train: 0.0561 loss_val: 0.0557\n",
      "Epoch: 4218 loss_train: 0.0565 loss_val: 0.0549\n",
      "Epoch: 4219 loss_train: 0.0569 loss_val: 0.0576\n",
      "Epoch: 4220 loss_train: 0.0564 loss_val: 0.0571\n",
      "Epoch: 4221 loss_train: 0.0558 loss_val: 0.0562\n",
      "Epoch: 4222 loss_train: 0.0570 loss_val: 0.0553\n",
      "Epoch: 4223 loss_train: 0.0569 loss_val: 0.0562\n",
      "Epoch: 4224 loss_train: 0.0555 loss_val: 0.0571\n",
      "Epoch: 4225 loss_train: 0.0566 loss_val: 0.0561\n",
      "Epoch: 4226 loss_train: 0.0563 loss_val: 0.0545\n",
      "Epoch: 4227 loss_train: 0.0571 loss_val: 0.0555\n",
      "Epoch: 4228 loss_train: 0.0565 loss_val: 0.0581\n",
      "Epoch: 4229 loss_train: 0.0562 loss_val: 0.0547\n",
      "Epoch: 4230 loss_train: 0.0570 loss_val: 0.0552\n",
      "Epoch: 4231 loss_train: 0.0568 loss_val: 0.0551\n",
      "Epoch: 4232 loss_train: 0.0564 loss_val: 0.0571\n",
      "Epoch: 4233 loss_train: 0.0556 loss_val: 0.0581\n",
      "Epoch: 4234 loss_train: 0.0566 loss_val: 0.0563\n",
      "Epoch: 4235 loss_train: 0.0561 loss_val: 0.0565\n",
      "Epoch: 4236 loss_train: 0.0561 loss_val: 0.0550\n",
      "Epoch: 4237 loss_train: 0.0560 loss_val: 0.0554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4238 loss_train: 0.0569 loss_val: 0.0564\n",
      "Epoch: 4239 loss_train: 0.0564 loss_val: 0.0562\n",
      "Epoch: 4240 loss_train: 0.0565 loss_val: 0.0566\n",
      "Epoch: 4241 loss_train: 0.0561 loss_val: 0.0557\n",
      "Epoch: 4242 loss_train: 0.0559 loss_val: 0.0556\n",
      "Epoch: 4243 loss_train: 0.0563 loss_val: 0.0570\n",
      "Epoch: 4244 loss_train: 0.0568 loss_val: 0.0553\n",
      "Epoch: 4245 loss_train: 0.0566 loss_val: 0.0552\n",
      "Epoch: 4246 loss_train: 0.0574 loss_val: 0.0565\n",
      "Epoch: 4247 loss_train: 0.0583 loss_val: 0.0590\n",
      "Epoch: 4248 loss_train: 0.0577 loss_val: 0.0565\n",
      "Epoch: 4249 loss_train: 0.0579 loss_val: 0.0556\n",
      "Epoch: 4250 loss_train: 0.0571 loss_val: 0.0546\n",
      "Epoch: 4251 loss_train: 0.0576 loss_val: 0.0567\n",
      "Epoch: 4252 loss_train: 0.0562 loss_val: 0.0551\n",
      "Epoch: 4253 loss_train: 0.0561 loss_val: 0.0558\n",
      "Epoch: 4254 loss_train: 0.0556 loss_val: 0.0564\n",
      "Epoch: 4255 loss_train: 0.0574 loss_val: 0.0557\n",
      "Epoch: 4256 loss_train: 0.0566 loss_val: 0.0544\n",
      "Epoch: 4257 loss_train: 0.0568 loss_val: 0.0533\n",
      "Epoch: 4258 loss_train: 0.0560 loss_val: 0.0555\n",
      "Epoch: 4259 loss_train: 0.0565 loss_val: 0.0553\n",
      "Epoch: 4260 loss_train: 0.0557 loss_val: 0.0552\n",
      "Epoch: 4261 loss_train: 0.0566 loss_val: 0.0537\n",
      "Epoch: 4262 loss_train: 0.0568 loss_val: 0.0558\n",
      "Epoch: 4263 loss_train: 0.0566 loss_val: 0.0554\n",
      "Epoch: 4264 loss_train: 0.0556 loss_val: 0.0560\n",
      "Epoch: 4265 loss_train: 0.0554 loss_val: 0.0570\n",
      "Epoch: 4266 loss_train: 0.0569 loss_val: 0.0557\n",
      "Epoch: 4267 loss_train: 0.0567 loss_val: 0.0562\n",
      "Epoch: 4268 loss_train: 0.0575 loss_val: 0.0557\n",
      "Epoch: 4269 loss_train: 0.0549 loss_val: 0.0568\n",
      "Epoch: 4270 loss_train: 0.0558 loss_val: 0.0547\n",
      "Epoch: 4271 loss_train: 0.0577 loss_val: 0.0543\n",
      "Epoch: 4272 loss_train: 0.0564 loss_val: 0.0548\n",
      "Epoch: 4273 loss_train: 0.0583 loss_val: 0.0548\n",
      "Epoch: 4274 loss_train: 0.0569 loss_val: 0.0582\n",
      "Epoch: 4275 loss_train: 0.0564 loss_val: 0.0527\n",
      "Epoch: 4276 loss_train: 0.0568 loss_val: 0.0552\n",
      "Epoch: 4277 loss_train: 0.0553 loss_val: 0.0537\n",
      "Epoch: 4278 loss_train: 0.0573 loss_val: 0.0554\n",
      "Epoch: 4279 loss_train: 0.0562 loss_val: 0.0550\n",
      "Epoch: 4280 loss_train: 0.0584 loss_val: 0.0537\n",
      "Epoch: 4281 loss_train: 0.0555 loss_val: 0.0556\n",
      "Epoch: 4282 loss_train: 0.0567 loss_val: 0.0543\n",
      "Epoch: 4283 loss_train: 0.0579 loss_val: 0.0556\n",
      "Epoch: 4284 loss_train: 0.0562 loss_val: 0.0555\n",
      "Epoch: 4285 loss_train: 0.0573 loss_val: 0.0563\n",
      "Epoch: 4286 loss_train: 0.0573 loss_val: 0.0545\n",
      "Epoch: 4287 loss_train: 0.0582 loss_val: 0.0549\n",
      "Epoch: 4288 loss_train: 0.0561 loss_val: 0.0548\n",
      "Epoch: 4289 loss_train: 0.0559 loss_val: 0.0564\n",
      "Epoch: 4290 loss_train: 0.0567 loss_val: 0.0560\n",
      "Epoch: 4291 loss_train: 0.0573 loss_val: 0.0544\n",
      "Epoch: 4292 loss_train: 0.0563 loss_val: 0.0553\n",
      "Epoch: 4293 loss_train: 0.0569 loss_val: 0.0558\n",
      "Epoch: 4294 loss_train: 0.0575 loss_val: 0.0543\n",
      "Epoch: 4295 loss_train: 0.0554 loss_val: 0.0550\n",
      "Epoch: 4296 loss_train: 0.0572 loss_val: 0.0555\n",
      "Epoch: 4297 loss_train: 0.0566 loss_val: 0.0574\n",
      "Epoch: 4298 loss_train: 0.0569 loss_val: 0.0558\n",
      "Epoch: 4299 loss_train: 0.0558 loss_val: 0.0572\n",
      "Epoch: 4300 loss_train: 0.0566 loss_val: 0.0548\n",
      "Epoch: 4301 loss_train: 0.0556 loss_val: 0.0578\n",
      "Epoch: 4302 loss_train: 0.0570 loss_val: 0.0549\n",
      "Epoch: 4303 loss_train: 0.0560 loss_val: 0.0563\n",
      "Epoch: 4304 loss_train: 0.0567 loss_val: 0.0567\n",
      "Epoch: 4305 loss_train: 0.0571 loss_val: 0.0537\n",
      "Epoch: 4306 loss_train: 0.0566 loss_val: 0.0558\n",
      "Epoch: 4307 loss_train: 0.0560 loss_val: 0.0543\n",
      "Epoch: 4308 loss_train: 0.0553 loss_val: 0.0554\n",
      "Epoch: 4309 loss_train: 0.0565 loss_val: 0.0557\n",
      "Epoch: 4310 loss_train: 0.0572 loss_val: 0.0551\n",
      "Epoch: 4311 loss_train: 0.0569 loss_val: 0.0542\n",
      "Epoch: 4312 loss_train: 0.0589 loss_val: 0.0556\n",
      "Epoch: 4313 loss_train: 0.0584 loss_val: 0.0553\n",
      "Epoch: 4314 loss_train: 0.0579 loss_val: 0.0551\n",
      "Epoch: 4315 loss_train: 0.0577 loss_val: 0.0549\n",
      "Epoch: 4316 loss_train: 0.0573 loss_val: 0.0564\n",
      "Epoch: 4317 loss_train: 0.0568 loss_val: 0.0566\n",
      "Epoch: 4318 loss_train: 0.0566 loss_val: 0.0576\n",
      "Epoch: 4319 loss_train: 0.0575 loss_val: 0.0545\n",
      "Epoch: 4320 loss_train: 0.0570 loss_val: 0.0553\n",
      "Epoch: 4321 loss_train: 0.0571 loss_val: 0.0551\n",
      "Epoch: 4322 loss_train: 0.0561 loss_val: 0.0546\n",
      "Epoch: 4323 loss_train: 0.0568 loss_val: 0.0555\n",
      "Epoch: 4324 loss_train: 0.0571 loss_val: 0.0553\n",
      "Epoch: 4325 loss_train: 0.0567 loss_val: 0.0563\n",
      "Epoch: 4326 loss_train: 0.0571 loss_val: 0.0542\n",
      "Epoch: 4327 loss_train: 0.0572 loss_val: 0.0551\n",
      "Epoch: 4328 loss_train: 0.0569 loss_val: 0.0551\n",
      "Epoch: 4329 loss_train: 0.0568 loss_val: 0.0543\n",
      "Epoch: 4330 loss_train: 0.0567 loss_val: 0.0554\n",
      "Epoch: 4331 loss_train: 0.0586 loss_val: 0.0555\n",
      "Epoch: 4332 loss_train: 0.0560 loss_val: 0.0554\n",
      "Epoch: 4333 loss_train: 0.0572 loss_val: 0.0547\n",
      "Epoch: 4334 loss_train: 0.0565 loss_val: 0.0555\n",
      "Epoch: 4335 loss_train: 0.0572 loss_val: 0.0558\n",
      "Epoch: 4336 loss_train: 0.0560 loss_val: 0.0550\n",
      "Epoch: 4337 loss_train: 0.0567 loss_val: 0.0568\n",
      "Epoch: 4338 loss_train: 0.0576 loss_val: 0.0581\n",
      "Epoch: 4339 loss_train: 0.0576 loss_val: 0.0578\n",
      "Epoch: 4340 loss_train: 0.0586 loss_val: 0.0541\n",
      "Epoch: 4341 loss_train: 0.0575 loss_val: 0.0561\n",
      "Epoch: 4342 loss_train: 0.0564 loss_val: 0.0589\n",
      "Epoch: 4343 loss_train: 0.0570 loss_val: 0.0555\n",
      "Epoch: 4344 loss_train: 0.0576 loss_val: 0.0554\n",
      "Epoch: 4345 loss_train: 0.0564 loss_val: 0.0553\n",
      "Epoch: 4346 loss_train: 0.0552 loss_val: 0.0569\n",
      "Epoch: 4347 loss_train: 0.0572 loss_val: 0.0568\n",
      "Epoch: 4348 loss_train: 0.0568 loss_val: 0.0565\n",
      "Epoch: 4349 loss_train: 0.0567 loss_val: 0.0580\n",
      "Epoch: 4350 loss_train: 0.0548 loss_val: 0.0577\n",
      "Epoch: 4351 loss_train: 0.0562 loss_val: 0.0555\n",
      "Epoch: 4352 loss_train: 0.0572 loss_val: 0.0554\n",
      "Epoch: 4353 loss_train: 0.0566 loss_val: 0.0555\n",
      "Epoch: 4354 loss_train: 0.0553 loss_val: 0.0544\n",
      "Epoch: 4355 loss_train: 0.0569 loss_val: 0.0546\n",
      "Epoch: 4356 loss_train: 0.0566 loss_val: 0.0540\n",
      "Epoch: 4357 loss_train: 0.0568 loss_val: 0.0543\n",
      "Epoch: 4358 loss_train: 0.0573 loss_val: 0.0555\n",
      "Epoch: 4359 loss_train: 0.0562 loss_val: 0.0515\n",
      "Epoch: 4360 loss_train: 0.0562 loss_val: 0.0539\n",
      "Epoch: 4361 loss_train: 0.0564 loss_val: 0.0561\n",
      "Epoch: 4362 loss_train: 0.0571 loss_val: 0.0559\n",
      "Epoch: 4363 loss_train: 0.0564 loss_val: 0.0543\n",
      "Epoch: 4364 loss_train: 0.0550 loss_val: 0.0555\n",
      "Epoch: 4365 loss_train: 0.0558 loss_val: 0.0536\n",
      "Epoch: 4366 loss_train: 0.0560 loss_val: 0.0554\n",
      "Epoch: 4367 loss_train: 0.0559 loss_val: 0.0555\n",
      "Epoch: 4368 loss_train: 0.0559 loss_val: 0.0548\n",
      "Epoch: 4369 loss_train: 0.0567 loss_val: 0.0564\n",
      "Epoch: 4370 loss_train: 0.0566 loss_val: 0.0545\n",
      "Epoch: 4371 loss_train: 0.0566 loss_val: 0.0558\n",
      "Epoch: 4372 loss_train: 0.0569 loss_val: 0.0570\n",
      "Epoch: 4373 loss_train: 0.0570 loss_val: 0.0567\n",
      "Epoch: 4374 loss_train: 0.0575 loss_val: 0.0570\n",
      "Epoch: 4375 loss_train: 0.0559 loss_val: 0.0557\n",
      "Epoch: 4376 loss_train: 0.0570 loss_val: 0.0580\n",
      "Epoch: 4377 loss_train: 0.0568 loss_val: 0.0582\n",
      "Epoch: 4378 loss_train: 0.0568 loss_val: 0.0568\n",
      "Epoch: 4379 loss_train: 0.0568 loss_val: 0.0568\n",
      "Epoch: 4380 loss_train: 0.0567 loss_val: 0.0565\n",
      "Epoch: 4381 loss_train: 0.0562 loss_val: 0.0571\n",
      "Epoch: 4382 loss_train: 0.0566 loss_val: 0.0580\n",
      "Epoch: 4383 loss_train: 0.0559 loss_val: 0.0559\n",
      "Epoch: 4384 loss_train: 0.0565 loss_val: 0.0570\n",
      "Epoch: 4385 loss_train: 0.0566 loss_val: 0.0538\n",
      "Epoch: 4386 loss_train: 0.0554 loss_val: 0.0554\n",
      "Epoch: 4387 loss_train: 0.0562 loss_val: 0.0557\n",
      "Epoch: 4388 loss_train: 0.0568 loss_val: 0.0557\n",
      "Epoch: 4389 loss_train: 0.0573 loss_val: 0.0540\n",
      "Epoch: 4390 loss_train: 0.0579 loss_val: 0.0523\n",
      "Epoch: 4391 loss_train: 0.0563 loss_val: 0.0528\n",
      "Epoch: 4392 loss_train: 0.0563 loss_val: 0.0581\n",
      "Epoch: 4393 loss_train: 0.0585 loss_val: 0.0537\n",
      "Epoch: 4394 loss_train: 0.0561 loss_val: 0.0531\n",
      "Epoch: 4395 loss_train: 0.0571 loss_val: 0.0553\n",
      "Epoch: 4396 loss_train: 0.0566 loss_val: 0.0542\n",
      "Epoch: 4397 loss_train: 0.0559 loss_val: 0.0547\n",
      "Epoch: 4398 loss_train: 0.0572 loss_val: 0.0554\n",
      "Epoch: 4399 loss_train: 0.0569 loss_val: 0.0566\n",
      "Epoch: 4400 loss_train: 0.0558 loss_val: 0.0549\n",
      "Epoch: 4401 loss_train: 0.0559 loss_val: 0.0541\n",
      "Epoch: 4402 loss_train: 0.0562 loss_val: 0.0555\n",
      "Epoch: 4403 loss_train: 0.0573 loss_val: 0.0558\n",
      "Epoch: 4404 loss_train: 0.0563 loss_val: 0.0558\n",
      "Epoch: 4405 loss_train: 0.0564 loss_val: 0.0563\n",
      "Epoch: 4406 loss_train: 0.0575 loss_val: 0.0571\n",
      "Epoch: 4407 loss_train: 0.0552 loss_val: 0.0538\n",
      "Epoch: 4408 loss_train: 0.0561 loss_val: 0.0537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4409 loss_train: 0.0569 loss_val: 0.0530\n",
      "Epoch: 4410 loss_train: 0.0559 loss_val: 0.0541\n",
      "Epoch: 4411 loss_train: 0.0559 loss_val: 0.0552\n",
      "Epoch: 4412 loss_train: 0.0556 loss_val: 0.0564\n",
      "Epoch: 4413 loss_train: 0.0565 loss_val: 0.0551\n",
      "Epoch: 4414 loss_train: 0.0561 loss_val: 0.0545\n",
      "Epoch: 4415 loss_train: 0.0560 loss_val: 0.0539\n",
      "Epoch: 4416 loss_train: 0.0560 loss_val: 0.0565\n",
      "Epoch: 4417 loss_train: 0.0562 loss_val: 0.0552\n",
      "Epoch: 4418 loss_train: 0.0574 loss_val: 0.0565\n",
      "Epoch: 4419 loss_train: 0.0575 loss_val: 0.0551\n",
      "Epoch: 4420 loss_train: 0.0579 loss_val: 0.0551\n",
      "Epoch: 4421 loss_train: 0.0569 loss_val: 0.0564\n",
      "Epoch: 4422 loss_train: 0.0572 loss_val: 0.0553\n",
      "Epoch: 4423 loss_train: 0.0566 loss_val: 0.0575\n",
      "Epoch: 4424 loss_train: 0.0574 loss_val: 0.0571\n",
      "Epoch: 4425 loss_train: 0.0581 loss_val: 0.0570\n",
      "Epoch: 4426 loss_train: 0.0572 loss_val: 0.0537\n",
      "Epoch: 4427 loss_train: 0.0567 loss_val: 0.0546\n",
      "Epoch: 4428 loss_train: 0.0557 loss_val: 0.0533\n",
      "Epoch: 4429 loss_train: 0.0565 loss_val: 0.0550\n",
      "Epoch: 4430 loss_train: 0.0563 loss_val: 0.0532\n",
      "Epoch: 4431 loss_train: 0.0553 loss_val: 0.0547\n",
      "Epoch: 4432 loss_train: 0.0559 loss_val: 0.0544\n",
      "Epoch: 4433 loss_train: 0.0571 loss_val: 0.0551\n",
      "Epoch: 4434 loss_train: 0.0585 loss_val: 0.0559\n",
      "Epoch: 4435 loss_train: 0.0570 loss_val: 0.0559\n",
      "Epoch: 4436 loss_train: 0.0570 loss_val: 0.0548\n",
      "Epoch: 4437 loss_train: 0.0566 loss_val: 0.0552\n",
      "Epoch: 4438 loss_train: 0.0572 loss_val: 0.0556\n",
      "Epoch: 4439 loss_train: 0.0572 loss_val: 0.0547\n",
      "Epoch: 4440 loss_train: 0.0563 loss_val: 0.0536\n",
      "Epoch: 4441 loss_train: 0.0559 loss_val: 0.0542\n",
      "Epoch: 4442 loss_train: 0.0563 loss_val: 0.0563\n",
      "Epoch: 4443 loss_train: 0.0555 loss_val: 0.0564\n",
      "Epoch: 4444 loss_train: 0.0574 loss_val: 0.0561\n",
      "Epoch: 4445 loss_train: 0.0578 loss_val: 0.0560\n",
      "Epoch: 4446 loss_train: 0.0574 loss_val: 0.0578\n",
      "Epoch: 4447 loss_train: 0.0563 loss_val: 0.0544\n",
      "Epoch: 4448 loss_train: 0.0566 loss_val: 0.0542\n",
      "Epoch: 4449 loss_train: 0.0577 loss_val: 0.0544\n",
      "Epoch: 4450 loss_train: 0.0556 loss_val: 0.0540\n",
      "Epoch: 4451 loss_train: 0.0559 loss_val: 0.0527\n",
      "Epoch: 4452 loss_train: 0.0575 loss_val: 0.0540\n",
      "Epoch: 4453 loss_train: 0.0570 loss_val: 0.0517\n",
      "Epoch: 4454 loss_train: 0.0562 loss_val: 0.0534\n",
      "Epoch: 4455 loss_train: 0.0559 loss_val: 0.0540\n",
      "Epoch: 4456 loss_train: 0.0559 loss_val: 0.0536\n",
      "Epoch: 4457 loss_train: 0.0567 loss_val: 0.0549\n",
      "Epoch: 4458 loss_train: 0.0567 loss_val: 0.0527\n",
      "Epoch: 4459 loss_train: 0.0570 loss_val: 0.0549\n",
      "Epoch: 4460 loss_train: 0.0574 loss_val: 0.0541\n",
      "Epoch: 4461 loss_train: 0.0557 loss_val: 0.0545\n",
      "Epoch: 4462 loss_train: 0.0566 loss_val: 0.0550\n",
      "Epoch: 4463 loss_train: 0.0567 loss_val: 0.0557\n",
      "Epoch: 4464 loss_train: 0.0562 loss_val: 0.0552\n",
      "Epoch: 4465 loss_train: 0.0559 loss_val: 0.0544\n",
      "Epoch: 4466 loss_train: 0.0554 loss_val: 0.0537\n",
      "Epoch: 4467 loss_train: 0.0563 loss_val: 0.0539\n",
      "Epoch: 4468 loss_train: 0.0560 loss_val: 0.0531\n",
      "Epoch: 4469 loss_train: 0.0557 loss_val: 0.0554\n",
      "Epoch: 4470 loss_train: 0.0552 loss_val: 0.0548\n",
      "Epoch: 4471 loss_train: 0.0563 loss_val: 0.0570\n",
      "Epoch: 4472 loss_train: 0.0573 loss_val: 0.0552\n",
      "Epoch: 4473 loss_train: 0.0573 loss_val: 0.0569\n",
      "Epoch: 4474 loss_train: 0.0564 loss_val: 0.0567\n",
      "Epoch: 4475 loss_train: 0.0567 loss_val: 0.0559\n",
      "Epoch: 4476 loss_train: 0.0546 loss_val: 0.0553\n",
      "Epoch: 4477 loss_train: 0.0565 loss_val: 0.0561\n",
      "Epoch: 4478 loss_train: 0.0574 loss_val: 0.0580\n",
      "Epoch: 4479 loss_train: 0.0563 loss_val: 0.0542\n",
      "Epoch: 4480 loss_train: 0.0554 loss_val: 0.0549\n",
      "Epoch: 4481 loss_train: 0.0557 loss_val: 0.0576\n",
      "Epoch: 4482 loss_train: 0.0558 loss_val: 0.0559\n",
      "Epoch: 4483 loss_train: 0.0567 loss_val: 0.0548\n",
      "Epoch: 4484 loss_train: 0.0563 loss_val: 0.0547\n",
      "Epoch: 4485 loss_train: 0.0559 loss_val: 0.0569\n",
      "Epoch: 4486 loss_train: 0.0560 loss_val: 0.0561\n",
      "Epoch: 4487 loss_train: 0.0576 loss_val: 0.0559\n",
      "Epoch: 4488 loss_train: 0.0563 loss_val: 0.0553\n",
      "Epoch: 4489 loss_train: 0.0551 loss_val: 0.0559\n",
      "Epoch: 4490 loss_train: 0.0555 loss_val: 0.0554\n",
      "Epoch: 4491 loss_train: 0.0564 loss_val: 0.0546\n",
      "Epoch: 4492 loss_train: 0.0562 loss_val: 0.0579\n",
      "Epoch: 4493 loss_train: 0.0563 loss_val: 0.0563\n",
      "Epoch: 4494 loss_train: 0.0565 loss_val: 0.0579\n",
      "Epoch: 4495 loss_train: 0.0566 loss_val: 0.0574\n",
      "Epoch: 4496 loss_train: 0.0552 loss_val: 0.0542\n",
      "Epoch: 4497 loss_train: 0.0564 loss_val: 0.0550\n",
      "Epoch: 4498 loss_train: 0.0564 loss_val: 0.0567\n",
      "Epoch: 4499 loss_train: 0.0566 loss_val: 0.0569\n",
      "Epoch: 4500 loss_train: 0.0558 loss_val: 0.0551\n",
      "Epoch: 4501 loss_train: 0.0564 loss_val: 0.0577\n",
      "Epoch: 4502 loss_train: 0.0557 loss_val: 0.0570\n",
      "Epoch: 4503 loss_train: 0.0557 loss_val: 0.0542\n",
      "Epoch: 4504 loss_train: 0.0569 loss_val: 0.0553\n",
      "Epoch: 4505 loss_train: 0.0558 loss_val: 0.0548\n",
      "Epoch: 4506 loss_train: 0.0568 loss_val: 0.0544\n",
      "Epoch: 4507 loss_train: 0.0564 loss_val: 0.0558\n",
      "Epoch: 4508 loss_train: 0.0555 loss_val: 0.0534\n",
      "Epoch: 4509 loss_train: 0.0564 loss_val: 0.0536\n",
      "Epoch: 4510 loss_train: 0.0571 loss_val: 0.0558\n",
      "Epoch: 4511 loss_train: 0.0571 loss_val: 0.0576\n",
      "Epoch: 4512 loss_train: 0.0562 loss_val: 0.0554\n",
      "Epoch: 4513 loss_train: 0.0571 loss_val: 0.0558\n",
      "Epoch: 4514 loss_train: 0.0566 loss_val: 0.0564\n",
      "Epoch: 4515 loss_train: 0.0566 loss_val: 0.0546\n",
      "Epoch: 4516 loss_train: 0.0569 loss_val: 0.0544\n",
      "Epoch: 4517 loss_train: 0.0567 loss_val: 0.0543\n",
      "Epoch: 4518 loss_train: 0.0588 loss_val: 0.0559\n",
      "Epoch: 4519 loss_train: 0.0581 loss_val: 0.0550\n",
      "Epoch: 4520 loss_train: 0.0577 loss_val: 0.0547\n",
      "Epoch: 4521 loss_train: 0.0568 loss_val: 0.0566\n",
      "Epoch: 4522 loss_train: 0.0572 loss_val: 0.0550\n",
      "Epoch: 4523 loss_train: 0.0564 loss_val: 0.0561\n",
      "Epoch: 4524 loss_train: 0.0570 loss_val: 0.0564\n",
      "Epoch: 4525 loss_train: 0.0558 loss_val: 0.0554\n",
      "Epoch: 4526 loss_train: 0.0554 loss_val: 0.0545\n",
      "Epoch: 4527 loss_train: 0.0553 loss_val: 0.0560\n",
      "Epoch: 4528 loss_train: 0.0567 loss_val: 0.0564\n",
      "Epoch: 4529 loss_train: 0.0564 loss_val: 0.0561\n",
      "Epoch: 4530 loss_train: 0.0558 loss_val: 0.0569\n",
      "Epoch: 4531 loss_train: 0.0572 loss_val: 0.0564\n",
      "Epoch: 4532 loss_train: 0.0562 loss_val: 0.0540\n",
      "Epoch: 4533 loss_train: 0.0568 loss_val: 0.0539\n",
      "Epoch: 4534 loss_train: 0.0567 loss_val: 0.0551\n",
      "Epoch: 4535 loss_train: 0.0547 loss_val: 0.0557\n",
      "Epoch: 4536 loss_train: 0.0545 loss_val: 0.0551\n",
      "Epoch: 4537 loss_train: 0.0569 loss_val: 0.0547\n",
      "Epoch: 4538 loss_train: 0.0560 loss_val: 0.0537\n",
      "Epoch: 4539 loss_train: 0.0557 loss_val: 0.0558\n",
      "Epoch: 4540 loss_train: 0.0571 loss_val: 0.0549\n",
      "Epoch: 4541 loss_train: 0.0564 loss_val: 0.0539\n",
      "Epoch: 4542 loss_train: 0.0570 loss_val: 0.0557\n",
      "Epoch: 4543 loss_train: 0.0555 loss_val: 0.0535\n",
      "Epoch: 4544 loss_train: 0.0559 loss_val: 0.0557\n",
      "Epoch: 4545 loss_train: 0.0569 loss_val: 0.0544\n",
      "Epoch: 4546 loss_train: 0.0558 loss_val: 0.0545\n",
      "Epoch: 4547 loss_train: 0.0560 loss_val: 0.0559\n",
      "Epoch: 4548 loss_train: 0.0564 loss_val: 0.0559\n",
      "Epoch: 4549 loss_train: 0.0559 loss_val: 0.0575\n",
      "Epoch: 4550 loss_train: 0.0569 loss_val: 0.0571\n",
      "Epoch: 4551 loss_train: 0.0558 loss_val: 0.0558\n",
      "Epoch: 4552 loss_train: 0.0563 loss_val: 0.0553\n",
      "Epoch: 4553 loss_train: 0.0552 loss_val: 0.0556\n",
      "Epoch: 4554 loss_train: 0.0563 loss_val: 0.0560\n",
      "Epoch: 4555 loss_train: 0.0572 loss_val: 0.0566\n",
      "Epoch: 4556 loss_train: 0.0563 loss_val: 0.0539\n",
      "Epoch: 4557 loss_train: 0.0563 loss_val: 0.0547\n",
      "Epoch: 4558 loss_train: 0.0568 loss_val: 0.0554\n",
      "Epoch: 4559 loss_train: 0.0557 loss_val: 0.0572\n",
      "Epoch: 4560 loss_train: 0.0569 loss_val: 0.0561\n",
      "Epoch: 4561 loss_train: 0.0574 loss_val: 0.0548\n",
      "Epoch: 4562 loss_train: 0.0549 loss_val: 0.0551\n",
      "Epoch: 4563 loss_train: 0.0559 loss_val: 0.0539\n",
      "Epoch: 4564 loss_train: 0.0560 loss_val: 0.0538\n",
      "Epoch: 4565 loss_train: 0.0565 loss_val: 0.0552\n",
      "Epoch: 4566 loss_train: 0.0562 loss_val: 0.0561\n",
      "Epoch: 4567 loss_train: 0.0563 loss_val: 0.0573\n",
      "Epoch: 4568 loss_train: 0.0562 loss_val: 0.0572\n",
      "Epoch: 4569 loss_train: 0.0553 loss_val: 0.0571\n",
      "Epoch: 4570 loss_train: 0.0562 loss_val: 0.0564\n",
      "Epoch: 4571 loss_train: 0.0546 loss_val: 0.0540\n",
      "Epoch: 4572 loss_train: 0.0569 loss_val: 0.0541\n",
      "Epoch: 4573 loss_train: 0.0555 loss_val: 0.0564\n",
      "Epoch: 4574 loss_train: 0.0563 loss_val: 0.0555\n",
      "Epoch: 4575 loss_train: 0.0553 loss_val: 0.0560\n",
      "Epoch: 4576 loss_train: 0.0557 loss_val: 0.0530\n",
      "Epoch: 4577 loss_train: 0.0559 loss_val: 0.0555\n",
      "Epoch: 4578 loss_train: 0.0559 loss_val: 0.0552\n",
      "Epoch: 4579 loss_train: 0.0559 loss_val: 0.0572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4580 loss_train: 0.0564 loss_val: 0.0570\n",
      "Epoch: 4581 loss_train: 0.0561 loss_val: 0.0539\n",
      "Epoch: 4582 loss_train: 0.0559 loss_val: 0.0582\n",
      "Epoch: 4583 loss_train: 0.0563 loss_val: 0.0561\n",
      "Epoch: 4584 loss_train: 0.0561 loss_val: 0.0559\n",
      "Epoch: 4585 loss_train: 0.0563 loss_val: 0.0546\n",
      "Epoch: 4586 loss_train: 0.0562 loss_val: 0.0556\n",
      "Epoch: 4587 loss_train: 0.0571 loss_val: 0.0552\n",
      "Epoch: 4588 loss_train: 0.0565 loss_val: 0.0542\n",
      "Epoch: 4589 loss_train: 0.0554 loss_val: 0.0555\n",
      "Epoch: 4590 loss_train: 0.0552 loss_val: 0.0556\n",
      "Epoch: 4591 loss_train: 0.0560 loss_val: 0.0545\n",
      "Epoch: 4592 loss_train: 0.0559 loss_val: 0.0551\n",
      "Epoch: 4593 loss_train: 0.0566 loss_val: 0.0564\n",
      "Epoch: 4594 loss_train: 0.0573 loss_val: 0.0548\n",
      "Epoch: 4595 loss_train: 0.0554 loss_val: 0.0550\n",
      "Epoch: 4596 loss_train: 0.0560 loss_val: 0.0532\n",
      "Epoch: 4597 loss_train: 0.0559 loss_val: 0.0557\n",
      "Epoch: 4598 loss_train: 0.0548 loss_val: 0.0540\n",
      "Epoch: 4599 loss_train: 0.0562 loss_val: 0.0550\n",
      "Epoch: 4600 loss_train: 0.0565 loss_val: 0.0550\n",
      "Epoch: 4601 loss_train: 0.0567 loss_val: 0.0565\n",
      "Epoch: 4602 loss_train: 0.0563 loss_val: 0.0544\n",
      "Epoch: 4603 loss_train: 0.0564 loss_val: 0.0565\n",
      "Epoch: 4604 loss_train: 0.0563 loss_val: 0.0548\n",
      "Epoch: 4605 loss_train: 0.0560 loss_val: 0.0533\n",
      "Epoch: 4606 loss_train: 0.0563 loss_val: 0.0542\n",
      "Epoch: 4607 loss_train: 0.0563 loss_val: 0.0555\n",
      "Epoch: 4608 loss_train: 0.0566 loss_val: 0.0538\n",
      "Epoch: 4609 loss_train: 0.0557 loss_val: 0.0537\n",
      "Epoch: 4610 loss_train: 0.0547 loss_val: 0.0531\n",
      "Epoch: 4611 loss_train: 0.0562 loss_val: 0.0536\n",
      "Epoch: 4612 loss_train: 0.0567 loss_val: 0.0545\n",
      "Epoch: 4613 loss_train: 0.0557 loss_val: 0.0560\n",
      "Epoch: 4614 loss_train: 0.0577 loss_val: 0.0555\n",
      "Epoch: 4615 loss_train: 0.0555 loss_val: 0.0577\n",
      "Epoch: 4616 loss_train: 0.0574 loss_val: 0.0576\n",
      "Epoch: 4617 loss_train: 0.0566 loss_val: 0.0570\n",
      "Epoch: 4618 loss_train: 0.0564 loss_val: 0.0555\n",
      "Epoch: 4619 loss_train: 0.0558 loss_val: 0.0560\n",
      "Epoch: 4620 loss_train: 0.0552 loss_val: 0.0563\n",
      "Epoch: 4621 loss_train: 0.0566 loss_val: 0.0558\n",
      "Epoch: 4622 loss_train: 0.0573 loss_val: 0.0573\n",
      "Epoch: 4623 loss_train: 0.0559 loss_val: 0.0563\n",
      "Epoch: 4624 loss_train: 0.0564 loss_val: 0.0558\n",
      "Epoch: 4625 loss_train: 0.0569 loss_val: 0.0582\n",
      "Epoch: 4626 loss_train: 0.0559 loss_val: 0.0574\n",
      "Epoch: 4627 loss_train: 0.0554 loss_val: 0.0562\n",
      "Epoch: 4628 loss_train: 0.0568 loss_val: 0.0573\n",
      "Epoch: 4629 loss_train: 0.0566 loss_val: 0.0572\n",
      "Epoch: 4630 loss_train: 0.0560 loss_val: 0.0570\n",
      "Epoch: 4631 loss_train: 0.0563 loss_val: 0.0570\n",
      "Epoch: 4632 loss_train: 0.0558 loss_val: 0.0574\n",
      "Epoch: 4633 loss_train: 0.0568 loss_val: 0.0571\n",
      "Epoch: 4634 loss_train: 0.0550 loss_val: 0.0572\n",
      "Epoch: 4635 loss_train: 0.0564 loss_val: 0.0559\n",
      "Epoch: 4636 loss_train: 0.0551 loss_val: 0.0567\n",
      "Epoch: 4637 loss_train: 0.0534 loss_val: 0.0553\n",
      "Epoch: 4638 loss_train: 0.0552 loss_val: 0.0561\n",
      "Epoch: 4639 loss_train: 0.0558 loss_val: 0.0564\n",
      "Epoch: 4640 loss_train: 0.0554 loss_val: 0.0549\n",
      "Epoch: 4641 loss_train: 0.0559 loss_val: 0.0579\n",
      "Epoch: 4642 loss_train: 0.0574 loss_val: 0.0560\n",
      "Epoch: 4643 loss_train: 0.0565 loss_val: 0.0578\n",
      "Epoch: 4644 loss_train: 0.0556 loss_val: 0.0580\n",
      "Epoch: 4645 loss_train: 0.0566 loss_val: 0.0556\n",
      "Epoch: 4646 loss_train: 0.0559 loss_val: 0.0551\n",
      "Epoch: 4647 loss_train: 0.0554 loss_val: 0.0557\n",
      "Epoch: 4648 loss_train: 0.0554 loss_val: 0.0543\n",
      "Epoch: 4649 loss_train: 0.0545 loss_val: 0.0539\n",
      "Epoch: 4650 loss_train: 0.0554 loss_val: 0.0537\n",
      "Epoch: 4651 loss_train: 0.0558 loss_val: 0.0547\n",
      "Epoch: 4652 loss_train: 0.0554 loss_val: 0.0540\n",
      "Epoch: 4653 loss_train: 0.0563 loss_val: 0.0550\n",
      "Epoch: 4654 loss_train: 0.0568 loss_val: 0.0534\n",
      "Epoch: 4655 loss_train: 0.0565 loss_val: 0.0551\n",
      "Epoch: 4656 loss_train: 0.0563 loss_val: 0.0547\n",
      "Epoch: 4657 loss_train: 0.0564 loss_val: 0.0536\n",
      "Epoch: 4658 loss_train: 0.0567 loss_val: 0.0559\n",
      "Epoch: 4659 loss_train: 0.0558 loss_val: 0.0542\n",
      "Epoch: 4660 loss_train: 0.0570 loss_val: 0.0547\n",
      "Epoch: 4661 loss_train: 0.0552 loss_val: 0.0542\n",
      "Epoch: 4662 loss_train: 0.0553 loss_val: 0.0530\n",
      "Epoch: 4663 loss_train: 0.0557 loss_val: 0.0572\n",
      "Epoch: 4664 loss_train: 0.0567 loss_val: 0.0574\n",
      "Epoch: 4665 loss_train: 0.0573 loss_val: 0.0569\n",
      "Epoch: 4666 loss_train: 0.0566 loss_val: 0.0551\n",
      "Epoch: 4667 loss_train: 0.0549 loss_val: 0.0555\n",
      "Epoch: 4668 loss_train: 0.0562 loss_val: 0.0580\n",
      "Epoch: 4669 loss_train: 0.0555 loss_val: 0.0581\n",
      "Epoch: 4670 loss_train: 0.0562 loss_val: 0.0564\n",
      "Epoch: 4671 loss_train: 0.0556 loss_val: 0.0560\n",
      "Epoch: 4672 loss_train: 0.0563 loss_val: 0.0553\n",
      "Epoch: 4673 loss_train: 0.0559 loss_val: 0.0551\n",
      "Epoch: 4674 loss_train: 0.0568 loss_val: 0.0563\n",
      "Epoch: 4675 loss_train: 0.0556 loss_val: 0.0563\n",
      "Epoch: 4676 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 4677 loss_train: 0.0560 loss_val: 0.0540\n",
      "Epoch: 4678 loss_train: 0.0564 loss_val: 0.0544\n",
      "Epoch: 4679 loss_train: 0.0557 loss_val: 0.0559\n",
      "Epoch: 4680 loss_train: 0.0559 loss_val: 0.0542\n",
      "Epoch: 4681 loss_train: 0.0560 loss_val: 0.0564\n",
      "Epoch: 4682 loss_train: 0.0553 loss_val: 0.0559\n",
      "Epoch: 4683 loss_train: 0.0545 loss_val: 0.0573\n",
      "Epoch: 4684 loss_train: 0.0568 loss_val: 0.0546\n",
      "Epoch: 4685 loss_train: 0.0555 loss_val: 0.0577\n",
      "Epoch: 4686 loss_train: 0.0562 loss_val: 0.0560\n",
      "Epoch: 4687 loss_train: 0.0559 loss_val: 0.0552\n",
      "Epoch: 4688 loss_train: 0.0550 loss_val: 0.0542\n",
      "Epoch: 4689 loss_train: 0.0547 loss_val: 0.0565\n",
      "Epoch: 4690 loss_train: 0.0562 loss_val: 0.0536\n",
      "Epoch: 4691 loss_train: 0.0552 loss_val: 0.0540\n",
      "Epoch: 4692 loss_train: 0.0550 loss_val: 0.0556\n",
      "Epoch: 4693 loss_train: 0.0561 loss_val: 0.0551\n",
      "Epoch: 4694 loss_train: 0.0566 loss_val: 0.0553\n",
      "Epoch: 4695 loss_train: 0.0560 loss_val: 0.0554\n",
      "Epoch: 4696 loss_train: 0.0568 loss_val: 0.0595\n",
      "Epoch: 4697 loss_train: 0.0561 loss_val: 0.0563\n",
      "Epoch: 4698 loss_train: 0.0565 loss_val: 0.0554\n",
      "Epoch: 4699 loss_train: 0.0565 loss_val: 0.0569\n",
      "Epoch: 4700 loss_train: 0.0570 loss_val: 0.0570\n",
      "Epoch: 4701 loss_train: 0.0554 loss_val: 0.0556\n",
      "Epoch: 4702 loss_train: 0.0564 loss_val: 0.0557\n",
      "Epoch: 4703 loss_train: 0.0558 loss_val: 0.0554\n",
      "Epoch: 4704 loss_train: 0.0553 loss_val: 0.0575\n",
      "Epoch: 4705 loss_train: 0.0565 loss_val: 0.0561\n",
      "Epoch: 4706 loss_train: 0.0554 loss_val: 0.0562\n",
      "Epoch: 4707 loss_train: 0.0554 loss_val: 0.0585\n",
      "Epoch: 4708 loss_train: 0.0557 loss_val: 0.0575\n",
      "Epoch: 4709 loss_train: 0.0555 loss_val: 0.0593\n",
      "Epoch: 4710 loss_train: 0.0555 loss_val: 0.0555\n",
      "Epoch: 4711 loss_train: 0.0569 loss_val: 0.0564\n",
      "Epoch: 4712 loss_train: 0.0555 loss_val: 0.0570\n",
      "Epoch: 4713 loss_train: 0.0575 loss_val: 0.0574\n",
      "Epoch: 4714 loss_train: 0.0582 loss_val: 0.0584\n",
      "Epoch: 4715 loss_train: 0.0562 loss_val: 0.0552\n",
      "Epoch: 4716 loss_train: 0.0551 loss_val: 0.0567\n",
      "Epoch: 4717 loss_train: 0.0563 loss_val: 0.0570\n",
      "Epoch: 4718 loss_train: 0.0556 loss_val: 0.0562\n",
      "Epoch: 4719 loss_train: 0.0563 loss_val: 0.0570\n",
      "Epoch: 4720 loss_train: 0.0555 loss_val: 0.0565\n",
      "Epoch: 4721 loss_train: 0.0551 loss_val: 0.0560\n",
      "Epoch: 4722 loss_train: 0.0563 loss_val: 0.0561\n",
      "Epoch: 4723 loss_train: 0.0548 loss_val: 0.0563\n",
      "Epoch: 4724 loss_train: 0.0556 loss_val: 0.0548\n",
      "Epoch: 4725 loss_train: 0.0549 loss_val: 0.0582\n",
      "Epoch: 4726 loss_train: 0.0571 loss_val: 0.0563\n",
      "Epoch: 4727 loss_train: 0.0562 loss_val: 0.0546\n",
      "Epoch: 4728 loss_train: 0.0557 loss_val: 0.0569\n",
      "Epoch: 4729 loss_train: 0.0576 loss_val: 0.0553\n",
      "Epoch: 4730 loss_train: 0.0556 loss_val: 0.0547\n",
      "Epoch: 4731 loss_train: 0.0570 loss_val: 0.0553\n",
      "Epoch: 4732 loss_train: 0.0561 loss_val: 0.0556\n",
      "Epoch: 4733 loss_train: 0.0570 loss_val: 0.0553\n",
      "Epoch: 4734 loss_train: 0.0556 loss_val: 0.0553\n",
      "Epoch: 4735 loss_train: 0.0555 loss_val: 0.0577\n",
      "Epoch: 4736 loss_train: 0.0573 loss_val: 0.0547\n",
      "Epoch: 4737 loss_train: 0.0560 loss_val: 0.0551\n",
      "Epoch: 4738 loss_train: 0.0563 loss_val: 0.0550\n",
      "Epoch: 4739 loss_train: 0.0557 loss_val: 0.0560\n",
      "Epoch: 4740 loss_train: 0.0550 loss_val: 0.0547\n",
      "Epoch: 4741 loss_train: 0.0556 loss_val: 0.0562\n",
      "Epoch: 4742 loss_train: 0.0551 loss_val: 0.0564\n",
      "Epoch: 4743 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 4744 loss_train: 0.0550 loss_val: 0.0554\n",
      "Epoch: 4745 loss_train: 0.0559 loss_val: 0.0564\n",
      "Epoch: 4746 loss_train: 0.0556 loss_val: 0.0563\n",
      "Epoch: 4747 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 4748 loss_train: 0.0569 loss_val: 0.0563\n",
      "Epoch: 4749 loss_train: 0.0558 loss_val: 0.0567\n",
      "Epoch: 4750 loss_train: 0.0570 loss_val: 0.0572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4751 loss_train: 0.0562 loss_val: 0.0557\n",
      "Epoch: 4752 loss_train: 0.0560 loss_val: 0.0570\n",
      "Epoch: 4753 loss_train: 0.0553 loss_val: 0.0546\n",
      "Epoch: 4754 loss_train: 0.0558 loss_val: 0.0554\n",
      "Epoch: 4755 loss_train: 0.0562 loss_val: 0.0566\n",
      "Epoch: 4756 loss_train: 0.0555 loss_val: 0.0539\n",
      "Epoch: 4757 loss_train: 0.0549 loss_val: 0.0542\n",
      "Epoch: 4758 loss_train: 0.0552 loss_val: 0.0564\n",
      "Epoch: 4759 loss_train: 0.0564 loss_val: 0.0562\n",
      "Epoch: 4760 loss_train: 0.0558 loss_val: 0.0559\n",
      "Epoch: 4761 loss_train: 0.0542 loss_val: 0.0558\n",
      "Epoch: 4762 loss_train: 0.0541 loss_val: 0.0548\n",
      "Epoch: 4763 loss_train: 0.0549 loss_val: 0.0571\n",
      "Epoch: 4764 loss_train: 0.0564 loss_val: 0.0567\n",
      "Epoch: 4765 loss_train: 0.0562 loss_val: 0.0558\n",
      "Epoch: 4766 loss_train: 0.0550 loss_val: 0.0555\n",
      "Epoch: 4767 loss_train: 0.0558 loss_val: 0.0555\n",
      "Epoch: 4768 loss_train: 0.0553 loss_val: 0.0547\n",
      "Epoch: 4769 loss_train: 0.0573 loss_val: 0.0549\n",
      "Epoch: 4770 loss_train: 0.0553 loss_val: 0.0547\n",
      "Epoch: 4771 loss_train: 0.0556 loss_val: 0.0575\n",
      "Epoch: 4772 loss_train: 0.0553 loss_val: 0.0563\n",
      "Epoch: 4773 loss_train: 0.0555 loss_val: 0.0557\n",
      "Epoch: 4774 loss_train: 0.0551 loss_val: 0.0570\n",
      "Epoch: 4775 loss_train: 0.0555 loss_val: 0.0567\n",
      "Epoch: 4776 loss_train: 0.0548 loss_val: 0.0584\n",
      "Epoch: 4777 loss_train: 0.0545 loss_val: 0.0574\n",
      "Epoch: 4778 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 4779 loss_train: 0.0567 loss_val: 0.0571\n",
      "Epoch: 4780 loss_train: 0.0569 loss_val: 0.0586\n",
      "Epoch: 4781 loss_train: 0.0568 loss_val: 0.0575\n",
      "Epoch: 4782 loss_train: 0.0545 loss_val: 0.0559\n",
      "Epoch: 4783 loss_train: 0.0564 loss_val: 0.0563\n",
      "Epoch: 4784 loss_train: 0.0558 loss_val: 0.0566\n",
      "Epoch: 4785 loss_train: 0.0566 loss_val: 0.0591\n",
      "Epoch: 4786 loss_train: 0.0563 loss_val: 0.0584\n",
      "Epoch: 4787 loss_train: 0.0564 loss_val: 0.0588\n",
      "Epoch: 4788 loss_train: 0.0571 loss_val: 0.0568\n",
      "Epoch: 4789 loss_train: 0.0549 loss_val: 0.0553\n",
      "Epoch: 4790 loss_train: 0.0549 loss_val: 0.0568\n",
      "Epoch: 4791 loss_train: 0.0546 loss_val: 0.0566\n",
      "Epoch: 4792 loss_train: 0.0552 loss_val: 0.0573\n",
      "Epoch: 4793 loss_train: 0.0551 loss_val: 0.0566\n",
      "Epoch: 4794 loss_train: 0.0552 loss_val: 0.0566\n",
      "Epoch: 4795 loss_train: 0.0564 loss_val: 0.0562\n",
      "Epoch: 4796 loss_train: 0.0566 loss_val: 0.0570\n",
      "Epoch: 4797 loss_train: 0.0557 loss_val: 0.0580\n",
      "Epoch: 4798 loss_train: 0.0559 loss_val: 0.0571\n",
      "Epoch: 4799 loss_train: 0.0567 loss_val: 0.0549\n",
      "Epoch: 4800 loss_train: 0.0555 loss_val: 0.0545\n",
      "Epoch: 4801 loss_train: 0.0560 loss_val: 0.0565\n",
      "Epoch: 4802 loss_train: 0.0564 loss_val: 0.0553\n",
      "Epoch: 4803 loss_train: 0.0564 loss_val: 0.0563\n",
      "Epoch: 4804 loss_train: 0.0557 loss_val: 0.0573\n",
      "Epoch: 4805 loss_train: 0.0560 loss_val: 0.0560\n",
      "Epoch: 4806 loss_train: 0.0562 loss_val: 0.0575\n",
      "Epoch: 4807 loss_train: 0.0549 loss_val: 0.0573\n",
      "Epoch: 4808 loss_train: 0.0554 loss_val: 0.0578\n",
      "Epoch: 4809 loss_train: 0.0558 loss_val: 0.0580\n",
      "Epoch: 4810 loss_train: 0.0563 loss_val: 0.0576\n",
      "Epoch: 4811 loss_train: 0.0552 loss_val: 0.0550\n",
      "Epoch: 4812 loss_train: 0.0546 loss_val: 0.0558\n",
      "Epoch: 4813 loss_train: 0.0578 loss_val: 0.0568\n",
      "Epoch: 4814 loss_train: 0.0559 loss_val: 0.0557\n",
      "Epoch: 4815 loss_train: 0.0563 loss_val: 0.0557\n",
      "Epoch: 4816 loss_train: 0.0554 loss_val: 0.0581\n",
      "Epoch: 4817 loss_train: 0.0548 loss_val: 0.0553\n",
      "Epoch: 4818 loss_train: 0.0556 loss_val: 0.0549\n",
      "Epoch: 4819 loss_train: 0.0554 loss_val: 0.0547\n",
      "Epoch: 4820 loss_train: 0.0547 loss_val: 0.0553\n",
      "Epoch: 4821 loss_train: 0.0560 loss_val: 0.0555\n",
      "Epoch: 4822 loss_train: 0.0552 loss_val: 0.0556\n",
      "Epoch: 4823 loss_train: 0.0555 loss_val: 0.0565\n",
      "Epoch: 4824 loss_train: 0.0555 loss_val: 0.0545\n",
      "Epoch: 4825 loss_train: 0.0546 loss_val: 0.0547\n",
      "Epoch: 4826 loss_train: 0.0543 loss_val: 0.0545\n",
      "Epoch: 4827 loss_train: 0.0563 loss_val: 0.0580\n",
      "Epoch: 4828 loss_train: 0.0567 loss_val: 0.0559\n",
      "Epoch: 4829 loss_train: 0.0564 loss_val: 0.0544\n",
      "Epoch: 4830 loss_train: 0.0566 loss_val: 0.0540\n",
      "Epoch: 4831 loss_train: 0.0556 loss_val: 0.0530\n",
      "Epoch: 4832 loss_train: 0.0566 loss_val: 0.0536\n",
      "Epoch: 4833 loss_train: 0.0573 loss_val: 0.0562\n",
      "Epoch: 4834 loss_train: 0.0559 loss_val: 0.0565\n",
      "Epoch: 4835 loss_train: 0.0556 loss_val: 0.0578\n",
      "Epoch: 4836 loss_train: 0.0569 loss_val: 0.0585\n",
      "Epoch: 4837 loss_train: 0.0564 loss_val: 0.0565\n",
      "Epoch: 4838 loss_train: 0.0563 loss_val: 0.0539\n",
      "Epoch: 4839 loss_train: 0.0549 loss_val: 0.0555\n",
      "Epoch: 4840 loss_train: 0.0568 loss_val: 0.0556\n",
      "Epoch: 4841 loss_train: 0.0563 loss_val: 0.0552\n",
      "Epoch: 4842 loss_train: 0.0557 loss_val: 0.0540\n",
      "Epoch: 4843 loss_train: 0.0554 loss_val: 0.0541\n",
      "Epoch: 4844 loss_train: 0.0562 loss_val: 0.0537\n",
      "Epoch: 4845 loss_train: 0.0559 loss_val: 0.0553\n",
      "Epoch: 4846 loss_train: 0.0556 loss_val: 0.0556\n",
      "Epoch: 4847 loss_train: 0.0559 loss_val: 0.0541\n",
      "Epoch: 4848 loss_train: 0.0556 loss_val: 0.0585\n",
      "Epoch: 4849 loss_train: 0.0558 loss_val: 0.0560\n",
      "Epoch: 4850 loss_train: 0.0548 loss_val: 0.0569\n",
      "Epoch: 4851 loss_train: 0.0566 loss_val: 0.0565\n",
      "Epoch: 4852 loss_train: 0.0567 loss_val: 0.0565\n",
      "Epoch: 4853 loss_train: 0.0564 loss_val: 0.0568\n",
      "Epoch: 4854 loss_train: 0.0575 loss_val: 0.0550\n",
      "Epoch: 4855 loss_train: 0.0561 loss_val: 0.0544\n",
      "Epoch: 4856 loss_train: 0.0554 loss_val: 0.0560\n",
      "Epoch: 4857 loss_train: 0.0553 loss_val: 0.0568\n",
      "Epoch: 4858 loss_train: 0.0572 loss_val: 0.0554\n",
      "Epoch: 4859 loss_train: 0.0565 loss_val: 0.0577\n",
      "Epoch: 4860 loss_train: 0.0565 loss_val: 0.0574\n",
      "Epoch: 4861 loss_train: 0.0558 loss_val: 0.0559\n",
      "Epoch: 4862 loss_train: 0.0555 loss_val: 0.0563\n",
      "Epoch: 4863 loss_train: 0.0552 loss_val: 0.0559\n",
      "Epoch: 4864 loss_train: 0.0560 loss_val: 0.0575\n",
      "Epoch: 4865 loss_train: 0.0574 loss_val: 0.0574\n",
      "Epoch: 4866 loss_train: 0.0564 loss_val: 0.0575\n",
      "Epoch: 4867 loss_train: 0.0563 loss_val: 0.0580\n",
      "Epoch: 4868 loss_train: 0.0558 loss_val: 0.0582\n",
      "Epoch: 4869 loss_train: 0.0575 loss_val: 0.0555\n",
      "Epoch: 4870 loss_train: 0.0561 loss_val: 0.0561\n",
      "Epoch: 4871 loss_train: 0.0556 loss_val: 0.0593\n",
      "Epoch: 4872 loss_train: 0.0562 loss_val: 0.0575\n",
      "Epoch: 4873 loss_train: 0.0555 loss_val: 0.0565\n",
      "Epoch: 4874 loss_train: 0.0552 loss_val: 0.0563\n",
      "Epoch: 4875 loss_train: 0.0566 loss_val: 0.0573\n",
      "Epoch: 4876 loss_train: 0.0567 loss_val: 0.0572\n",
      "Epoch: 4877 loss_train: 0.0556 loss_val: 0.0555\n",
      "Epoch: 4878 loss_train: 0.0568 loss_val: 0.0566\n",
      "Epoch: 4879 loss_train: 0.0559 loss_val: 0.0557\n",
      "Epoch: 4880 loss_train: 0.0562 loss_val: 0.0543\n",
      "Epoch: 4881 loss_train: 0.0559 loss_val: 0.0563\n",
      "Epoch: 4882 loss_train: 0.0567 loss_val: 0.0537\n",
      "Epoch: 4883 loss_train: 0.0553 loss_val: 0.0566\n",
      "Epoch: 4884 loss_train: 0.0562 loss_val: 0.0552\n",
      "Epoch: 4885 loss_train: 0.0573 loss_val: 0.0557\n",
      "Epoch: 4886 loss_train: 0.0556 loss_val: 0.0547\n",
      "Epoch: 4887 loss_train: 0.0558 loss_val: 0.0547\n",
      "Epoch: 4888 loss_train: 0.0562 loss_val: 0.0561\n",
      "Epoch: 4889 loss_train: 0.0566 loss_val: 0.0547\n",
      "Epoch: 4890 loss_train: 0.0566 loss_val: 0.0545\n",
      "Epoch: 4891 loss_train: 0.0570 loss_val: 0.0559\n",
      "Epoch: 4892 loss_train: 0.0560 loss_val: 0.0556\n",
      "Epoch: 4893 loss_train: 0.0568 loss_val: 0.0587\n",
      "Epoch: 4894 loss_train: 0.0553 loss_val: 0.0591\n",
      "Epoch: 4895 loss_train: 0.0545 loss_val: 0.0575\n",
      "Epoch: 4896 loss_train: 0.0554 loss_val: 0.0603\n",
      "Epoch: 4897 loss_train: 0.0566 loss_val: 0.0573\n",
      "Epoch: 4898 loss_train: 0.0561 loss_val: 0.0584\n",
      "Epoch: 4899 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 4900 loss_train: 0.0552 loss_val: 0.0565\n",
      "Epoch: 4901 loss_train: 0.0550 loss_val: 0.0557\n",
      "Epoch: 4902 loss_train: 0.0546 loss_val: 0.0556\n",
      "Epoch: 4903 loss_train: 0.0563 loss_val: 0.0550\n",
      "Epoch: 4904 loss_train: 0.0566 loss_val: 0.0545\n",
      "Epoch: 4905 loss_train: 0.0569 loss_val: 0.0552\n",
      "Epoch: 4906 loss_train: 0.0563 loss_val: 0.0561\n",
      "Epoch: 4907 loss_train: 0.0552 loss_val: 0.0552\n",
      "Epoch: 4908 loss_train: 0.0569 loss_val: 0.0539\n",
      "Epoch: 4909 loss_train: 0.0551 loss_val: 0.0573\n",
      "Epoch: 4910 loss_train: 0.0565 loss_val: 0.0567\n",
      "Epoch: 4911 loss_train: 0.0558 loss_val: 0.0581\n",
      "Epoch: 4912 loss_train: 0.0556 loss_val: 0.0563\n",
      "Epoch: 4913 loss_train: 0.0565 loss_val: 0.0558\n",
      "Epoch: 4914 loss_train: 0.0566 loss_val: 0.0568\n",
      "Epoch: 4915 loss_train: 0.0555 loss_val: 0.0574\n",
      "Epoch: 4916 loss_train: 0.0556 loss_val: 0.0568\n",
      "Epoch: 4917 loss_train: 0.0546 loss_val: 0.0554\n",
      "Epoch: 4918 loss_train: 0.0558 loss_val: 0.0567\n",
      "Epoch: 4919 loss_train: 0.0552 loss_val: 0.0563\n",
      "Epoch: 4920 loss_train: 0.0551 loss_val: 0.0572\n",
      "Epoch: 4921 loss_train: 0.0548 loss_val: 0.0586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4922 loss_train: 0.0553 loss_val: 0.0549\n",
      "Epoch: 4923 loss_train: 0.0535 loss_val: 0.0587\n",
      "Epoch: 4924 loss_train: 0.0561 loss_val: 0.0608\n",
      "Epoch: 4925 loss_train: 0.0570 loss_val: 0.0573\n",
      "Epoch: 4926 loss_train: 0.0559 loss_val: 0.0573\n",
      "Epoch: 4927 loss_train: 0.0549 loss_val: 0.0590\n",
      "Epoch: 4928 loss_train: 0.0551 loss_val: 0.0578\n",
      "Epoch: 4929 loss_train: 0.0563 loss_val: 0.0567\n",
      "Epoch: 4930 loss_train: 0.0566 loss_val: 0.0543\n",
      "Epoch: 4931 loss_train: 0.0551 loss_val: 0.0569\n",
      "Epoch: 4932 loss_train: 0.0557 loss_val: 0.0563\n",
      "Epoch: 4933 loss_train: 0.0550 loss_val: 0.0563\n",
      "Epoch: 4934 loss_train: 0.0558 loss_val: 0.0556\n",
      "Epoch: 4935 loss_train: 0.0560 loss_val: 0.0568\n",
      "Epoch: 4936 loss_train: 0.0547 loss_val: 0.0558\n",
      "Epoch: 4937 loss_train: 0.0561 loss_val: 0.0564\n",
      "Epoch: 4938 loss_train: 0.0555 loss_val: 0.0556\n",
      "Epoch: 4939 loss_train: 0.0545 loss_val: 0.0570\n",
      "Epoch: 4940 loss_train: 0.0544 loss_val: 0.0553\n",
      "Epoch: 4941 loss_train: 0.0552 loss_val: 0.0568\n",
      "Epoch: 4942 loss_train: 0.0560 loss_val: 0.0566\n",
      "Epoch: 4943 loss_train: 0.0546 loss_val: 0.0568\n",
      "Epoch: 4944 loss_train: 0.0569 loss_val: 0.0562\n",
      "Epoch: 4945 loss_train: 0.0562 loss_val: 0.0548\n",
      "Epoch: 4946 loss_train: 0.0559 loss_val: 0.0551\n",
      "Epoch: 4947 loss_train: 0.0556 loss_val: 0.0568\n",
      "Epoch: 4948 loss_train: 0.0575 loss_val: 0.0566\n",
      "Epoch: 4949 loss_train: 0.0569 loss_val: 0.0548\n",
      "Epoch: 4950 loss_train: 0.0562 loss_val: 0.0563\n",
      "Epoch: 4951 loss_train: 0.0556 loss_val: 0.0556\n",
      "Epoch: 4952 loss_train: 0.0557 loss_val: 0.0550\n",
      "Epoch: 4953 loss_train: 0.0544 loss_val: 0.0556\n",
      "Epoch: 4954 loss_train: 0.0552 loss_val: 0.0577\n",
      "Epoch: 4955 loss_train: 0.0555 loss_val: 0.0573\n",
      "Epoch: 4956 loss_train: 0.0556 loss_val: 0.0581\n",
      "Epoch: 4957 loss_train: 0.0564 loss_val: 0.0558\n",
      "Epoch: 4958 loss_train: 0.0549 loss_val: 0.0558\n",
      "Epoch: 4959 loss_train: 0.0562 loss_val: 0.0565\n",
      "Epoch: 4960 loss_train: 0.0553 loss_val: 0.0561\n",
      "Epoch: 4961 loss_train: 0.0545 loss_val: 0.0547\n",
      "Epoch: 4962 loss_train: 0.0565 loss_val: 0.0560\n",
      "Epoch: 4963 loss_train: 0.0555 loss_val: 0.0567\n",
      "Epoch: 4964 loss_train: 0.0552 loss_val: 0.0568\n",
      "Epoch: 4965 loss_train: 0.0542 loss_val: 0.0575\n",
      "Epoch: 4966 loss_train: 0.0551 loss_val: 0.0554\n",
      "Epoch: 4967 loss_train: 0.0566 loss_val: 0.0559\n",
      "Epoch: 4968 loss_train: 0.0559 loss_val: 0.0563\n",
      "Epoch: 4969 loss_train: 0.0556 loss_val: 0.0585\n",
      "Epoch: 4970 loss_train: 0.0569 loss_val: 0.0569\n",
      "Epoch: 4971 loss_train: 0.0569 loss_val: 0.0570\n",
      "Epoch: 4972 loss_train: 0.0553 loss_val: 0.0556\n",
      "Epoch: 4973 loss_train: 0.0561 loss_val: 0.0562\n",
      "Epoch: 4974 loss_train: 0.0557 loss_val: 0.0577\n",
      "Epoch: 4975 loss_train: 0.0554 loss_val: 0.0576\n",
      "Epoch: 4976 loss_train: 0.0553 loss_val: 0.0579\n",
      "Epoch: 4977 loss_train: 0.0556 loss_val: 0.0575\n",
      "Epoch: 4978 loss_train: 0.0572 loss_val: 0.0557\n",
      "Epoch: 4979 loss_train: 0.0555 loss_val: 0.0564\n",
      "Epoch: 4980 loss_train: 0.0553 loss_val: 0.0561\n",
      "Epoch: 4981 loss_train: 0.0556 loss_val: 0.0562\n",
      "Epoch: 4982 loss_train: 0.0560 loss_val: 0.0584\n",
      "Epoch: 4983 loss_train: 0.0555 loss_val: 0.0563\n",
      "Epoch: 4984 loss_train: 0.0563 loss_val: 0.0567\n",
      "Epoch: 4985 loss_train: 0.0563 loss_val: 0.0569\n",
      "Epoch: 4986 loss_train: 0.0538 loss_val: 0.0580\n",
      "Epoch: 4987 loss_train: 0.0550 loss_val: 0.0591\n",
      "Epoch: 4988 loss_train: 0.0559 loss_val: 0.0569\n",
      "Epoch: 4989 loss_train: 0.0553 loss_val: 0.0579\n",
      "Epoch: 4990 loss_train: 0.0554 loss_val: 0.0593\n",
      "Epoch: 4991 loss_train: 0.0562 loss_val: 0.0564\n",
      "Epoch: 4992 loss_train: 0.0569 loss_val: 0.0546\n",
      "Epoch: 4993 loss_train: 0.0546 loss_val: 0.0563\n",
      "Epoch: 4994 loss_train: 0.0544 loss_val: 0.0580\n",
      "Epoch: 4995 loss_train: 0.0558 loss_val: 0.0570\n",
      "Epoch: 4996 loss_train: 0.0550 loss_val: 0.0583\n",
      "Epoch: 4997 loss_train: 0.0563 loss_val: 0.0556\n",
      "Epoch: 4998 loss_train: 0.0557 loss_val: 0.0562\n",
      "Epoch: 4999 loss_train: 0.0554 loss_val: 0.0578\n",
      "Epoch: 5000 loss_train: 0.0551 loss_val: 0.0555\n",
      "Epoch: 5001 loss_train: 0.0566 loss_val: 0.0582\n",
      "Epoch: 5002 loss_train: 0.0565 loss_val: 0.0558\n",
      "Epoch: 5003 loss_train: 0.0559 loss_val: 0.0571\n",
      "Epoch: 5004 loss_train: 0.0557 loss_val: 0.0567\n",
      "Epoch: 5005 loss_train: 0.0550 loss_val: 0.0571\n",
      "Epoch: 5006 loss_train: 0.0549 loss_val: 0.0535\n",
      "Epoch: 5007 loss_train: 0.0559 loss_val: 0.0549\n",
      "Epoch: 5008 loss_train: 0.0550 loss_val: 0.0545\n",
      "Epoch: 5009 loss_train: 0.0558 loss_val: 0.0545\n",
      "Epoch: 5010 loss_train: 0.0557 loss_val: 0.0559\n",
      "Epoch: 5011 loss_train: 0.0551 loss_val: 0.0568\n",
      "Epoch: 5012 loss_train: 0.0551 loss_val: 0.0563\n",
      "Epoch: 5013 loss_train: 0.0551 loss_val: 0.0575\n",
      "Epoch: 5014 loss_train: 0.0562 loss_val: 0.0557\n",
      "Epoch: 5015 loss_train: 0.0557 loss_val: 0.0551\n",
      "Epoch: 5016 loss_train: 0.0545 loss_val: 0.0577\n",
      "Epoch: 5017 loss_train: 0.0568 loss_val: 0.0549\n",
      "Epoch: 5018 loss_train: 0.0552 loss_val: 0.0553\n",
      "Epoch: 5019 loss_train: 0.0555 loss_val: 0.0565\n",
      "Epoch: 5020 loss_train: 0.0567 loss_val: 0.0559\n",
      "Epoch: 5021 loss_train: 0.0540 loss_val: 0.0575\n",
      "Epoch: 5022 loss_train: 0.0536 loss_val: 0.0582\n",
      "Epoch: 5023 loss_train: 0.0560 loss_val: 0.0552\n",
      "Epoch: 5024 loss_train: 0.0548 loss_val: 0.0567\n",
      "Epoch: 5025 loss_train: 0.0549 loss_val: 0.0569\n",
      "Epoch: 5026 loss_train: 0.0543 loss_val: 0.0575\n",
      "Epoch: 5027 loss_train: 0.0556 loss_val: 0.0581\n",
      "Epoch: 5028 loss_train: 0.0554 loss_val: 0.0567\n",
      "Epoch: 5029 loss_train: 0.0559 loss_val: 0.0562\n",
      "Epoch: 5030 loss_train: 0.0557 loss_val: 0.0558\n",
      "Epoch: 5031 loss_train: 0.0539 loss_val: 0.0551\n",
      "Epoch: 5032 loss_train: 0.0550 loss_val: 0.0557\n",
      "Epoch: 5033 loss_train: 0.0551 loss_val: 0.0558\n",
      "Epoch: 5034 loss_train: 0.0548 loss_val: 0.0546\n",
      "Epoch: 5035 loss_train: 0.0557 loss_val: 0.0559\n",
      "Epoch: 5036 loss_train: 0.0554 loss_val: 0.0558\n",
      "Epoch: 5037 loss_train: 0.0545 loss_val: 0.0552\n",
      "Epoch: 5038 loss_train: 0.0550 loss_val: 0.0572\n",
      "Epoch: 5039 loss_train: 0.0550 loss_val: 0.0561\n",
      "Epoch: 5040 loss_train: 0.0548 loss_val: 0.0571\n",
      "Epoch: 5041 loss_train: 0.0550 loss_val: 0.0588\n",
      "Epoch: 5042 loss_train: 0.0551 loss_val: 0.0598\n",
      "Epoch: 5043 loss_train: 0.0571 loss_val: 0.0590\n",
      "Epoch: 5044 loss_train: 0.0553 loss_val: 0.0562\n",
      "Epoch: 5045 loss_train: 0.0566 loss_val: 0.0568\n",
      "Epoch: 5046 loss_train: 0.0554 loss_val: 0.0541\n",
      "Epoch: 5047 loss_train: 0.0559 loss_val: 0.0545\n",
      "Epoch: 5048 loss_train: 0.0550 loss_val: 0.0563\n",
      "Epoch: 5049 loss_train: 0.0555 loss_val: 0.0567\n",
      "Epoch: 5050 loss_train: 0.0562 loss_val: 0.0558\n",
      "Epoch: 5051 loss_train: 0.0550 loss_val: 0.0548\n",
      "Epoch: 5052 loss_train: 0.0555 loss_val: 0.0554\n",
      "Epoch: 5053 loss_train: 0.0555 loss_val: 0.0564\n",
      "Epoch: 5054 loss_train: 0.0546 loss_val: 0.0574\n",
      "Epoch: 5055 loss_train: 0.0553 loss_val: 0.0555\n",
      "Epoch: 5056 loss_train: 0.0550 loss_val: 0.0580\n",
      "Epoch: 5057 loss_train: 0.0554 loss_val: 0.0564\n",
      "Epoch: 5058 loss_train: 0.0554 loss_val: 0.0540\n",
      "Epoch: 5059 loss_train: 0.0551 loss_val: 0.0564\n",
      "Epoch: 5060 loss_train: 0.0560 loss_val: 0.0550\n",
      "Epoch: 5061 loss_train: 0.0558 loss_val: 0.0544\n",
      "Epoch: 5062 loss_train: 0.0566 loss_val: 0.0557\n",
      "Epoch: 5063 loss_train: 0.0550 loss_val: 0.0572\n",
      "Epoch: 5064 loss_train: 0.0548 loss_val: 0.0567\n",
      "Epoch: 5065 loss_train: 0.0556 loss_val: 0.0566\n",
      "Epoch: 5066 loss_train: 0.0555 loss_val: 0.0547\n",
      "Epoch: 5067 loss_train: 0.0549 loss_val: 0.0557\n",
      "Epoch: 5068 loss_train: 0.0555 loss_val: 0.0573\n",
      "Epoch: 5069 loss_train: 0.0553 loss_val: 0.0577\n",
      "Epoch: 5070 loss_train: 0.0559 loss_val: 0.0555\n",
      "Epoch: 5071 loss_train: 0.0559 loss_val: 0.0565\n",
      "Epoch: 5072 loss_train: 0.0574 loss_val: 0.0579\n",
      "Epoch: 5073 loss_train: 0.0578 loss_val: 0.0566\n",
      "Epoch: 5074 loss_train: 0.0553 loss_val: 0.0541\n",
      "Epoch: 5075 loss_train: 0.0558 loss_val: 0.0535\n",
      "Epoch: 5076 loss_train: 0.0550 loss_val: 0.0551\n",
      "Epoch: 5077 loss_train: 0.0548 loss_val: 0.0559\n",
      "Epoch: 5078 loss_train: 0.0552 loss_val: 0.0553\n",
      "Epoch: 5079 loss_train: 0.0555 loss_val: 0.0554\n",
      "Epoch: 5080 loss_train: 0.0548 loss_val: 0.0560\n",
      "Epoch: 5081 loss_train: 0.0561 loss_val: 0.0584\n",
      "Epoch: 5082 loss_train: 0.0574 loss_val: 0.0571\n",
      "Epoch: 5083 loss_train: 0.0561 loss_val: 0.0562\n",
      "Epoch: 5084 loss_train: 0.0574 loss_val: 0.0578\n",
      "Epoch: 5085 loss_train: 0.0554 loss_val: 0.0580\n",
      "Epoch: 5086 loss_train: 0.0550 loss_val: 0.0580\n",
      "Epoch: 5087 loss_train: 0.0554 loss_val: 0.0592\n",
      "Epoch: 5088 loss_train: 0.0550 loss_val: 0.0575\n",
      "Epoch: 5089 loss_train: 0.0547 loss_val: 0.0578\n",
      "Epoch: 5090 loss_train: 0.0548 loss_val: 0.0568\n",
      "Epoch: 5091 loss_train: 0.0548 loss_val: 0.0554\n",
      "Epoch: 5092 loss_train: 0.0548 loss_val: 0.0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5093 loss_train: 0.0568 loss_val: 0.0563\n",
      "Epoch: 5094 loss_train: 0.0556 loss_val: 0.0570\n",
      "Epoch: 5095 loss_train: 0.0547 loss_val: 0.0539\n",
      "Epoch: 5096 loss_train: 0.0550 loss_val: 0.0556\n",
      "Epoch: 5097 loss_train: 0.0552 loss_val: 0.0575\n",
      "Epoch: 5098 loss_train: 0.0558 loss_val: 0.0570\n",
      "Epoch: 5099 loss_train: 0.0558 loss_val: 0.0549\n",
      "Epoch: 5100 loss_train: 0.0553 loss_val: 0.0578\n",
      "Epoch: 5101 loss_train: 0.0556 loss_val: 0.0558\n",
      "Epoch: 5102 loss_train: 0.0552 loss_val: 0.0581\n",
      "Epoch: 5103 loss_train: 0.0552 loss_val: 0.0568\n",
      "Epoch: 5104 loss_train: 0.0550 loss_val: 0.0555\n",
      "Epoch: 5105 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 5106 loss_train: 0.0556 loss_val: 0.0552\n",
      "Epoch: 5107 loss_train: 0.0552 loss_val: 0.0551\n",
      "Epoch: 5108 loss_train: 0.0545 loss_val: 0.0551\n",
      "Epoch: 5109 loss_train: 0.0553 loss_val: 0.0528\n",
      "Epoch: 5110 loss_train: 0.0549 loss_val: 0.0544\n",
      "Epoch: 5111 loss_train: 0.0553 loss_val: 0.0557\n",
      "Epoch: 5112 loss_train: 0.0570 loss_val: 0.0539\n",
      "Epoch: 5113 loss_train: 0.0549 loss_val: 0.0538\n",
      "Epoch: 5114 loss_train: 0.0548 loss_val: 0.0541\n",
      "Epoch: 5115 loss_train: 0.0558 loss_val: 0.0565\n",
      "Epoch: 5116 loss_train: 0.0572 loss_val: 0.0563\n",
      "Epoch: 5117 loss_train: 0.0550 loss_val: 0.0553\n",
      "Epoch: 5118 loss_train: 0.0562 loss_val: 0.0550\n",
      "Epoch: 5119 loss_train: 0.0561 loss_val: 0.0554\n",
      "Epoch: 5120 loss_train: 0.0560 loss_val: 0.0539\n",
      "Epoch: 5121 loss_train: 0.0563 loss_val: 0.0562\n",
      "Epoch: 5122 loss_train: 0.0556 loss_val: 0.0559\n",
      "Epoch: 5123 loss_train: 0.0545 loss_val: 0.0582\n",
      "Epoch: 5124 loss_train: 0.0550 loss_val: 0.0570\n",
      "Epoch: 5125 loss_train: 0.0546 loss_val: 0.0572\n",
      "Epoch: 5126 loss_train: 0.0551 loss_val: 0.0569\n",
      "Epoch: 5127 loss_train: 0.0556 loss_val: 0.0560\n",
      "Epoch: 5128 loss_train: 0.0555 loss_val: 0.0572\n",
      "Epoch: 5129 loss_train: 0.0545 loss_val: 0.0563\n",
      "Epoch: 5130 loss_train: 0.0547 loss_val: 0.0559\n",
      "Epoch: 5131 loss_train: 0.0552 loss_val: 0.0575\n",
      "Epoch: 5132 loss_train: 0.0555 loss_val: 0.0566\n",
      "Epoch: 5133 loss_train: 0.0546 loss_val: 0.0574\n",
      "Epoch: 5134 loss_train: 0.0552 loss_val: 0.0567\n",
      "Epoch: 5135 loss_train: 0.0554 loss_val: 0.0556\n",
      "Epoch: 5136 loss_train: 0.0537 loss_val: 0.0543\n",
      "Epoch: 5137 loss_train: 0.0564 loss_val: 0.0543\n",
      "Epoch: 5138 loss_train: 0.0561 loss_val: 0.0561\n",
      "Epoch: 5139 loss_train: 0.0559 loss_val: 0.0560\n",
      "Epoch: 5140 loss_train: 0.0553 loss_val: 0.0555\n",
      "Epoch: 5141 loss_train: 0.0554 loss_val: 0.0542\n",
      "Epoch: 5142 loss_train: 0.0553 loss_val: 0.0535\n",
      "Epoch: 5143 loss_train: 0.0540 loss_val: 0.0548\n",
      "Epoch: 5144 loss_train: 0.0541 loss_val: 0.0548\n",
      "Epoch: 5145 loss_train: 0.0552 loss_val: 0.0550\n",
      "Epoch: 5146 loss_train: 0.0559 loss_val: 0.0553\n",
      "Epoch: 5147 loss_train: 0.0570 loss_val: 0.0547\n",
      "Epoch: 5148 loss_train: 0.0553 loss_val: 0.0558\n",
      "Epoch: 5149 loss_train: 0.0555 loss_val: 0.0555\n",
      "Epoch: 5150 loss_train: 0.0564 loss_val: 0.0561\n",
      "Epoch: 5151 loss_train: 0.0558 loss_val: 0.0548\n",
      "Epoch: 5152 loss_train: 0.0542 loss_val: 0.0570\n",
      "Epoch: 5153 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 5154 loss_train: 0.0552 loss_val: 0.0557\n",
      "Epoch: 5155 loss_train: 0.0551 loss_val: 0.0542\n",
      "Epoch: 5156 loss_train: 0.0548 loss_val: 0.0555\n",
      "Epoch: 5157 loss_train: 0.0569 loss_val: 0.0567\n",
      "Epoch: 5158 loss_train: 0.0574 loss_val: 0.0579\n",
      "Epoch: 5159 loss_train: 0.0549 loss_val: 0.0568\n",
      "Epoch: 5160 loss_train: 0.0555 loss_val: 0.0569\n",
      "Epoch: 5161 loss_train: 0.0548 loss_val: 0.0554\n",
      "Epoch: 5162 loss_train: 0.0562 loss_val: 0.0577\n",
      "Epoch: 5163 loss_train: 0.0554 loss_val: 0.0574\n",
      "Epoch: 5164 loss_train: 0.0572 loss_val: 0.0588\n",
      "Epoch: 5165 loss_train: 0.0572 loss_val: 0.0566\n",
      "Epoch: 5166 loss_train: 0.0559 loss_val: 0.0559\n",
      "Epoch: 5167 loss_train: 0.0557 loss_val: 0.0567\n",
      "Epoch: 5168 loss_train: 0.0545 loss_val: 0.0558\n",
      "Epoch: 5169 loss_train: 0.0547 loss_val: 0.0587\n",
      "Epoch: 5170 loss_train: 0.0561 loss_val: 0.0550\n",
      "Epoch: 5171 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 5172 loss_train: 0.0555 loss_val: 0.0559\n",
      "Epoch: 5173 loss_train: 0.0544 loss_val: 0.0551\n",
      "Epoch: 5174 loss_train: 0.0560 loss_val: 0.0550\n",
      "Epoch: 5175 loss_train: 0.0559 loss_val: 0.0561\n",
      "Epoch: 5176 loss_train: 0.0560 loss_val: 0.0564\n",
      "Epoch: 5177 loss_train: 0.0560 loss_val: 0.0542\n",
      "Epoch: 5178 loss_train: 0.0556 loss_val: 0.0541\n",
      "Epoch: 5179 loss_train: 0.0552 loss_val: 0.0551\n",
      "Epoch: 5180 loss_train: 0.0547 loss_val: 0.0562\n",
      "Epoch: 5181 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 5182 loss_train: 0.0558 loss_val: 0.0556\n",
      "Epoch: 5183 loss_train: 0.0545 loss_val: 0.0555\n",
      "Epoch: 5184 loss_train: 0.0555 loss_val: 0.0549\n",
      "Epoch: 5185 loss_train: 0.0547 loss_val: 0.0553\n",
      "Epoch: 5186 loss_train: 0.0551 loss_val: 0.0552\n",
      "Epoch: 5187 loss_train: 0.0550 loss_val: 0.0557\n",
      "Epoch: 5188 loss_train: 0.0545 loss_val: 0.0561\n",
      "Epoch: 5189 loss_train: 0.0552 loss_val: 0.0559\n",
      "Epoch: 5190 loss_train: 0.0557 loss_val: 0.0562\n",
      "Epoch: 5191 loss_train: 0.0546 loss_val: 0.0551\n",
      "Epoch: 5192 loss_train: 0.0550 loss_val: 0.0571\n",
      "Epoch: 5193 loss_train: 0.0552 loss_val: 0.0577\n",
      "Epoch: 5194 loss_train: 0.0565 loss_val: 0.0575\n",
      "Epoch: 5195 loss_train: 0.0550 loss_val: 0.0561\n",
      "Epoch: 5196 loss_train: 0.0550 loss_val: 0.0544\n",
      "Epoch: 5197 loss_train: 0.0550 loss_val: 0.0573\n",
      "Epoch: 5198 loss_train: 0.0565 loss_val: 0.0560\n",
      "Epoch: 5199 loss_train: 0.0559 loss_val: 0.0545\n",
      "Epoch: 5200 loss_train: 0.0552 loss_val: 0.0554\n",
      "Epoch: 5201 loss_train: 0.0556 loss_val: 0.0561\n",
      "Epoch: 5202 loss_train: 0.0551 loss_val: 0.0552\n",
      "Epoch: 5203 loss_train: 0.0550 loss_val: 0.0565\n",
      "Epoch: 5204 loss_train: 0.0544 loss_val: 0.0559\n",
      "Epoch: 5205 loss_train: 0.0539 loss_val: 0.0567\n",
      "Epoch: 5206 loss_train: 0.0551 loss_val: 0.0559\n",
      "Epoch: 5207 loss_train: 0.0548 loss_val: 0.0565\n",
      "Epoch: 5208 loss_train: 0.0558 loss_val: 0.0539\n",
      "Epoch: 5209 loss_train: 0.0542 loss_val: 0.0559\n",
      "Epoch: 5210 loss_train: 0.0548 loss_val: 0.0547\n",
      "Epoch: 5211 loss_train: 0.0537 loss_val: 0.0546\n",
      "Epoch: 5212 loss_train: 0.0539 loss_val: 0.0565\n",
      "Epoch: 5213 loss_train: 0.0540 loss_val: 0.0554\n",
      "Epoch: 5214 loss_train: 0.0547 loss_val: 0.0568\n",
      "Epoch: 5215 loss_train: 0.0551 loss_val: 0.0559\n",
      "Epoch: 5216 loss_train: 0.0552 loss_val: 0.0579\n",
      "Epoch: 5217 loss_train: 0.0562 loss_val: 0.0553\n",
      "Epoch: 5218 loss_train: 0.0545 loss_val: 0.0573\n",
      "Epoch: 5219 loss_train: 0.0545 loss_val: 0.0566\n",
      "Epoch: 5220 loss_train: 0.0550 loss_val: 0.0566\n",
      "Epoch: 5221 loss_train: 0.0556 loss_val: 0.0552\n",
      "Epoch: 5222 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 5223 loss_train: 0.0546 loss_val: 0.0558\n",
      "Epoch: 5224 loss_train: 0.0555 loss_val: 0.0594\n",
      "Epoch: 5225 loss_train: 0.0567 loss_val: 0.0594\n",
      "Epoch: 5226 loss_train: 0.0553 loss_val: 0.0547\n",
      "Epoch: 5227 loss_train: 0.0549 loss_val: 0.0546\n",
      "Epoch: 5228 loss_train: 0.0546 loss_val: 0.0555\n",
      "Epoch: 5229 loss_train: 0.0557 loss_val: 0.0560\n",
      "Epoch: 5230 loss_train: 0.0547 loss_val: 0.0553\n",
      "Epoch: 5231 loss_train: 0.0553 loss_val: 0.0565\n",
      "Epoch: 5232 loss_train: 0.0558 loss_val: 0.0555\n",
      "Epoch: 5233 loss_train: 0.0553 loss_val: 0.0567\n",
      "Epoch: 5234 loss_train: 0.0563 loss_val: 0.0561\n",
      "Epoch: 5235 loss_train: 0.0553 loss_val: 0.0556\n",
      "Epoch: 5236 loss_train: 0.0560 loss_val: 0.0566\n",
      "Epoch: 5237 loss_train: 0.0554 loss_val: 0.0555\n",
      "Epoch: 5238 loss_train: 0.0539 loss_val: 0.0568\n",
      "Epoch: 5239 loss_train: 0.0556 loss_val: 0.0569\n",
      "Epoch: 5240 loss_train: 0.0551 loss_val: 0.0566\n",
      "Epoch: 5241 loss_train: 0.0551 loss_val: 0.0570\n",
      "Epoch: 5242 loss_train: 0.0543 loss_val: 0.0558\n",
      "Epoch: 5243 loss_train: 0.0544 loss_val: 0.0555\n",
      "Epoch: 5244 loss_train: 0.0544 loss_val: 0.0561\n",
      "Epoch: 5245 loss_train: 0.0542 loss_val: 0.0578\n",
      "Epoch: 5246 loss_train: 0.0551 loss_val: 0.0566\n",
      "Epoch: 5247 loss_train: 0.0557 loss_val: 0.0559\n",
      "Epoch: 5248 loss_train: 0.0551 loss_val: 0.0607\n",
      "Epoch: 5249 loss_train: 0.0566 loss_val: 0.0586\n",
      "Epoch: 5250 loss_train: 0.0562 loss_val: 0.0588\n",
      "Epoch: 5251 loss_train: 0.0550 loss_val: 0.0570\n",
      "Epoch: 5252 loss_train: 0.0548 loss_val: 0.0585\n",
      "Epoch: 5253 loss_train: 0.0550 loss_val: 0.0560\n",
      "Epoch: 5254 loss_train: 0.0552 loss_val: 0.0576\n",
      "Epoch: 5255 loss_train: 0.0541 loss_val: 0.0565\n",
      "Epoch: 5256 loss_train: 0.0552 loss_val: 0.0568\n",
      "Epoch: 5257 loss_train: 0.0560 loss_val: 0.0587\n",
      "Epoch: 5258 loss_train: 0.0563 loss_val: 0.0564\n",
      "Epoch: 5259 loss_train: 0.0551 loss_val: 0.0558\n",
      "Epoch: 5260 loss_train: 0.0542 loss_val: 0.0565\n",
      "Epoch: 5261 loss_train: 0.0537 loss_val: 0.0574\n",
      "Epoch: 5262 loss_train: 0.0538 loss_val: 0.0561\n",
      "Epoch: 5263 loss_train: 0.0552 loss_val: 0.0572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5264 loss_train: 0.0544 loss_val: 0.0573\n",
      "Epoch: 5265 loss_train: 0.0561 loss_val: 0.0579\n",
      "Epoch: 5266 loss_train: 0.0553 loss_val: 0.0571\n",
      "Epoch: 5267 loss_train: 0.0549 loss_val: 0.0558\n",
      "Epoch: 5268 loss_train: 0.0550 loss_val: 0.0567\n",
      "Epoch: 5269 loss_train: 0.0548 loss_val: 0.0568\n",
      "Epoch: 5270 loss_train: 0.0546 loss_val: 0.0556\n",
      "Epoch: 5271 loss_train: 0.0553 loss_val: 0.0556\n",
      "Epoch: 5272 loss_train: 0.0567 loss_val: 0.0585\n",
      "Epoch: 5273 loss_train: 0.0554 loss_val: 0.0552\n",
      "Epoch: 5274 loss_train: 0.0533 loss_val: 0.0581\n",
      "Epoch: 5275 loss_train: 0.0554 loss_val: 0.0578\n",
      "Epoch: 5276 loss_train: 0.0543 loss_val: 0.0571\n",
      "Epoch: 5277 loss_train: 0.0559 loss_val: 0.0569\n",
      "Epoch: 5278 loss_train: 0.0561 loss_val: 0.0568\n",
      "Epoch: 5279 loss_train: 0.0545 loss_val: 0.0573\n",
      "Epoch: 5280 loss_train: 0.0553 loss_val: 0.0553\n",
      "Epoch: 5281 loss_train: 0.0546 loss_val: 0.0563\n",
      "Epoch: 5282 loss_train: 0.0555 loss_val: 0.0580\n",
      "Epoch: 5283 loss_train: 0.0552 loss_val: 0.0584\n",
      "Epoch: 5284 loss_train: 0.0558 loss_val: 0.0569\n",
      "Epoch: 5285 loss_train: 0.0544 loss_val: 0.0555\n",
      "Epoch: 5286 loss_train: 0.0544 loss_val: 0.0551\n",
      "Epoch: 5287 loss_train: 0.0555 loss_val: 0.0550\n",
      "Epoch: 5288 loss_train: 0.0540 loss_val: 0.0537\n",
      "Epoch: 5289 loss_train: 0.0551 loss_val: 0.0564\n",
      "Epoch: 5290 loss_train: 0.0554 loss_val: 0.0560\n",
      "Epoch: 5291 loss_train: 0.0537 loss_val: 0.0541\n",
      "Epoch: 5292 loss_train: 0.0552 loss_val: 0.0553\n",
      "Epoch: 5293 loss_train: 0.0544 loss_val: 0.0544\n",
      "Epoch: 5294 loss_train: 0.0548 loss_val: 0.0552\n",
      "Epoch: 5295 loss_train: 0.0544 loss_val: 0.0571\n",
      "Epoch: 5296 loss_train: 0.0545 loss_val: 0.0536\n",
      "Epoch: 5297 loss_train: 0.0542 loss_val: 0.0554\n",
      "Epoch: 5298 loss_train: 0.0541 loss_val: 0.0545\n",
      "Epoch: 5299 loss_train: 0.0558 loss_val: 0.0591\n",
      "Epoch: 5300 loss_train: 0.0553 loss_val: 0.0560\n",
      "Epoch: 5301 loss_train: 0.0567 loss_val: 0.0555\n",
      "Epoch: 5302 loss_train: 0.0555 loss_val: 0.0545\n",
      "Epoch: 5303 loss_train: 0.0548 loss_val: 0.0565\n",
      "Epoch: 5304 loss_train: 0.0553 loss_val: 0.0561\n",
      "Epoch: 5305 loss_train: 0.0549 loss_val: 0.0570\n",
      "Epoch: 5306 loss_train: 0.0559 loss_val: 0.0577\n",
      "Epoch: 5307 loss_train: 0.0551 loss_val: 0.0581\n",
      "Epoch: 5308 loss_train: 0.0551 loss_val: 0.0549\n",
      "Epoch: 5309 loss_train: 0.0554 loss_val: 0.0584\n",
      "Epoch: 5310 loss_train: 0.0558 loss_val: 0.0570\n",
      "Epoch: 5311 loss_train: 0.0550 loss_val: 0.0567\n",
      "Epoch: 5312 loss_train: 0.0550 loss_val: 0.0567\n",
      "Epoch: 5313 loss_train: 0.0552 loss_val: 0.0577\n",
      "Epoch: 5314 loss_train: 0.0556 loss_val: 0.0572\n",
      "Epoch: 5315 loss_train: 0.0538 loss_val: 0.0588\n",
      "Epoch: 5316 loss_train: 0.0548 loss_val: 0.0546\n",
      "Epoch: 5317 loss_train: 0.0546 loss_val: 0.0545\n",
      "Epoch: 5318 loss_train: 0.0556 loss_val: 0.0587\n",
      "Epoch: 5319 loss_train: 0.0560 loss_val: 0.0562\n",
      "Epoch: 5320 loss_train: 0.0555 loss_val: 0.0562\n",
      "Epoch: 5321 loss_train: 0.0551 loss_val: 0.0564\n",
      "Epoch: 5322 loss_train: 0.0555 loss_val: 0.0564\n",
      "Epoch: 5323 loss_train: 0.0561 loss_val: 0.0566\n",
      "Epoch: 5324 loss_train: 0.0554 loss_val: 0.0570\n",
      "Epoch: 5325 loss_train: 0.0558 loss_val: 0.0556\n",
      "Epoch: 5326 loss_train: 0.0542 loss_val: 0.0567\n",
      "Epoch: 5327 loss_train: 0.0552 loss_val: 0.0556\n",
      "Epoch: 5328 loss_train: 0.0550 loss_val: 0.0535\n",
      "Epoch: 5329 loss_train: 0.0543 loss_val: 0.0561\n",
      "Epoch: 5330 loss_train: 0.0542 loss_val: 0.0570\n",
      "Epoch: 5331 loss_train: 0.0556 loss_val: 0.0574\n",
      "Epoch: 5332 loss_train: 0.0545 loss_val: 0.0580\n",
      "Epoch: 5333 loss_train: 0.0555 loss_val: 0.0584\n",
      "Epoch: 5334 loss_train: 0.0543 loss_val: 0.0568\n",
      "Epoch: 5335 loss_train: 0.0555 loss_val: 0.0546\n",
      "Epoch: 5336 loss_train: 0.0553 loss_val: 0.0556\n",
      "Epoch: 5337 loss_train: 0.0540 loss_val: 0.0580\n",
      "Epoch: 5338 loss_train: 0.0547 loss_val: 0.0580\n",
      "Epoch: 5339 loss_train: 0.0546 loss_val: 0.0556\n",
      "Epoch: 5340 loss_train: 0.0547 loss_val: 0.0561\n",
      "Epoch: 5341 loss_train: 0.0553 loss_val: 0.0561\n",
      "Epoch: 5342 loss_train: 0.0539 loss_val: 0.0558\n",
      "Epoch: 5343 loss_train: 0.0551 loss_val: 0.0573\n",
      "Epoch: 5344 loss_train: 0.0541 loss_val: 0.0596\n",
      "Epoch: 5345 loss_train: 0.0549 loss_val: 0.0567\n",
      "Epoch: 5346 loss_train: 0.0552 loss_val: 0.0559\n",
      "Epoch: 5347 loss_train: 0.0546 loss_val: 0.0577\n",
      "Epoch: 5348 loss_train: 0.0555 loss_val: 0.0573\n",
      "Epoch: 5349 loss_train: 0.0560 loss_val: 0.0564\n",
      "Epoch: 5350 loss_train: 0.0546 loss_val: 0.0561\n",
      "Epoch: 5351 loss_train: 0.0552 loss_val: 0.0555\n",
      "Epoch: 5352 loss_train: 0.0547 loss_val: 0.0576\n",
      "Epoch: 5353 loss_train: 0.0560 loss_val: 0.0553\n",
      "Epoch: 5354 loss_train: 0.0545 loss_val: 0.0561\n",
      "Epoch: 5355 loss_train: 0.0551 loss_val: 0.0555\n",
      "Epoch: 5356 loss_train: 0.0549 loss_val: 0.0553\n",
      "Epoch: 5357 loss_train: 0.0549 loss_val: 0.0561\n",
      "Epoch: 5358 loss_train: 0.0550 loss_val: 0.0574\n",
      "Epoch: 5359 loss_train: 0.0555 loss_val: 0.0578\n",
      "Epoch: 5360 loss_train: 0.0539 loss_val: 0.0562\n",
      "Epoch: 5361 loss_train: 0.0533 loss_val: 0.0558\n",
      "Epoch: 5362 loss_train: 0.0556 loss_val: 0.0560\n",
      "Epoch: 5363 loss_train: 0.0548 loss_val: 0.0575\n",
      "Epoch: 5364 loss_train: 0.0553 loss_val: 0.0563\n",
      "Epoch: 5365 loss_train: 0.0548 loss_val: 0.0549\n",
      "Epoch: 5366 loss_train: 0.0549 loss_val: 0.0546\n",
      "Epoch: 5367 loss_train: 0.0552 loss_val: 0.0556\n",
      "Epoch: 5368 loss_train: 0.0543 loss_val: 0.0550\n",
      "Epoch: 5369 loss_train: 0.0545 loss_val: 0.0554\n",
      "Epoch: 5370 loss_train: 0.0545 loss_val: 0.0559\n",
      "Epoch: 5371 loss_train: 0.0554 loss_val: 0.0553\n",
      "Epoch: 5372 loss_train: 0.0538 loss_val: 0.0532\n",
      "Epoch: 5373 loss_train: 0.0551 loss_val: 0.0542\n",
      "Epoch: 5374 loss_train: 0.0542 loss_val: 0.0557\n",
      "Epoch: 5375 loss_train: 0.0543 loss_val: 0.0558\n",
      "Epoch: 5376 loss_train: 0.0555 loss_val: 0.0548\n",
      "Epoch: 5377 loss_train: 0.0541 loss_val: 0.0560\n",
      "Epoch: 5378 loss_train: 0.0556 loss_val: 0.0557\n",
      "Epoch: 5379 loss_train: 0.0548 loss_val: 0.0540\n",
      "Epoch: 5380 loss_train: 0.0554 loss_val: 0.0552\n",
      "Epoch: 5381 loss_train: 0.0539 loss_val: 0.0541\n",
      "Epoch: 5382 loss_train: 0.0548 loss_val: 0.0540\n",
      "Epoch: 5383 loss_train: 0.0549 loss_val: 0.0538\n",
      "Epoch: 5384 loss_train: 0.0536 loss_val: 0.0571\n",
      "Epoch: 5385 loss_train: 0.0539 loss_val: 0.0562\n",
      "Epoch: 5386 loss_train: 0.0542 loss_val: 0.0570\n",
      "Epoch: 5387 loss_train: 0.0553 loss_val: 0.0593\n",
      "Epoch: 5388 loss_train: 0.0554 loss_val: 0.0575\n",
      "Epoch: 5389 loss_train: 0.0550 loss_val: 0.0558\n",
      "Epoch: 5390 loss_train: 0.0541 loss_val: 0.0539\n",
      "Epoch: 5391 loss_train: 0.0547 loss_val: 0.0542\n",
      "Epoch: 5392 loss_train: 0.0531 loss_val: 0.0537\n",
      "Epoch: 5393 loss_train: 0.0549 loss_val: 0.0538\n",
      "Epoch: 5394 loss_train: 0.0546 loss_val: 0.0538\n",
      "Epoch: 5395 loss_train: 0.0530 loss_val: 0.0568\n",
      "Epoch: 5396 loss_train: 0.0550 loss_val: 0.0542\n",
      "Epoch: 5397 loss_train: 0.0546 loss_val: 0.0557\n",
      "Epoch: 5398 loss_train: 0.0551 loss_val: 0.0552\n",
      "Epoch: 5399 loss_train: 0.0531 loss_val: 0.0547\n",
      "Epoch: 5400 loss_train: 0.0536 loss_val: 0.0573\n",
      "Epoch: 5401 loss_train: 0.0544 loss_val: 0.0572\n",
      "Epoch: 5402 loss_train: 0.0559 loss_val: 0.0563\n",
      "Epoch: 5403 loss_train: 0.0567 loss_val: 0.0559\n",
      "Epoch: 5404 loss_train: 0.0561 loss_val: 0.0564\n",
      "Epoch: 5405 loss_train: 0.0552 loss_val: 0.0572\n",
      "Epoch: 5406 loss_train: 0.0550 loss_val: 0.0587\n",
      "Epoch: 5407 loss_train: 0.0558 loss_val: 0.0561\n",
      "Epoch: 5408 loss_train: 0.0556 loss_val: 0.0544\n",
      "Epoch: 5409 loss_train: 0.0545 loss_val: 0.0546\n",
      "Epoch: 5410 loss_train: 0.0533 loss_val: 0.0554\n",
      "Epoch: 5411 loss_train: 0.0535 loss_val: 0.0544\n",
      "Epoch: 5412 loss_train: 0.0550 loss_val: 0.0562\n",
      "Epoch: 5413 loss_train: 0.0557 loss_val: 0.0560\n",
      "Epoch: 5414 loss_train: 0.0527 loss_val: 0.0575\n",
      "Epoch: 5415 loss_train: 0.0540 loss_val: 0.0535\n",
      "Epoch: 5416 loss_train: 0.0556 loss_val: 0.0570\n",
      "Epoch: 5417 loss_train: 0.0550 loss_val: 0.0538\n",
      "Epoch: 5418 loss_train: 0.0554 loss_val: 0.0575\n",
      "Epoch: 5419 loss_train: 0.0560 loss_val: 0.0561\n",
      "Epoch: 5420 loss_train: 0.0543 loss_val: 0.0572\n",
      "Epoch: 5421 loss_train: 0.0550 loss_val: 0.0552\n",
      "Epoch: 5422 loss_train: 0.0555 loss_val: 0.0548\n",
      "Epoch: 5423 loss_train: 0.0550 loss_val: 0.0564\n",
      "Epoch: 5424 loss_train: 0.0562 loss_val: 0.0548\n",
      "Epoch: 5425 loss_train: 0.0554 loss_val: 0.0549\n",
      "Epoch: 5426 loss_train: 0.0566 loss_val: 0.0577\n",
      "Epoch: 5427 loss_train: 0.0555 loss_val: 0.0566\n",
      "Epoch: 5428 loss_train: 0.0550 loss_val: 0.0562\n",
      "Epoch: 5429 loss_train: 0.0540 loss_val: 0.0565\n",
      "Epoch: 5430 loss_train: 0.0536 loss_val: 0.0561\n",
      "Epoch: 5431 loss_train: 0.0548 loss_val: 0.0548\n",
      "Epoch: 5432 loss_train: 0.0536 loss_val: 0.0559\n",
      "Epoch: 5433 loss_train: 0.0555 loss_val: 0.0555\n",
      "Epoch: 5434 loss_train: 0.0558 loss_val: 0.0549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5435 loss_train: 0.0552 loss_val: 0.0579\n",
      "Epoch: 5436 loss_train: 0.0558 loss_val: 0.0557\n",
      "Epoch: 5437 loss_train: 0.0563 loss_val: 0.0560\n",
      "Epoch: 5438 loss_train: 0.0560 loss_val: 0.0548\n",
      "Epoch: 5439 loss_train: 0.0536 loss_val: 0.0551\n",
      "Epoch: 5440 loss_train: 0.0546 loss_val: 0.0535\n",
      "Epoch: 5441 loss_train: 0.0549 loss_val: 0.0561\n",
      "Epoch: 5442 loss_train: 0.0550 loss_val: 0.0560\n",
      "Epoch: 5443 loss_train: 0.0547 loss_val: 0.0560\n",
      "Epoch: 5444 loss_train: 0.0553 loss_val: 0.0571\n",
      "Epoch: 5445 loss_train: 0.0541 loss_val: 0.0548\n",
      "Epoch: 5446 loss_train: 0.0538 loss_val: 0.0558\n",
      "Epoch: 5447 loss_train: 0.0556 loss_val: 0.0541\n",
      "Epoch: 5448 loss_train: 0.0551 loss_val: 0.0562\n",
      "Epoch: 5449 loss_train: 0.0549 loss_val: 0.0559\n",
      "Epoch: 5450 loss_train: 0.0541 loss_val: 0.0532\n",
      "Epoch: 5451 loss_train: 0.0548 loss_val: 0.0544\n",
      "Epoch: 5452 loss_train: 0.0544 loss_val: 0.0549\n",
      "Epoch: 5453 loss_train: 0.0544 loss_val: 0.0581\n",
      "Epoch: 5454 loss_train: 0.0550 loss_val: 0.0558\n",
      "Epoch: 5455 loss_train: 0.0537 loss_val: 0.0550\n",
      "Epoch: 5456 loss_train: 0.0548 loss_val: 0.0575\n",
      "Epoch: 5457 loss_train: 0.0549 loss_val: 0.0538\n",
      "Epoch: 5458 loss_train: 0.0537 loss_val: 0.0547\n",
      "Epoch: 5459 loss_train: 0.0562 loss_val: 0.0560\n",
      "Epoch: 5460 loss_train: 0.0548 loss_val: 0.0544\n",
      "Epoch: 5461 loss_train: 0.0553 loss_val: 0.0531\n",
      "Epoch: 5462 loss_train: 0.0553 loss_val: 0.0549\n",
      "Epoch: 5463 loss_train: 0.0532 loss_val: 0.0567\n",
      "Epoch: 5464 loss_train: 0.0555 loss_val: 0.0575\n",
      "Epoch: 5465 loss_train: 0.0559 loss_val: 0.0542\n",
      "Epoch: 5466 loss_train: 0.0542 loss_val: 0.0534\n",
      "Epoch: 5467 loss_train: 0.0546 loss_val: 0.0528\n",
      "Epoch: 5468 loss_train: 0.0550 loss_val: 0.0550\n",
      "Epoch: 5469 loss_train: 0.0539 loss_val: 0.0530\n",
      "Epoch: 5470 loss_train: 0.0544 loss_val: 0.0544\n",
      "Epoch: 5471 loss_train: 0.0555 loss_val: 0.0559\n",
      "Epoch: 5472 loss_train: 0.0540 loss_val: 0.0549\n",
      "Epoch: 5473 loss_train: 0.0550 loss_val: 0.0540\n",
      "Epoch: 5474 loss_train: 0.0555 loss_val: 0.0582\n",
      "Epoch: 5475 loss_train: 0.0548 loss_val: 0.0548\n",
      "Epoch: 5476 loss_train: 0.0544 loss_val: 0.0579\n",
      "Epoch: 5477 loss_train: 0.0554 loss_val: 0.0551\n",
      "Epoch: 5478 loss_train: 0.0558 loss_val: 0.0567\n",
      "Epoch: 5479 loss_train: 0.0541 loss_val: 0.0606\n",
      "Epoch: 5480 loss_train: 0.0547 loss_val: 0.0590\n",
      "Epoch: 5481 loss_train: 0.0552 loss_val: 0.0578\n",
      "Epoch: 5482 loss_train: 0.0542 loss_val: 0.0581\n",
      "Epoch: 5483 loss_train: 0.0543 loss_val: 0.0569\n",
      "Epoch: 5484 loss_train: 0.0541 loss_val: 0.0565\n",
      "Epoch: 5485 loss_train: 0.0544 loss_val: 0.0571\n",
      "Epoch: 5486 loss_train: 0.0546 loss_val: 0.0593\n",
      "Epoch: 5487 loss_train: 0.0544 loss_val: 0.0576\n",
      "Epoch: 5488 loss_train: 0.0524 loss_val: 0.0574\n",
      "Epoch: 5489 loss_train: 0.0536 loss_val: 0.0576\n",
      "Epoch: 5490 loss_train: 0.0543 loss_val: 0.0590\n",
      "Epoch: 5491 loss_train: 0.0542 loss_val: 0.0558\n",
      "Epoch: 5492 loss_train: 0.0537 loss_val: 0.0561\n",
      "Epoch: 5493 loss_train: 0.0555 loss_val: 0.0562\n",
      "Epoch: 5494 loss_train: 0.0550 loss_val: 0.0553\n",
      "Epoch: 5495 loss_train: 0.0543 loss_val: 0.0555\n",
      "Epoch: 5496 loss_train: 0.0539 loss_val: 0.0570\n",
      "Epoch: 5497 loss_train: 0.0553 loss_val: 0.0559\n",
      "Epoch: 5498 loss_train: 0.0542 loss_val: 0.0568\n",
      "Epoch: 5499 loss_train: 0.0540 loss_val: 0.0550\n",
      "Epoch: 5500 loss_train: 0.0538 loss_val: 0.0566\n",
      "Epoch: 5501 loss_train: 0.0546 loss_val: 0.0532\n",
      "Epoch: 5502 loss_train: 0.0551 loss_val: 0.0547\n",
      "Epoch: 5503 loss_train: 0.0549 loss_val: 0.0560\n",
      "Epoch: 5504 loss_train: 0.0550 loss_val: 0.0558\n",
      "Epoch: 5505 loss_train: 0.0546 loss_val: 0.0579\n",
      "Epoch: 5506 loss_train: 0.0541 loss_val: 0.0555\n",
      "Epoch: 5507 loss_train: 0.0541 loss_val: 0.0557\n",
      "Epoch: 5508 loss_train: 0.0541 loss_val: 0.0556\n",
      "Epoch: 5509 loss_train: 0.0556 loss_val: 0.0565\n",
      "Epoch: 5510 loss_train: 0.0549 loss_val: 0.0546\n",
      "Epoch: 5511 loss_train: 0.0546 loss_val: 0.0546\n",
      "Epoch: 5512 loss_train: 0.0549 loss_val: 0.0546\n",
      "Epoch: 5513 loss_train: 0.0545 loss_val: 0.0553\n",
      "Epoch: 5514 loss_train: 0.0546 loss_val: 0.0547\n",
      "Epoch: 5515 loss_train: 0.0537 loss_val: 0.0548\n",
      "Epoch: 5516 loss_train: 0.0544 loss_val: 0.0572\n",
      "Epoch: 5517 loss_train: 0.0541 loss_val: 0.0548\n",
      "Epoch: 5518 loss_train: 0.0544 loss_val: 0.0561\n",
      "Epoch: 5519 loss_train: 0.0567 loss_val: 0.0557\n",
      "Epoch: 5520 loss_train: 0.0546 loss_val: 0.0571\n",
      "Epoch: 5521 loss_train: 0.0543 loss_val: 0.0546\n",
      "Epoch: 5522 loss_train: 0.0549 loss_val: 0.0559\n",
      "Epoch: 5523 loss_train: 0.0547 loss_val: 0.0547\n",
      "Epoch: 5524 loss_train: 0.0549 loss_val: 0.0550\n",
      "Epoch: 5525 loss_train: 0.0545 loss_val: 0.0547\n",
      "Epoch: 5526 loss_train: 0.0533 loss_val: 0.0565\n",
      "Epoch: 5527 loss_train: 0.0540 loss_val: 0.0572\n",
      "Epoch: 5528 loss_train: 0.0552 loss_val: 0.0579\n",
      "Epoch: 5529 loss_train: 0.0558 loss_val: 0.0560\n",
      "Epoch: 5530 loss_train: 0.0553 loss_val: 0.0540\n",
      "Epoch: 5531 loss_train: 0.0549 loss_val: 0.0547\n",
      "Epoch: 5532 loss_train: 0.0554 loss_val: 0.0551\n",
      "Epoch: 5533 loss_train: 0.0541 loss_val: 0.0556\n",
      "Epoch: 5534 loss_train: 0.0541 loss_val: 0.0566\n",
      "Epoch: 5535 loss_train: 0.0549 loss_val: 0.0551\n",
      "Epoch: 5536 loss_train: 0.0561 loss_val: 0.0561\n",
      "Epoch: 5537 loss_train: 0.0547 loss_val: 0.0557\n",
      "Epoch: 5538 loss_train: 0.0549 loss_val: 0.0570\n",
      "Epoch: 5539 loss_train: 0.0545 loss_val: 0.0559\n",
      "Epoch: 5540 loss_train: 0.0549 loss_val: 0.0574\n",
      "Epoch: 5541 loss_train: 0.0548 loss_val: 0.0578\n",
      "Epoch: 5542 loss_train: 0.0547 loss_val: 0.0577\n",
      "Epoch: 5543 loss_train: 0.0539 loss_val: 0.0587\n",
      "Epoch: 5544 loss_train: 0.0535 loss_val: 0.0597\n",
      "Epoch: 5545 loss_train: 0.0550 loss_val: 0.0584\n",
      "Epoch: 5546 loss_train: 0.0535 loss_val: 0.0577\n",
      "Epoch: 5547 loss_train: 0.0537 loss_val: 0.0582\n",
      "Epoch: 5548 loss_train: 0.0541 loss_val: 0.0587\n",
      "Epoch: 5549 loss_train: 0.0547 loss_val: 0.0573\n",
      "Epoch: 5550 loss_train: 0.0535 loss_val: 0.0578\n",
      "Epoch: 5551 loss_train: 0.0534 loss_val: 0.0576\n",
      "Epoch: 5552 loss_train: 0.0550 loss_val: 0.0582\n",
      "Epoch: 5553 loss_train: 0.0556 loss_val: 0.0574\n",
      "Epoch: 5554 loss_train: 0.0562 loss_val: 0.0589\n",
      "Epoch: 5555 loss_train: 0.0553 loss_val: 0.0577\n",
      "Epoch: 5556 loss_train: 0.0551 loss_val: 0.0568\n",
      "Epoch: 5557 loss_train: 0.0546 loss_val: 0.0571\n",
      "Epoch: 5558 loss_train: 0.0551 loss_val: 0.0561\n",
      "Epoch: 5559 loss_train: 0.0549 loss_val: 0.0569\n",
      "Epoch: 5560 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 5561 loss_train: 0.0545 loss_val: 0.0580\n",
      "Epoch: 5562 loss_train: 0.0534 loss_val: 0.0584\n",
      "Epoch: 5563 loss_train: 0.0554 loss_val: 0.0572\n",
      "Epoch: 5564 loss_train: 0.0551 loss_val: 0.0562\n",
      "Epoch: 5565 loss_train: 0.0538 loss_val: 0.0574\n",
      "Epoch: 5566 loss_train: 0.0538 loss_val: 0.0568\n",
      "Epoch: 5567 loss_train: 0.0541 loss_val: 0.0548\n",
      "Epoch: 5568 loss_train: 0.0538 loss_val: 0.0558\n",
      "Epoch: 5569 loss_train: 0.0535 loss_val: 0.0553\n",
      "Epoch: 5570 loss_train: 0.0541 loss_val: 0.0568\n",
      "Epoch: 5571 loss_train: 0.0527 loss_val: 0.0547\n",
      "Epoch: 5572 loss_train: 0.0537 loss_val: 0.0561\n",
      "Epoch: 5573 loss_train: 0.0545 loss_val: 0.0579\n",
      "Epoch: 5574 loss_train: 0.0552 loss_val: 0.0542\n",
      "Epoch: 5575 loss_train: 0.0558 loss_val: 0.0562\n",
      "Epoch: 5576 loss_train: 0.0569 loss_val: 0.0552\n",
      "Epoch: 5577 loss_train: 0.0548 loss_val: 0.0537\n",
      "Epoch: 5578 loss_train: 0.0537 loss_val: 0.0543\n",
      "Epoch: 5579 loss_train: 0.0543 loss_val: 0.0548\n",
      "Epoch: 5580 loss_train: 0.0567 loss_val: 0.0568\n",
      "Epoch: 5581 loss_train: 0.0565 loss_val: 0.0572\n",
      "Epoch: 5582 loss_train: 0.0557 loss_val: 0.0558\n",
      "Epoch: 5583 loss_train: 0.0550 loss_val: 0.0559\n",
      "Epoch: 5584 loss_train: 0.0545 loss_val: 0.0576\n",
      "Epoch: 5585 loss_train: 0.0549 loss_val: 0.0570\n",
      "Epoch: 5586 loss_train: 0.0553 loss_val: 0.0574\n",
      "Epoch: 5587 loss_train: 0.0548 loss_val: 0.0558\n",
      "Epoch: 5588 loss_train: 0.0537 loss_val: 0.0567\n",
      "Epoch: 5589 loss_train: 0.0545 loss_val: 0.0544\n",
      "Epoch: 5590 loss_train: 0.0551 loss_val: 0.0566\n",
      "Epoch: 5591 loss_train: 0.0549 loss_val: 0.0551\n",
      "Epoch: 5592 loss_train: 0.0556 loss_val: 0.0560\n",
      "Epoch: 5593 loss_train: 0.0555 loss_val: 0.0557\n",
      "Epoch: 5594 loss_train: 0.0543 loss_val: 0.0554\n",
      "Epoch: 5595 loss_train: 0.0549 loss_val: 0.0563\n",
      "Epoch: 5596 loss_train: 0.0555 loss_val: 0.0553\n",
      "Epoch: 5597 loss_train: 0.0540 loss_val: 0.0548\n",
      "Epoch: 5598 loss_train: 0.0559 loss_val: 0.0570\n",
      "Epoch: 5599 loss_train: 0.0560 loss_val: 0.0555\n",
      "Epoch: 5600 loss_train: 0.0545 loss_val: 0.0557\n",
      "Epoch: 5601 loss_train: 0.0547 loss_val: 0.0546\n",
      "Epoch: 5602 loss_train: 0.0549 loss_val: 0.0543\n",
      "Epoch: 5603 loss_train: 0.0549 loss_val: 0.0568\n",
      "Epoch: 5604 loss_train: 0.0550 loss_val: 0.0569\n",
      "Epoch: 5605 loss_train: 0.0544 loss_val: 0.0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5606 loss_train: 0.0544 loss_val: 0.0549\n",
      "Epoch: 5607 loss_train: 0.0540 loss_val: 0.0554\n",
      "Epoch: 5608 loss_train: 0.0544 loss_val: 0.0550\n",
      "Epoch: 5609 loss_train: 0.0543 loss_val: 0.0563\n",
      "Epoch: 5610 loss_train: 0.0554 loss_val: 0.0573\n",
      "Epoch: 5611 loss_train: 0.0542 loss_val: 0.0559\n",
      "Epoch: 5612 loss_train: 0.0547 loss_val: 0.0577\n",
      "Epoch: 5613 loss_train: 0.0548 loss_val: 0.0551\n",
      "Epoch: 5614 loss_train: 0.0545 loss_val: 0.0560\n",
      "Epoch: 5615 loss_train: 0.0542 loss_val: 0.0556\n",
      "Epoch: 5616 loss_train: 0.0550 loss_val: 0.0568\n",
      "Epoch: 5617 loss_train: 0.0550 loss_val: 0.0534\n",
      "Epoch: 5618 loss_train: 0.0546 loss_val: 0.0547\n",
      "Epoch: 5619 loss_train: 0.0543 loss_val: 0.0561\n",
      "Epoch: 5620 loss_train: 0.0543 loss_val: 0.0559\n",
      "Epoch: 5621 loss_train: 0.0534 loss_val: 0.0576\n",
      "Epoch: 5622 loss_train: 0.0547 loss_val: 0.0554\n",
      "Epoch: 5623 loss_train: 0.0546 loss_val: 0.0548\n",
      "Epoch: 5624 loss_train: 0.0538 loss_val: 0.0547\n",
      "Epoch: 5625 loss_train: 0.0548 loss_val: 0.0558\n",
      "Epoch: 5626 loss_train: 0.0553 loss_val: 0.0559\n",
      "Epoch: 5627 loss_train: 0.0539 loss_val: 0.0570\n",
      "Epoch: 5628 loss_train: 0.0552 loss_val: 0.0570\n",
      "Epoch: 5629 loss_train: 0.0552 loss_val: 0.0567\n",
      "Epoch: 5630 loss_train: 0.0548 loss_val: 0.0560\n",
      "Epoch: 5631 loss_train: 0.0543 loss_val: 0.0581\n",
      "Epoch: 5632 loss_train: 0.0548 loss_val: 0.0584\n",
      "Epoch: 5633 loss_train: 0.0541 loss_val: 0.0574\n",
      "Epoch: 5634 loss_train: 0.0545 loss_val: 0.0556\n",
      "Epoch: 5635 loss_train: 0.0543 loss_val: 0.0556\n",
      "Epoch: 5636 loss_train: 0.0553 loss_val: 0.0565\n",
      "Epoch: 5637 loss_train: 0.0559 loss_val: 0.0570\n",
      "Epoch: 5638 loss_train: 0.0544 loss_val: 0.0557\n",
      "Epoch: 5639 loss_train: 0.0543 loss_val: 0.0561\n",
      "Epoch: 5640 loss_train: 0.0541 loss_val: 0.0556\n",
      "Epoch: 5641 loss_train: 0.0546 loss_val: 0.0548\n",
      "Epoch: 5642 loss_train: 0.0553 loss_val: 0.0559\n",
      "Epoch: 5643 loss_train: 0.0545 loss_val: 0.0547\n",
      "Epoch: 5644 loss_train: 0.0552 loss_val: 0.0571\n",
      "Epoch: 5645 loss_train: 0.0554 loss_val: 0.0582\n",
      "Epoch: 5646 loss_train: 0.0547 loss_val: 0.0554\n",
      "Epoch: 5647 loss_train: 0.0551 loss_val: 0.0555\n",
      "Epoch: 5648 loss_train: 0.0556 loss_val: 0.0565\n",
      "Epoch: 5649 loss_train: 0.0537 loss_val: 0.0561\n",
      "Epoch: 5650 loss_train: 0.0548 loss_val: 0.0559\n",
      "Epoch: 5651 loss_train: 0.0541 loss_val: 0.0562\n",
      "Epoch: 5652 loss_train: 0.0545 loss_val: 0.0566\n",
      "Epoch: 5653 loss_train: 0.0539 loss_val: 0.0553\n",
      "Epoch: 5654 loss_train: 0.0547 loss_val: 0.0565\n",
      "Epoch: 5655 loss_train: 0.0540 loss_val: 0.0546\n",
      "Epoch: 5656 loss_train: 0.0550 loss_val: 0.0546\n",
      "Epoch: 5657 loss_train: 0.0550 loss_val: 0.0536\n",
      "Epoch: 5658 loss_train: 0.0545 loss_val: 0.0542\n",
      "Epoch: 5659 loss_train: 0.0529 loss_val: 0.0538\n",
      "Epoch: 5660 loss_train: 0.0549 loss_val: 0.0557\n",
      "Epoch: 5661 loss_train: 0.0548 loss_val: 0.0562\n",
      "Epoch: 5662 loss_train: 0.0540 loss_val: 0.0529\n",
      "Epoch: 5663 loss_train: 0.0544 loss_val: 0.0533\n",
      "Epoch: 5664 loss_train: 0.0531 loss_val: 0.0565\n",
      "Epoch: 5665 loss_train: 0.0541 loss_val: 0.0558\n",
      "Epoch: 5666 loss_train: 0.0558 loss_val: 0.0568\n",
      "Epoch: 5667 loss_train: 0.0546 loss_val: 0.0559\n",
      "Epoch: 5668 loss_train: 0.0537 loss_val: 0.0535\n",
      "Epoch: 5669 loss_train: 0.0548 loss_val: 0.0556\n",
      "Epoch: 5670 loss_train: 0.0545 loss_val: 0.0555\n",
      "Epoch: 5671 loss_train: 0.0539 loss_val: 0.0566\n",
      "Epoch: 5672 loss_train: 0.0543 loss_val: 0.0562\n",
      "Epoch: 5673 loss_train: 0.0538 loss_val: 0.0557\n",
      "Epoch: 5674 loss_train: 0.0537 loss_val: 0.0556\n",
      "Epoch: 5675 loss_train: 0.0530 loss_val: 0.0545\n",
      "Epoch: 5676 loss_train: 0.0541 loss_val: 0.0578\n",
      "Epoch: 5677 loss_train: 0.0527 loss_val: 0.0563\n",
      "Epoch: 5678 loss_train: 0.0539 loss_val: 0.0558\n",
      "Epoch: 5679 loss_train: 0.0548 loss_val: 0.0559\n",
      "Epoch: 5680 loss_train: 0.0543 loss_val: 0.0537\n",
      "Epoch: 5681 loss_train: 0.0540 loss_val: 0.0536\n",
      "Epoch: 5682 loss_train: 0.0543 loss_val: 0.0543\n",
      "Epoch: 5683 loss_train: 0.0543 loss_val: 0.0563\n",
      "Epoch: 5684 loss_train: 0.0544 loss_val: 0.0535\n",
      "Epoch: 5685 loss_train: 0.0535 loss_val: 0.0562\n",
      "Epoch: 5686 loss_train: 0.0535 loss_val: 0.0553\n",
      "Epoch: 5687 loss_train: 0.0543 loss_val: 0.0548\n",
      "Epoch: 5688 loss_train: 0.0543 loss_val: 0.0545\n",
      "Epoch: 5689 loss_train: 0.0531 loss_val: 0.0542\n",
      "Epoch: 5690 loss_train: 0.0541 loss_val: 0.0544\n",
      "Epoch: 5691 loss_train: 0.0539 loss_val: 0.0532\n",
      "Epoch: 5692 loss_train: 0.0534 loss_val: 0.0558\n",
      "Epoch: 5693 loss_train: 0.0539 loss_val: 0.0536\n",
      "Epoch: 5694 loss_train: 0.0547 loss_val: 0.0543\n",
      "Epoch: 5695 loss_train: 0.0560 loss_val: 0.0543\n",
      "Epoch: 5696 loss_train: 0.0551 loss_val: 0.0565\n",
      "Epoch: 5697 loss_train: 0.0561 loss_val: 0.0560\n",
      "Epoch: 5698 loss_train: 0.0574 loss_val: 0.0557\n",
      "Epoch: 5699 loss_train: 0.0559 loss_val: 0.0577\n",
      "Epoch: 5700 loss_train: 0.0550 loss_val: 0.0562\n",
      "Epoch: 5701 loss_train: 0.0554 loss_val: 0.0543\n",
      "Epoch: 5702 loss_train: 0.0558 loss_val: 0.0549\n",
      "Epoch: 5703 loss_train: 0.0542 loss_val: 0.0589\n",
      "Epoch: 5704 loss_train: 0.0539 loss_val: 0.0569\n",
      "Epoch: 5705 loss_train: 0.0550 loss_val: 0.0547\n",
      "Epoch: 5706 loss_train: 0.0535 loss_val: 0.0538\n",
      "Epoch: 5707 loss_train: 0.0545 loss_val: 0.0540\n",
      "Epoch: 5708 loss_train: 0.0553 loss_val: 0.0564\n",
      "Epoch: 5709 loss_train: 0.0536 loss_val: 0.0537\n",
      "Epoch: 5710 loss_train: 0.0548 loss_val: 0.0572\n",
      "Epoch: 5711 loss_train: 0.0550 loss_val: 0.0557\n",
      "Epoch: 5712 loss_train: 0.0537 loss_val: 0.0565\n",
      "Epoch: 5713 loss_train: 0.0560 loss_val: 0.0564\n",
      "Epoch: 5714 loss_train: 0.0567 loss_val: 0.0574\n",
      "Epoch: 5715 loss_train: 0.0547 loss_val: 0.0564\n",
      "Epoch: 5716 loss_train: 0.0558 loss_val: 0.0570\n",
      "Epoch: 5717 loss_train: 0.0542 loss_val: 0.0561\n",
      "Epoch: 5718 loss_train: 0.0548 loss_val: 0.0568\n",
      "Epoch: 5719 loss_train: 0.0542 loss_val: 0.0575\n",
      "Epoch: 5720 loss_train: 0.0556 loss_val: 0.0562\n",
      "Epoch: 5721 loss_train: 0.0530 loss_val: 0.0546\n",
      "Epoch: 5722 loss_train: 0.0527 loss_val: 0.0541\n",
      "Epoch: 5723 loss_train: 0.0559 loss_val: 0.0540\n",
      "Epoch: 5724 loss_train: 0.0537 loss_val: 0.0552\n",
      "Epoch: 5725 loss_train: 0.0540 loss_val: 0.0551\n",
      "Epoch: 5726 loss_train: 0.0536 loss_val: 0.0529\n",
      "Epoch: 5727 loss_train: 0.0535 loss_val: 0.0554\n",
      "Epoch: 5728 loss_train: 0.0551 loss_val: 0.0563\n",
      "Epoch: 5729 loss_train: 0.0556 loss_val: 0.0558\n",
      "Epoch: 5730 loss_train: 0.0546 loss_val: 0.0555\n",
      "Epoch: 5731 loss_train: 0.0535 loss_val: 0.0562\n",
      "Epoch: 5732 loss_train: 0.0558 loss_val: 0.0563\n",
      "Epoch: 5733 loss_train: 0.0553 loss_val: 0.0575\n",
      "Epoch: 5734 loss_train: 0.0546 loss_val: 0.0560\n",
      "Epoch: 5735 loss_train: 0.0566 loss_val: 0.0590\n",
      "Epoch: 5736 loss_train: 0.0554 loss_val: 0.0565\n",
      "Epoch: 5737 loss_train: 0.0530 loss_val: 0.0556\n",
      "Epoch: 5738 loss_train: 0.0543 loss_val: 0.0597\n",
      "Epoch: 5739 loss_train: 0.0551 loss_val: 0.0560\n",
      "Epoch: 5740 loss_train: 0.0550 loss_val: 0.0561\n",
      "Epoch: 5741 loss_train: 0.0547 loss_val: 0.0553\n",
      "Epoch: 5742 loss_train: 0.0543 loss_val: 0.0565\n",
      "Epoch: 5743 loss_train: 0.0538 loss_val: 0.0576\n",
      "Epoch: 5744 loss_train: 0.0542 loss_val: 0.0565\n",
      "Epoch: 5745 loss_train: 0.0545 loss_val: 0.0548\n",
      "Epoch: 5746 loss_train: 0.0541 loss_val: 0.0546\n",
      "Epoch: 5747 loss_train: 0.0536 loss_val: 0.0551\n",
      "Epoch: 5748 loss_train: 0.0542 loss_val: 0.0538\n",
      "Epoch: 5749 loss_train: 0.0546 loss_val: 0.0564\n",
      "Epoch: 5750 loss_train: 0.0538 loss_val: 0.0557\n",
      "Epoch: 5751 loss_train: 0.0547 loss_val: 0.0571\n",
      "Epoch: 5752 loss_train: 0.0539 loss_val: 0.0564\n",
      "Epoch: 5753 loss_train: 0.0543 loss_val: 0.0556\n",
      "Epoch: 5754 loss_train: 0.0545 loss_val: 0.0547\n",
      "Epoch: 5755 loss_train: 0.0538 loss_val: 0.0560\n",
      "Epoch: 5756 loss_train: 0.0532 loss_val: 0.0551\n",
      "Epoch: 5757 loss_train: 0.0542 loss_val: 0.0579\n",
      "Epoch: 5758 loss_train: 0.0540 loss_val: 0.0549\n",
      "Epoch: 5759 loss_train: 0.0521 loss_val: 0.0555\n",
      "Epoch: 5760 loss_train: 0.0546 loss_val: 0.0571\n",
      "Epoch: 5761 loss_train: 0.0545 loss_val: 0.0546\n",
      "Epoch: 5762 loss_train: 0.0541 loss_val: 0.0559\n",
      "Epoch: 5763 loss_train: 0.0537 loss_val: 0.0557\n",
      "Epoch: 5764 loss_train: 0.0534 loss_val: 0.0553\n",
      "Epoch: 5765 loss_train: 0.0543 loss_val: 0.0553\n",
      "Epoch: 5766 loss_train: 0.0538 loss_val: 0.0555\n",
      "Epoch: 5767 loss_train: 0.0547 loss_val: 0.0572\n",
      "Epoch: 5768 loss_train: 0.0530 loss_val: 0.0562\n",
      "Epoch: 5769 loss_train: 0.0544 loss_val: 0.0563\n",
      "Epoch: 5770 loss_train: 0.0545 loss_val: 0.0566\n",
      "Epoch: 5771 loss_train: 0.0536 loss_val: 0.0561\n",
      "Epoch: 5772 loss_train: 0.0535 loss_val: 0.0559\n",
      "Epoch: 5773 loss_train: 0.0542 loss_val: 0.0569\n",
      "Epoch: 5774 loss_train: 0.0538 loss_val: 0.0566\n",
      "Epoch: 5775 loss_train: 0.0542 loss_val: 0.0561\n",
      "Epoch: 5776 loss_train: 0.0541 loss_val: 0.0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5777 loss_train: 0.0546 loss_val: 0.0565\n",
      "Epoch: 5778 loss_train: 0.0538 loss_val: 0.0550\n",
      "Epoch: 5779 loss_train: 0.0546 loss_val: 0.0533\n",
      "Epoch: 5780 loss_train: 0.0531 loss_val: 0.0580\n",
      "Epoch: 5781 loss_train: 0.0543 loss_val: 0.0582\n",
      "Epoch: 5782 loss_train: 0.0531 loss_val: 0.0567\n",
      "Epoch: 5783 loss_train: 0.0535 loss_val: 0.0554\n",
      "Epoch: 5784 loss_train: 0.0542 loss_val: 0.0578\n",
      "Epoch: 5785 loss_train: 0.0541 loss_val: 0.0591\n",
      "Epoch: 5786 loss_train: 0.0551 loss_val: 0.0558\n",
      "Epoch: 5787 loss_train: 0.0543 loss_val: 0.0572\n",
      "Epoch: 5788 loss_train: 0.0537 loss_val: 0.0585\n",
      "Epoch: 5789 loss_train: 0.0543 loss_val: 0.0567\n",
      "Epoch: 5790 loss_train: 0.0540 loss_val: 0.0568\n",
      "Epoch: 5791 loss_train: 0.0539 loss_val: 0.0588\n",
      "Epoch: 5792 loss_train: 0.0536 loss_val: 0.0591\n",
      "Epoch: 5793 loss_train: 0.0553 loss_val: 0.0569\n",
      "Epoch: 5794 loss_train: 0.0543 loss_val: 0.0556\n",
      "Epoch: 5795 loss_train: 0.0552 loss_val: 0.0581\n",
      "Epoch: 5796 loss_train: 0.0565 loss_val: 0.0561\n",
      "Epoch: 5797 loss_train: 0.0551 loss_val: 0.0558\n",
      "Epoch: 5798 loss_train: 0.0560 loss_val: 0.0565\n",
      "Epoch: 5799 loss_train: 0.0559 loss_val: 0.0574\n",
      "Epoch: 5800 loss_train: 0.0548 loss_val: 0.0563\n",
      "Epoch: 5801 loss_train: 0.0537 loss_val: 0.0559\n",
      "Epoch: 5802 loss_train: 0.0542 loss_val: 0.0530\n",
      "Epoch: 5803 loss_train: 0.0551 loss_val: 0.0561\n",
      "Epoch: 5804 loss_train: 0.0541 loss_val: 0.0558\n",
      "Epoch: 5805 loss_train: 0.0541 loss_val: 0.0564\n",
      "Epoch: 5806 loss_train: 0.0539 loss_val: 0.0562\n",
      "Epoch: 5807 loss_train: 0.0556 loss_val: 0.0546\n",
      "Epoch: 5808 loss_train: 0.0543 loss_val: 0.0540\n",
      "Epoch: 5809 loss_train: 0.0551 loss_val: 0.0527\n",
      "Epoch: 5810 loss_train: 0.0534 loss_val: 0.0543\n",
      "Epoch: 5811 loss_train: 0.0551 loss_val: 0.0540\n",
      "Epoch: 5812 loss_train: 0.0535 loss_val: 0.0579\n",
      "Epoch: 5813 loss_train: 0.0548 loss_val: 0.0562\n",
      "Epoch: 5814 loss_train: 0.0535 loss_val: 0.0557\n",
      "Epoch: 5815 loss_train: 0.0535 loss_val: 0.0559\n",
      "Epoch: 5816 loss_train: 0.0540 loss_val: 0.0560\n",
      "Epoch: 5817 loss_train: 0.0547 loss_val: 0.0572\n",
      "Epoch: 5818 loss_train: 0.0532 loss_val: 0.0562\n",
      "Epoch: 5819 loss_train: 0.0539 loss_val: 0.0556\n",
      "Epoch: 5820 loss_train: 0.0547 loss_val: 0.0556\n",
      "Epoch: 5821 loss_train: 0.0555 loss_val: 0.0562\n",
      "Epoch: 5822 loss_train: 0.0543 loss_val: 0.0566\n",
      "Epoch: 5823 loss_train: 0.0548 loss_val: 0.0554\n",
      "Epoch: 5824 loss_train: 0.0535 loss_val: 0.0554\n",
      "Epoch: 5825 loss_train: 0.0543 loss_val: 0.0552\n",
      "Epoch: 5826 loss_train: 0.0539 loss_val: 0.0558\n",
      "Epoch: 5827 loss_train: 0.0542 loss_val: 0.0564\n",
      "Epoch: 5828 loss_train: 0.0536 loss_val: 0.0550\n",
      "Epoch: 5829 loss_train: 0.0530 loss_val: 0.0570\n",
      "Epoch: 5830 loss_train: 0.0541 loss_val: 0.0578\n",
      "Epoch: 5831 loss_train: 0.0546 loss_val: 0.0561\n",
      "Epoch: 5832 loss_train: 0.0536 loss_val: 0.0553\n",
      "Epoch: 5833 loss_train: 0.0541 loss_val: 0.0567\n",
      "Epoch: 5834 loss_train: 0.0537 loss_val: 0.0560\n",
      "Epoch: 5835 loss_train: 0.0538 loss_val: 0.0563\n",
      "Epoch: 5836 loss_train: 0.0535 loss_val: 0.0537\n",
      "Epoch: 5837 loss_train: 0.0547 loss_val: 0.0525\n",
      "Epoch: 5838 loss_train: 0.0543 loss_val: 0.0563\n",
      "Epoch: 5839 loss_train: 0.0552 loss_val: 0.0547\n",
      "Epoch: 5840 loss_train: 0.0546 loss_val: 0.0573\n",
      "Epoch: 5841 loss_train: 0.0527 loss_val: 0.0570\n",
      "Epoch: 5842 loss_train: 0.0530 loss_val: 0.0554\n",
      "Epoch: 5843 loss_train: 0.0542 loss_val: 0.0552\n",
      "Epoch: 5844 loss_train: 0.0548 loss_val: 0.0565\n",
      "Epoch: 5845 loss_train: 0.0542 loss_val: 0.0565\n",
      "Epoch: 5846 loss_train: 0.0543 loss_val: 0.0566\n",
      "Epoch: 5847 loss_train: 0.0537 loss_val: 0.0565\n",
      "Epoch: 5848 loss_train: 0.0548 loss_val: 0.0584\n",
      "Epoch: 5849 loss_train: 0.0540 loss_val: 0.0555\n",
      "Epoch: 5850 loss_train: 0.0551 loss_val: 0.0552\n",
      "Epoch: 5851 loss_train: 0.0552 loss_val: 0.0566\n",
      "Epoch: 5852 loss_train: 0.0536 loss_val: 0.0547\n",
      "Epoch: 5853 loss_train: 0.0539 loss_val: 0.0561\n",
      "Epoch: 5854 loss_train: 0.0549 loss_val: 0.0560\n",
      "Epoch: 5855 loss_train: 0.0546 loss_val: 0.0568\n",
      "Epoch: 5856 loss_train: 0.0549 loss_val: 0.0555\n",
      "Epoch: 5857 loss_train: 0.0545 loss_val: 0.0560\n",
      "Epoch: 5858 loss_train: 0.0541 loss_val: 0.0556\n",
      "Epoch: 5859 loss_train: 0.0554 loss_val: 0.0556\n",
      "Epoch: 5860 loss_train: 0.0551 loss_val: 0.0564\n",
      "Epoch: 5861 loss_train: 0.0543 loss_val: 0.0555\n",
      "Epoch: 5862 loss_train: 0.0552 loss_val: 0.0572\n",
      "Epoch: 5863 loss_train: 0.0560 loss_val: 0.0583\n",
      "Epoch: 5864 loss_train: 0.0559 loss_val: 0.0537\n",
      "Epoch: 5865 loss_train: 0.0557 loss_val: 0.0536\n",
      "Epoch: 5866 loss_train: 0.0560 loss_val: 0.0554\n",
      "Epoch: 5867 loss_train: 0.0546 loss_val: 0.0546\n",
      "Epoch: 5868 loss_train: 0.0542 loss_val: 0.0539\n",
      "Epoch: 5869 loss_train: 0.0535 loss_val: 0.0547\n",
      "Epoch: 5870 loss_train: 0.0537 loss_val: 0.0548\n",
      "Epoch: 5871 loss_train: 0.0550 loss_val: 0.0556\n",
      "Epoch: 5872 loss_train: 0.0542 loss_val: 0.0563\n",
      "Epoch: 5873 loss_train: 0.0548 loss_val: 0.0561\n",
      "Epoch: 5874 loss_train: 0.0544 loss_val: 0.0559\n",
      "Epoch: 5875 loss_train: 0.0546 loss_val: 0.0540\n",
      "Epoch: 5876 loss_train: 0.0537 loss_val: 0.0562\n",
      "Epoch: 5877 loss_train: 0.0543 loss_val: 0.0555\n",
      "Epoch: 5878 loss_train: 0.0540 loss_val: 0.0541\n",
      "Epoch: 5879 loss_train: 0.0539 loss_val: 0.0559\n",
      "Epoch: 5880 loss_train: 0.0543 loss_val: 0.0548\n",
      "Epoch: 5881 loss_train: 0.0544 loss_val: 0.0570\n",
      "Epoch: 5882 loss_train: 0.0552 loss_val: 0.0559\n",
      "Epoch: 5883 loss_train: 0.0560 loss_val: 0.0580\n",
      "Epoch: 5884 loss_train: 0.0563 loss_val: 0.0562\n",
      "Epoch: 5885 loss_train: 0.0556 loss_val: 0.0555\n",
      "Epoch: 5886 loss_train: 0.0561 loss_val: 0.0564\n",
      "Epoch: 5887 loss_train: 0.0549 loss_val: 0.0584\n",
      "Epoch: 5888 loss_train: 0.0564 loss_val: 0.0563\n",
      "Epoch: 5889 loss_train: 0.0562 loss_val: 0.0562\n",
      "Epoch: 5890 loss_train: 0.0550 loss_val: 0.0562\n",
      "Epoch: 5891 loss_train: 0.0546 loss_val: 0.0543\n",
      "Epoch: 5892 loss_train: 0.0550 loss_val: 0.0556\n",
      "Epoch: 5893 loss_train: 0.0551 loss_val: 0.0573\n",
      "Epoch: 5894 loss_train: 0.0563 loss_val: 0.0563\n",
      "Epoch: 5895 loss_train: 0.0561 loss_val: 0.0563\n",
      "Epoch: 5896 loss_train: 0.0548 loss_val: 0.0574\n",
      "Epoch: 5897 loss_train: 0.0543 loss_val: 0.0580\n",
      "Epoch: 5898 loss_train: 0.0530 loss_val: 0.0558\n",
      "Epoch: 5899 loss_train: 0.0539 loss_val: 0.0564\n",
      "Epoch: 5900 loss_train: 0.0549 loss_val: 0.0564\n",
      "Epoch: 5901 loss_train: 0.0545 loss_val: 0.0574\n",
      "Epoch: 5902 loss_train: 0.0542 loss_val: 0.0566\n",
      "Epoch: 5903 loss_train: 0.0530 loss_val: 0.0549\n",
      "Epoch: 5904 loss_train: 0.0529 loss_val: 0.0577\n",
      "Epoch: 5905 loss_train: 0.0547 loss_val: 0.0563\n",
      "Epoch: 5906 loss_train: 0.0538 loss_val: 0.0554\n",
      "Epoch: 5907 loss_train: 0.0538 loss_val: 0.0558\n",
      "Epoch: 5908 loss_train: 0.0535 loss_val: 0.0560\n",
      "Epoch: 5909 loss_train: 0.0557 loss_val: 0.0585\n",
      "Epoch: 5910 loss_train: 0.0538 loss_val: 0.0568\n",
      "Epoch: 5911 loss_train: 0.0553 loss_val: 0.0577\n",
      "Epoch: 5912 loss_train: 0.0551 loss_val: 0.0560\n",
      "Epoch: 5913 loss_train: 0.0544 loss_val: 0.0574\n",
      "Epoch: 5914 loss_train: 0.0543 loss_val: 0.0561\n",
      "Epoch: 5915 loss_train: 0.0534 loss_val: 0.0543\n",
      "Epoch: 5916 loss_train: 0.0534 loss_val: 0.0572\n",
      "Epoch: 5917 loss_train: 0.0542 loss_val: 0.0562\n",
      "Epoch: 5918 loss_train: 0.0537 loss_val: 0.0562\n",
      "Epoch: 5919 loss_train: 0.0549 loss_val: 0.0573\n",
      "Epoch: 5920 loss_train: 0.0543 loss_val: 0.0583\n",
      "Epoch: 5921 loss_train: 0.0546 loss_val: 0.0560\n",
      "Epoch: 5922 loss_train: 0.0543 loss_val: 0.0566\n",
      "Epoch: 5923 loss_train: 0.0532 loss_val: 0.0551\n",
      "Epoch: 5924 loss_train: 0.0532 loss_val: 0.0569\n",
      "Epoch: 5925 loss_train: 0.0541 loss_val: 0.0563\n",
      "Epoch: 5926 loss_train: 0.0537 loss_val: 0.0558\n",
      "Epoch: 5927 loss_train: 0.0543 loss_val: 0.0589\n",
      "Epoch: 5928 loss_train: 0.0543 loss_val: 0.0570\n",
      "Epoch: 5929 loss_train: 0.0540 loss_val: 0.0569\n",
      "Epoch: 5930 loss_train: 0.0543 loss_val: 0.0561\n",
      "Epoch: 5931 loss_train: 0.0541 loss_val: 0.0565\n",
      "Epoch: 5932 loss_train: 0.0529 loss_val: 0.0576\n",
      "Epoch: 5933 loss_train: 0.0556 loss_val: 0.0562\n",
      "Epoch: 5934 loss_train: 0.0552 loss_val: 0.0572\n",
      "Epoch: 5935 loss_train: 0.0563 loss_val: 0.0562\n",
      "Epoch: 5936 loss_train: 0.0550 loss_val: 0.0566\n",
      "Epoch: 5937 loss_train: 0.0527 loss_val: 0.0561\n",
      "Epoch: 5938 loss_train: 0.0539 loss_val: 0.0572\n",
      "Epoch: 5939 loss_train: 0.0546 loss_val: 0.0566\n",
      "Epoch: 5940 loss_train: 0.0545 loss_val: 0.0548\n",
      "Epoch: 5941 loss_train: 0.0537 loss_val: 0.0532\n",
      "Epoch: 5942 loss_train: 0.0530 loss_val: 0.0546\n",
      "Epoch: 5943 loss_train: 0.0532 loss_val: 0.0565\n",
      "Epoch: 5944 loss_train: 0.0544 loss_val: 0.0550\n",
      "Epoch: 5945 loss_train: 0.0547 loss_val: 0.0550\n",
      "Epoch: 5946 loss_train: 0.0545 loss_val: 0.0550\n",
      "Epoch: 5947 loss_train: 0.0540 loss_val: 0.0549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5948 loss_train: 0.0556 loss_val: 0.0553\n",
      "Epoch: 5949 loss_train: 0.0546 loss_val: 0.0564\n",
      "Epoch: 5950 loss_train: 0.0547 loss_val: 0.0553\n",
      "Epoch: 5951 loss_train: 0.0551 loss_val: 0.0538\n",
      "Epoch: 5952 loss_train: 0.0544 loss_val: 0.0556\n",
      "Epoch: 5953 loss_train: 0.0542 loss_val: 0.0552\n",
      "Epoch: 5954 loss_train: 0.0556 loss_val: 0.0543\n",
      "Epoch: 5955 loss_train: 0.0547 loss_val: 0.0564\n",
      "Epoch: 5956 loss_train: 0.0541 loss_val: 0.0543\n",
      "Epoch: 5957 loss_train: 0.0547 loss_val: 0.0534\n",
      "Epoch: 5958 loss_train: 0.0537 loss_val: 0.0563\n",
      "Epoch: 5959 loss_train: 0.0536 loss_val: 0.0541\n",
      "Epoch: 5960 loss_train: 0.0534 loss_val: 0.0533\n",
      "Epoch: 5961 loss_train: 0.0535 loss_val: 0.0555\n",
      "Epoch: 5962 loss_train: 0.0534 loss_val: 0.0568\n",
      "Epoch: 5963 loss_train: 0.0532 loss_val: 0.0567\n",
      "Epoch: 5964 loss_train: 0.0531 loss_val: 0.0581\n",
      "Epoch: 5965 loss_train: 0.0535 loss_val: 0.0579\n",
      "Epoch: 5966 loss_train: 0.0547 loss_val: 0.0563\n",
      "Epoch: 5967 loss_train: 0.0522 loss_val: 0.0566\n",
      "Epoch: 5968 loss_train: 0.0535 loss_val: 0.0564\n",
      "Epoch: 5969 loss_train: 0.0537 loss_val: 0.0563\n",
      "Epoch: 5970 loss_train: 0.0537 loss_val: 0.0545\n",
      "Epoch: 5971 loss_train: 0.0547 loss_val: 0.0572\n",
      "Epoch: 5972 loss_train: 0.0551 loss_val: 0.0549\n",
      "Epoch: 5973 loss_train: 0.0541 loss_val: 0.0564\n",
      "Epoch: 5974 loss_train: 0.0542 loss_val: 0.0558\n",
      "Epoch: 5975 loss_train: 0.0536 loss_val: 0.0560\n",
      "Epoch: 5976 loss_train: 0.0552 loss_val: 0.0566\n",
      "Epoch: 5977 loss_train: 0.0557 loss_val: 0.0568\n",
      "Epoch: 5978 loss_train: 0.0546 loss_val: 0.0563\n",
      "Epoch: 5979 loss_train: 0.0540 loss_val: 0.0564\n",
      "Epoch: 5980 loss_train: 0.0540 loss_val: 0.0556\n",
      "Epoch: 5981 loss_train: 0.0545 loss_val: 0.0551\n",
      "Epoch: 5982 loss_train: 0.0543 loss_val: 0.0572\n",
      "Epoch: 5983 loss_train: 0.0545 loss_val: 0.0537\n",
      "Epoch: 5984 loss_train: 0.0540 loss_val: 0.0560\n",
      "Epoch: 5985 loss_train: 0.0548 loss_val: 0.0551\n",
      "Epoch: 5986 loss_train: 0.0533 loss_val: 0.0557\n",
      "Epoch: 5987 loss_train: 0.0532 loss_val: 0.0546\n",
      "Epoch: 5988 loss_train: 0.0545 loss_val: 0.0555\n",
      "Epoch: 5989 loss_train: 0.0539 loss_val: 0.0566\n",
      "Epoch: 5990 loss_train: 0.0534 loss_val: 0.0548\n",
      "Epoch: 5991 loss_train: 0.0533 loss_val: 0.0574\n",
      "Epoch: 5992 loss_train: 0.0537 loss_val: 0.0565\n",
      "Epoch: 5993 loss_train: 0.0528 loss_val: 0.0556\n",
      "Epoch: 5994 loss_train: 0.0542 loss_val: 0.0564\n",
      "Epoch: 5995 loss_train: 0.0537 loss_val: 0.0569\n",
      "Epoch: 5996 loss_train: 0.0534 loss_val: 0.0552\n",
      "Epoch: 5997 loss_train: 0.0531 loss_val: 0.0565\n",
      "Epoch: 5998 loss_train: 0.0519 loss_val: 0.0551\n",
      "Epoch: 5999 loss_train: 0.0530 loss_val: 0.0563\n",
      " total time: 2244.3759s\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(modelsavepath,str(9360)+'.pt')))\n",
    "    \n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    loss_all=0\n",
    "    for i in range(int(np.ceil(trainIdx.shape[0]/batchsize))):\n",
    "        trainIdx_i=trainIdx[i*batchsize:min((i+1)*batchsize,trainIdx.shape[0])]\n",
    "        \n",
    "#         trainInput=trainInputnp[trainIdx]\n",
    "#         labels=trainLabelsnp[trainIdx]\n",
    "        if use_cuda:\n",
    "            trainInput=torch.tensor(allstats[trainIdx_i]).cuda().float()\n",
    "            labels=torch.tensor(alllabels[trainIdx_i]).cuda().long()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(trainInput)\n",
    "#         print(trainInput)\n",
    "#         print(pred)\n",
    "\n",
    "\n",
    "        loss=lossCE(pred,labels)\n",
    "        loss_all+=loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_all=loss_all/int(np.ceil(trainIdx.shape[0]/batchsize))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_val_all=0\n",
    "        nvalBatches=int(np.ceil(valIdx.shape[0]/batchsize))\n",
    "        for i in range(nvalBatches):\n",
    "            valIdx_i=valIdx[i*batchsize:min((i+1)*batchsize,valIdx.shape[0])]\n",
    "            if use_cuda:\n",
    "                valInput=torch.tensor(allstats[valIdx_i]).cuda().float()\n",
    "                labels=torch.tensor(alllabels[valIdx_i]).cuda().long()\n",
    "                \n",
    "            pred= model(valInput)\n",
    "\n",
    "            loss_val=lossCE(pred,labels).item()\n",
    "\n",
    "            loss_val_all+=loss_val\n",
    "\n",
    "        loss_val_all=loss_val_all/nvalBatches\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.4f}'.format(loss_all),\n",
    "          'loss_val: {:.4f}'.format(loss_val_all))\n",
    "    return loss_all,loss_val_all\n",
    "    \n",
    "train_loss_ep=[None]*epochs\n",
    "val_loss_ep=[None]*epochs\n",
    "t_ep=time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "# for ep in range(10000,20000):\n",
    "    \n",
    "    train_loss_ep[ep],val_loss_ep[ep]=train(ep)\n",
    "\n",
    "        \n",
    "    if ep%saveFreq == 0 and ep!=0:\n",
    "        torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,str(ep)+'.pt'))\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "print(' total time: {:.4f}s'.format(time.time() - t_ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(logsavepath,'train_loss'), 'wb') as output:\n",
    "    pickle.dump(train_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss'), 'wb') as output:\n",
    "    pickle.dump(val_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(epochs),train_loss_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_ep)\n",
    "plt.legend(['training loss','validation loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath,'loss_seed3.jpg'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05427918117493391\n"
     ]
    }
   ],
   "source": [
    "#test loss\n",
    "ep=5800\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(ep)+'.pt')))\n",
    "predtest=np.array([])\n",
    "with torch.no_grad():\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    loss_test_all=0\n",
    "    ntestBatches=int(np.ceil(testIdx.shape[0]/batchsize))\n",
    "    for i in range(ntestBatches):\n",
    "        testIdx_i=testIdx[i*batchsize:min((i+1)*batchsize,testIdx.shape[0])]\n",
    "        testInput=torch.tensor(allstats[testIdx_i])\n",
    "        if use_cuda:\n",
    "            testInput=testInput.cuda().float()\n",
    "            labels=torch.tensor(alllabels[testIdx_i]).cuda().long()\n",
    "        pred = model(testInput)\n",
    "        predtest=np.concatenate((predtest,np.argmax(pred.cpu().detach().numpy(),axis=1)))\n",
    "        \n",
    "        loss_test=lossCE(pred,labels).item()\n",
    "\n",
    "        loss_test_all+=loss_test\n",
    "\n",
    "    loss_test_all=loss_test_all/ntestBatches\n",
    "    \n",
    "print(loss_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion\n",
    "def plotCTcomp(labels,ctlist,savepath,savenamecluster,byCT,addname=''):\n",
    "    res=np.zeros((np.unique(labels).size,np.unique(ctlist).size))\n",
    "    for li in range(res.shape[0]):\n",
    "        l=np.unique(labels)[li]\n",
    "        nl=np.sum(labels==l)\n",
    "        ctlist_l=ctlist[labels==l]\n",
    "        for ci in range(res.shape[1]):\n",
    "            c=np.unique(ctlist)[ci]\n",
    "            res[li,ci]=np.sum(ctlist_l==c)\n",
    "#             res[li,ci]=np.sum(ctlist_l==c)/nl\n",
    "    if not byCT:\n",
    "        addname+=''\n",
    "        for li in range(res.shape[0]):\n",
    "            l=np.unique(labels)[li]\n",
    "            nl=np.sum(labels==l)\n",
    "            res[li]=res[li]/nl\n",
    "    else:\n",
    "        addname+='_normbyCT'\n",
    "        for ci in range(res.shape[1]):\n",
    "            c=np.unique(ctlist)[ci]\n",
    "            nc=np.sum(ctlist==c)\n",
    "            res[:,ci]=res[:,ci]/nc\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(res,cmap='binary')\n",
    "    fig.colorbar(im)\n",
    "    ax.set_yticks(np.arange(np.unique(labels).size))\n",
    "    ax.set_yticklabels(np.unique(labels))\n",
    "    ax.set_xticks(np.arange(np.unique(ctlist).size))\n",
    "    ax.set_xticklabels(np.unique(ctlist))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(savepath,savenamecluster+addname+'.jpg'))\n",
    "    plt.close()\n",
    "    \n",
    "plotCTcomp(alllabels[testIdx],predtest,plotsavepath,'confusion_test',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028236948242732162\n"
     ]
    }
   ],
   "source": [
    "#train loss\n",
    "ep=5800\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(ep)+'.pt')))\n",
    "predtrain=np.array([])\n",
    "with torch.no_grad():\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    loss_train_all=0\n",
    "    ntrainBatches=int(np.ceil(trainIdx.shape[0]/batchsize))\n",
    "    for i in range(ntrainBatches):\n",
    "        trainIdx_i=trainIdx[i*batchsize:min((i+1)*batchsize,trainIdx.shape[0])]\n",
    "        trainInput=torch.tensor(allstats[trainIdx_i])\n",
    "        if use_cuda:\n",
    "            trainInput=trainInput.cuda().float()\n",
    "            labels=torch.tensor(alllabels[trainIdx_i]).cuda().long()\n",
    "        pred = model(trainInput)\n",
    "        predtrain=np.concatenate((predtrain,np.argmax(pred.cpu().detach().numpy(),axis=1)))\n",
    "        \n",
    "        loss_train=lossCE(pred,labels).item()\n",
    "\n",
    "        loss_train_all+=loss_train\n",
    "\n",
    "    loss_train_all=loss_train_all/ntrainBatches\n",
    "    \n",
    "print(loss_train_all)\n",
    "plotCTcomp(alllabels[trainIdx],predtrain,plotsavepath,'confusion_train',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_s=pd.read_csv(path_s)\n",
    "stats_s.index=stats_s.loc[:,'label']\n",
    "stats_s.columns[2:-2][np.isnan(allstats[0].astype(float))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats_s=pd.read_csv(path_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_s.index=stats_s.loc[:,'label']\n",
    "stats_s.columns[2:-2][np.isnan(allstats[1].astype(float))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
